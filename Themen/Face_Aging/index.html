
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../Empfehlungssysteme/">
      
      
        <link rel="next" href="../LLMs/">
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.1, mkdocs-material-9.1.21">
    
    
      
        <title>Face Aging - Seminar Aktuelle Themen der künstlichen Intelligenz</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.eebd395e.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css">
    
      <link rel="stylesheet" href="../../javascripts/mathjax.js">
    
      <link rel="stylesheet" href="https://polyfill.io/v3/polyfill.min.js?features=es6">
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#face-aging" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Seminar Aktuelle Themen der künstlichen Intelligenz" class="md-header__button md-logo" aria-label="Seminar Aktuelle Themen der künstlichen Intelligenz" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Seminar Aktuelle Themen der künstlichen Intelligenz
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Face Aging
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Seminar Aktuelle Themen der künstlichen Intelligenz" class="md-nav__button md-logo" aria-label="Seminar Aktuelle Themen der künstlichen Intelligenz" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Seminar Aktuelle Themen der künstlichen Intelligenz
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Aktuelles
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          Themen
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Themen
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Einf%C3%A4rben%20von%20Bildern/" class="md-nav__link">
        Einfärben von Bildern - Zwei Ansätze
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Empfehlungssysteme/" class="md-nav__link">
        Empfehlungssysteme
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Face Aging
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Face Aging
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#abstract" class="md-nav__link">
    Abstract
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#einleitung-motivation" class="md-nav__link">
    Einleitung / Motivation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stand-der-forschung" class="md-nav__link">
    Stand der Forschung
  </a>
  
    <nav class="md-nav" aria-label="Stand der Forschung">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#datensatze" class="md-nav__link">
    Datensätze
  </a>
  
    <nav class="md-nav" aria-label="Datensätze">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#morph" class="md-nav__link">
    MORPH
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-age-celebrity-dataset-cacd" class="md-nav__link">
    Cross-Age Celebrity Dataset (CACD)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fg-net" class="md-nav__link">
    FG-NET
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#imdb-wiki" class="md-nav__link">
    IMDB-WIKI
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#generative-adversarial-networks-gans" class="md-nav__link">
    Generative Adversarial Networks (GANS)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#methoden" class="md-nav__link">
    Methoden
  </a>
  
    <nav class="md-nav" aria-label="Methoden">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#physikalische-modellbasierte-methoden" class="md-nav__link">
    Physikalische modellbasierte Methoden
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prototypbasierte-methoden" class="md-nav__link">
    Prototypbasierte Methoden
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deep-generative-networks" class="md-nav__link">
    Deep Generative Networks
  </a>
  
    <nav class="md-nav" aria-label="Deep Generative Networks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generative-adversarial-networks-gans_1" class="md-nav__link">
    Generative Adversarial Networks (GANs)
  </a>
  
    <nav class="md-nav" aria-label="Generative Adversarial Networks (GANs)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pytorch-example" class="md-nav__link">
    PyTorch Example
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fazit" class="md-nav__link">
    Fazit
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#conditional-generative-adversiral-networks" class="md-nav__link">
    Conditional Generative Adversiral Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#face-aging-with-conditional-generative-adversarial-networks" class="md-nav__link">
    Face Aging with Conditional Generative Adversarial Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#face-aging-with-contextual-generative-adversarial-networks" class="md-nav__link">
    Face Aging with Contextual Generative Adversarial Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#face-aging-with-identity-preserved-conditional-generative-adversarial-networks" class="md-nav__link">
    Face Aging With Identity-Preserved Conditional Generative Adversarial Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-face-age-progression-a-pyramid-architecture-of-gans" class="md-nav__link">
    Learning Face Age Progression: A Pyramid Architecture of GANs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#triple-gan-progressive-face-aging-with-triple-translation-loss" class="md-nav__link">
    Triple-GAN: Progressive Face Aging with Triple Translation Loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#only-a-matter-of-style-age-transformation-using-a-style-based-regression-model" class="md-nav__link">
    Only a Matter of Style: Age Transformation Using a Style-Based Regression Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pfa-gan-progressive-face-aging-with-generative-adversarial-network" class="md-nav__link">
    PFA-GAN: Progressive Face Aging With Generative Adversarial Network
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#anwendungen" class="md-nav__link">
    Anwendungen
  </a>
  
    <nav class="md-nav" aria-label="Anwendungen">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#smokerface-app" class="md-nav__link">
    Smokerface App
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sunface-uv-selfie" class="md-nav__link">
    Sunface - UV-Selfie
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#aprilage-inc" class="md-nav__link">
    AprilAge Inc.
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fazit_1" class="md-nav__link">
    Fazit
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#weiterfuhrendes-material" class="md-nav__link">
    Weiterführendes Material
  </a>
  
    <nav class="md-nav" aria-label="Weiterführendes Material">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#talk" class="md-nav__link">
    Talk
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#demo" class="md-nav__link">
    Demo
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#literaturliste" class="md-nav__link">
    Literaturliste
  </a>
  
    <nav class="md-nav" aria-label="Literaturliste">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#datensatze_1" class="md-nav__link">
    Datensätze
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#physikalische-modellbasierte-methoden_1" class="md-nav__link">
    Physikalische modellbasierte Methoden
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prototypbasierte-methoden_1" class="md-nav__link">
    Prototypbasierte Methoden
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deep-generative-networks_1" class="md-nav__link">
    Deep Generative Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#anwendungen_1" class="md-nav__link">
    Anwendungen
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../LLMs/" class="md-nav__link">
        Large Language Models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Spracherkennung/" class="md-nav__link">
        Spracherkennung
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../T09_Dialekte_Spracherkennung/" class="md-nav__link">
        T09 - Dialekte in der Spracherkennung
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../T12_feature-extraction/" class="md-nav__link">
        Feature Extraction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../TextToSpeech/" class="md-nav__link">
        Text-to-Speech
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Zeitserienanalyse/" class="md-nav__link">
        Zeitserienanalyse
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../bayesian_modeling/" class="md-nav__link">
        Bayesian Modeling
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../t7_Process_Mining/" class="md-nav__link">
        Process Mining
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../transfer_learning_nlp/" class="md-nav__link">
        Transfer Learning in der Sprachverarbeitung
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../impressum/" class="md-nav__link">
        Impressum
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#abstract" class="md-nav__link">
    Abstract
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#einleitung-motivation" class="md-nav__link">
    Einleitung / Motivation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stand-der-forschung" class="md-nav__link">
    Stand der Forschung
  </a>
  
    <nav class="md-nav" aria-label="Stand der Forschung">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#datensatze" class="md-nav__link">
    Datensätze
  </a>
  
    <nav class="md-nav" aria-label="Datensätze">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#morph" class="md-nav__link">
    MORPH
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-age-celebrity-dataset-cacd" class="md-nav__link">
    Cross-Age Celebrity Dataset (CACD)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fg-net" class="md-nav__link">
    FG-NET
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#imdb-wiki" class="md-nav__link">
    IMDB-WIKI
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#generative-adversarial-networks-gans" class="md-nav__link">
    Generative Adversarial Networks (GANS)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#methoden" class="md-nav__link">
    Methoden
  </a>
  
    <nav class="md-nav" aria-label="Methoden">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#physikalische-modellbasierte-methoden" class="md-nav__link">
    Physikalische modellbasierte Methoden
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prototypbasierte-methoden" class="md-nav__link">
    Prototypbasierte Methoden
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deep-generative-networks" class="md-nav__link">
    Deep Generative Networks
  </a>
  
    <nav class="md-nav" aria-label="Deep Generative Networks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generative-adversarial-networks-gans_1" class="md-nav__link">
    Generative Adversarial Networks (GANs)
  </a>
  
    <nav class="md-nav" aria-label="Generative Adversarial Networks (GANs)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pytorch-example" class="md-nav__link">
    PyTorch Example
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fazit" class="md-nav__link">
    Fazit
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#conditional-generative-adversiral-networks" class="md-nav__link">
    Conditional Generative Adversiral Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#face-aging-with-conditional-generative-adversarial-networks" class="md-nav__link">
    Face Aging with Conditional Generative Adversarial Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#face-aging-with-contextual-generative-adversarial-networks" class="md-nav__link">
    Face Aging with Contextual Generative Adversarial Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#face-aging-with-identity-preserved-conditional-generative-adversarial-networks" class="md-nav__link">
    Face Aging With Identity-Preserved Conditional Generative Adversarial Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-face-age-progression-a-pyramid-architecture-of-gans" class="md-nav__link">
    Learning Face Age Progression: A Pyramid Architecture of GANs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#triple-gan-progressive-face-aging-with-triple-translation-loss" class="md-nav__link">
    Triple-GAN: Progressive Face Aging with Triple Translation Loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#only-a-matter-of-style-age-transformation-using-a-style-based-regression-model" class="md-nav__link">
    Only a Matter of Style: Age Transformation Using a Style-Based Regression Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pfa-gan-progressive-face-aging-with-generative-adversarial-network" class="md-nav__link">
    PFA-GAN: Progressive Face Aging With Generative Adversarial Network
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#anwendungen" class="md-nav__link">
    Anwendungen
  </a>
  
    <nav class="md-nav" aria-label="Anwendungen">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#smokerface-app" class="md-nav__link">
    Smokerface App
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sunface-uv-selfie" class="md-nav__link">
    Sunface - UV-Selfie
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#aprilage-inc" class="md-nav__link">
    AprilAge Inc.
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fazit_1" class="md-nav__link">
    Fazit
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#weiterfuhrendes-material" class="md-nav__link">
    Weiterführendes Material
  </a>
  
    <nav class="md-nav" aria-label="Weiterführendes Material">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#talk" class="md-nav__link">
    Talk
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#demo" class="md-nav__link">
    Demo
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#literaturliste" class="md-nav__link">
    Literaturliste
  </a>
  
    <nav class="md-nav" aria-label="Literaturliste">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#datensatze_1" class="md-nav__link">
    Datensätze
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#physikalische-modellbasierte-methoden_1" class="md-nav__link">
    Physikalische modellbasierte Methoden
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prototypbasierte-methoden_1" class="md-nav__link">
    Prototypbasierte Methoden
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deep-generative-networks_1" class="md-nav__link">
    Deep Generative Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#anwendungen_1" class="md-nav__link">
    Anwendungen
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="face-aging">Face Aging</h1>
<p>von <em>Felix Rösch, Julian Steiner</em></p>
<h2 id="abstract">Abstract</h2>
<p>Bei Face Aging handelt es sich um die technische Möglichkeit Gesichter von Menschen digital altern zu lassen. Wichtig hierbei ist, dass das zukünftige Aussehen mit natürlichen Alterungseffekten vorhergesagt werden und gleichzeitig die persönlichen Merkmale erhalten bleiben. Neuentwickelte Generative Adversarial Networks (GANs) in verschiedenen Formen erzielten eine bessere Vorhersage der Alterung als herkömmliche Methoden. </p>
<p>In diesen Report gehen wir auf den aktuellen Stand der Forschung, die Methoden und potentielle Anwendungen ein. Zusätzlich zu dieser schriftlichen Ausarbeitung wurde ein Fachvortrag und eine Code-Präsentation erarbeitet.</p>
<p>Im Fachvortrag gehen wir detaillierter auf das Paper <a href="https://arxiv.org/abs/2012.03459">"PFA-GAN: Progressive Face Aging With Generative Adversarial Network"</a> und dessen Generative Adversarial Network zum Thema Face Aging ein. Die Code-Demonstration greift diesen Vortrag auf und implementiert das neuronale Netz in einem Jupyter Notebook.</p>
<h2 id="einleitung-motivation">Einleitung / Motivation</h2>
<div class="admonition info">
<p class="admonition-title">Definition</p>
<p>Gesichtsalterung (Face Aging), auch bekannt als Alterssynthese (Age Synthesis) und Altersprogression (Age Progression), wird als ästhetische Darstellung eines Gesichtsbildes mit natürlichen Alterungs- und Verjüngungseffekten auf das einzelne Gesicht definiert.</p>
<p><em><a href="https://ieeexplore.ieee.org/document/5406526">Fu Y, Guo G, Huang TS</a></em></p>
</div>
<p>Anwendung finden diese Methoden zum Beispiel in den Bereichen der Unterhaltung, sozialer Sicherheit, altersübergreifenden Gesichtserkennung, Forensik und Medizin.</p>
<p>Mögliche Anwendungsbeispiele sind zum einen Applikationen, die das Gesicht einer Person mit einem bestimmten Lebensstil vorhersagen. Solche Anwendungen können zum Beispiel dazu genutzt werden, um Personen mit einen übermäßigen Alkohol- und Nikotinkonsum die Auswirkungen aufzuzeigen um damit den Konsum zu verringern. Des Weiteren können solche Technologien helfen den Menschenhandel zu bekämpfen und Familien wieder zusammenführen, denn das menschliche Gesicht durchläuft von der Kindheit bis zum Erwachsenenalter eine deutliche körperliche Veränderung. Deswegen ist es zehn bis 15 Jahre nach der Entführung schwierig, ein verlorenes Kind wiederzuerkennen.</p>
<p>Die traditionellen Alterungsmethoden basieren meist auf der mechanischen Modellierung von Falten, Haaren, Textur und Gesichtsausdruck oder nutzen viele Daten, um Prototypen als Altersmuster zu konstruieren. Neue Deep Learning Methoden erzielten große Erfolge bei dem Thema Face Aging. Durch das Training zum Erlernen spezifischer Altersmuster und Zuordnungen zwischen Eingabegesichtern und Zielaltersbezeichnungen können Deep-Learning-Methoden Gesichter einer bestimmten Altersgruppe direkt generieren. Grob können die verschiedenen Methoden in drei Kategorien eingeteilt werden:</p>
<ol>
<li>Physikalische modellbasierte Methoden</li>
<li>Prototypbasierte Methoden</li>
<li>Deep Generative Networks (Tiefe generative Netzwerke)</li>
</ol>
<p>Obwohl sich mit Deep-Learning-Methoden die Altersmuster leicht erlernen lassen, können sie in der gewünschten Altersgruppe keine zufriedenstellenden Ergebnisse erzielen. Die altersgruppenbasierte Synthese teilt den Langzeitverlauf in mehrere unabhängige Gruppen auf und fügt Identitätserhaltung zwischen Eingabe und Ausgabe hinzu. Jedoch werden die fortschreitende Änderung des Altersmuster und Identitätserhaltung zwischen den synthetisierten Bildern ignoriert. Um diese Probleme zu lösen wurden verschiedene Arten von Deep-Learning-Methoden entwickelt.</p>
<p>In den folgenden Kapiteln werden wir die Herausforderungen bei der Erstellung der Datensätze und der Entwicklung solcher neuronalen Netzwerke in ihren verschiedenen Methoden genauer beleuchten.</p>
<h2 id="stand-der-forschung">Stand der Forschung</h2>
<h3 id="datensatze">Datensätze</h3>
<p>In diesem Abschnitt wollen wir die wichtigsten aktuell verfügbaren und verwendeten Datensätze für das Thema Face Aging vorstellen. Diese Datensätze wurden hauptsächlich in den Papern verwendet, die im Kapitel Methoden genauer vorgestellt werden. Bei MORPH und CACD handelt es sich dabei um die meistverwendeten Datensätze.</p>
<h4 id="morph">MORPH</h4>
<p>Im Jahr 2006 gab es nur drei öffentlich zugängliche, bekannte Datenbanken, die Doppelbilder einer Person in verschiedenen Altersstufen enthielten. <a href="https://ieeexplore.ieee.org/document/1613043">MORPH</a>, <a href="https://ieeexplore.ieee.org/document/879790">FERET</a> und <a href="https://yanweifu.github.io/FG_NET_data/">FG-NET</a>.</p>
<p>MORPH war zu diesem Zeitpunkt die einzige dieser drei Datenbanken, die die ethnische Zugehörigkeit, die Größe, das Gewicht und das Geschlecht der Probanden erfasste. Diese Eigenschaften sind für das Verständnis der Veränderung des Aussehens des menschlichen Gesichts im Alter von entscheidender Bedeutung. Zusätzlich beinhaltete diese Datenbank den größten Satz öffentlich verfügbarer Bilder von Personen über einen längeren Zeitraum, von einigen Monaten bis zu einer Zeitspanne von mehreren Jahrzehnten. Bei den damaligen Methoden war diese Eigenschaft Voraussetzung für die Erstellung eines erfolgreichen Modells.</p>
<p>Seitdem wird der Datensatz ständig weiterentwickelt. Aktuell gibt es den Datensatz der University of North Carolina Wilmington in drei unterschiedliche Varianten: Dem MORPH Commercial Set, MORPH Academic Set und die MORPH Longitudinal Database. Um den Datensatz zu erhalten, muss die Universität persönlich kontaktiert werden. Anschließend wird eine Lizenz ausgestellt und Zugriff auf die Daten gewährt. Weitere Informationen finden Sie auf dieser <a href="https://uncw.edu/myuncw/research/innovation-commercialization/technology-portfolio/morph">Website</a>.</p>
<figure>
<p><img alt="MORPH Figure Examples" src="../img/Face%20Aging/MORPH_Fig_Example.jpg" width="800" />
  </p>
<figcaption>Beispiel Bilder MORPH Datensatz</figcaption>
</figure>
<h4 id="cross-age-celebrity-dataset-cacd">Cross-Age Celebrity Dataset (CACD)</h4>
<p>Der <a href="https://bcsiriuschen.github.io/CARC/">CACD</a> Datensatz beinhaltet über 160.000 Bilder von über 2.000 berühmten Persönlichkeiten. Diese Bilder stammen aus dem Internet und wurden automatisch über Suchmaschinen gesammelt und gespeichert. Dabei dienten die Namen der Personen und das Jahr (2004-2013) als Schlüsselwörter. Das Alter einer Person wurde bestimmt, indem das Geburtsjahr der Person von dem Jahr subtrahiert wurde, indem das Foto aufgenommen wurde. Zusätzliche zu den Bildern gibt es einen Metadatensatz im MATLAB-Format, welcher wichtige Informationen zu den Bildern beinhaltet. Hierbei handelt es sich z.B. um den Namen und das Alter der Person. </p>
<figure>
<p><img alt="CACD Figure Examples" src="../img/Face%20Aging/CACD_fig_example.jpg" width="600" />
  </p>
<figcaption>Beispiel Bilder CACD Datensatz</figcaption>
</figure>
<p>Der Datensatz und die Metadaten können auf der <a href="https://bcsiriuschen.github.io/CARC/">Homepage</a> heruntergeladen werden.</p>
<h4 id="fg-net">FG-NET</h4>
<p>Eingeführt wurde der Datensatz FG-NET mit dem Paper <a href="https://ieeexplore.ieee.org/document/993553">Toward automatic simulation of aging effects on face images</a>. Insgesamt beinhaltet er über 1.000 Bilder von 82 Personen. Es wird beim Alter eine Spanne von 0 bis 69 Jahren und ein Altersunterschied von bis zu 45 Jahren abgedeckt. Der Datensatz auf dieser <a href="https://yanweifu.github.io/FG_NET_data/">Webseite</a> verfügbar.</p>
<h4 id="imdb-wiki">IMDB-WIKI</h4>
<p>Im <a href="https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/">IMDB-WIKI</a> Datensatz wurden eine große Sammlung von Bildern und Metadten von Berühmtheiten zusammengestellt. Hier sammelten die Ersteller automatisch das Geburtsdatum, den Namen, das Geschlecht und alle Bilder der 100.000 beliebtesten Schauspieler auf der IMDb-Website. Zusätzlich wurden zu diesen Personen alle Profilbilder von Personenseiten aus Wikipedia mit denselben Metadaten automatische gesammelt. Entfernt wurden die Bilder, die keinen Zeitstempel hatten. Das reale Alter der Person auf einem Bild wurde durch das Geburtsdatum und den Zeitstempel des Bildes errechnet. Der Datensatz beinhaltet über 460.000 Gesichtsbilder von mehr als 20.000 Prominenten aus IMDb und mehr als 62.000 Bilder aus Wikipedia. Insgesamt umfasst er über 520.000 Bilder.</p>
<figure>
<p><img alt="IMDB-WIKI Figure Examples" src="../img/Face%20Aging/imdb-wiki-teaser.png" width="600" />
  </p>
<figcaption>Beispiel Bilder IMDB-WIKI Datensatz</figcaption>
</figure>
<p>Auf der <a href="https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/">Webseite</a> des Datensatzes können die Bilder und Metadaten in verschiedensten Varianten heruntergeladen werden. Auch bereits vortrainierte Gewichte zu implementierten Modellen sind hier zu finden.</p>
<h3 id="generative-adversarial-networks-gans">Generative Adversarial Networks (GANS)</h3>
<p>Mit der folgenden Abbildung wollen wir die Entwicklung der GAN-Methoden darstellen, die wir für dieses Projekt betrachtet haben. Die Zeitreihe stellt die jeweiligen Paper mit dem Jahr der Veröffentlich dar.</p>
<figure>
<p><img alt="Timeline GAN Methods" src="../img/Face%20Aging/Timeline_GANs.jpg" width="600" />
  </p>
<figcaption>Entwicklung GAN Methoden</figcaption>
</figure>
<h2 id="methoden">Methoden</h2>
<h3 id="physikalische-modellbasierte-methoden">Physikalische modellbasierte Methoden</h3>
<p>Physikalische, modellbasierte Methoden befassen sich mit dem Entwurf eines komplexen Modells zur Nachahmung des Gesichtsaussehens und zur Simulation von Alterungsmechanismen in Bezug auf Haare, Muskeln und Haut bei Erwachsenen und mit der Anwendung spezifischer Transformationen auf eine Reihe von Orientierungspunkten oder statistischer Parameter zur Modellierung altersbedingter Formveränderungen bei Kindern. Für diese Methode muss jedoch ein parametrisches Modell erstellt werden, und es werden viele Gesichter derselben Identitäten in verschiedenen Altersstufen benötigt, was rechenintensiv und schwer zu erfassen ist.</p>
<h3 id="prototypbasierte-methoden">Prototypbasierte Methoden</h3>
<p>Die prototypenbasierten Methoden verwenden ein nichtparametrisches Modell. Die Gesichter sollten zunächst in Gruppen nach verschiedenen Altersgruppen eingeteilt werden. Das durchschnittliche Gesicht jeder Altersgruppe wird als Prototyp und Altersmuster einer bestimmten Altersgruppe bezeichnet. Im Paper <a href="https://arxiv.org/abs/1510.06503">Personalized Age Progression with Aging Dictionary</a> hat das Autorenteam eine auf Wörterbüchern basierende Alterssynthesemethode vorgeschlagen. <a href="https://ieeexplore.ieee.org/document/7442560">Yang et al.</a> haben mit Hilfe der Hidden Factor Analysis eine gemeinsame spärliche Darstellung eingeführt. Diese vorgeschlagenen Alterungsmethoden modellieren getrennt die stabilen personenabhängigen Eigenschaften über einen relativ langen Zeitraum und die altersabhängigen Informationen, die sich im Laufe der Zeit allmählich ändern. Da das Altersmuster jedoch aus dem Durchschnittsgesicht gewonnen wird, tendieren prototypbasierte Methoden dazu die identitätsgebenden Merkmale eines speziellen Gesichtes zu verlieren.</p>
<h3 id="deep-generative-networks">Deep Generative Networks</h3>
<p>Die beiden obengenannten Ansätze erfordern jedoch häufig die Erstellung von Alterungssequenzen derselben Person mit einem breiten Altersspektrum, deren Erfassung sehr schwierig und kostspielig ist. Generative Adversarial Networks (GANs) benötigen keine gepaarten Bilder von Gesichtern und erzielen dabei eine bessere Alterungsleistung als diese Methoden.</p>
<p>Bei den nachfolgend vorgestellten Methoden gehen wir ganz grob auf das jeweilige Paper ein. Diese wurden verlinkt und sind auch in der Literaturliste zu finden.</p>
<h4 id="generative-adversarial-networks-gans_1">Generative Adversarial Networks (GANs)</h4>
<p>Generative Adversarial Networks sind tiefe neuronale Netzwerke. Sie nutzen unbeaufsichtigtes maschinelles Lernen um Daten zu generieren. Eingeführt wurden solche Netze 2014 in einem <a href="https://arxiv.org/abs/1406.2661">Paper</a> von Ian Goodfellow, Yoshua Bengio, and Aaron Courville.</p>
<p>Ein solches neuronales Netzwerk besteht aus zwei weiteren Netzwerken. Einem Generator Netzwerk und einem Discriminator Netzwerk. Durch mehrere Zyklen von Generierung und Diskriminierung neuer Inhalte trainieren sich beide Netzwerke gegenseitig und versuchen gleichzeitig, sich gegenseitig zu überlisten. Das Ziel solcher Netze ist es, Datenpunkte zu generieren, die einigen Datenpunkten im Trainingssatz so stark ähneln, dass sie vom Discriminator Netz nicht mehr als KI-generiert erkannt werden.</p>
<h5 id="pytorch-example">PyTorch Example</h5>
<p>In diesem kurzen Code-Beispiel wollen wir eine einfache Implementierung eines Generative Adversarial Networks mit dem PyTorch-Framework zeigen. Den kompletten Code inklusive dem Laden der Daten und dem Trainieren findet ihr in der <a href="https://github.com/julian-steiner-ai/face-aging">GIT Repository</a> unter dem Kapitel <code>01_GAN</code>. </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Wichtig ist hier noch zu nennen, dass diese Implementierung noch nicht den Aspekt Face Aging berücksichtigt. Mit diesem GAN können nur Gesichter anhand des Trainingdatensatzes generiert werden. Ohne Bedingungen etc.</p>
</div>
<p>Wir starten mit der Implementierung der Generator-Netzwerks. Hierbei erben wir von der <code>nn.Module</code> Klasse. Dies ist die Basisklasse für alle neuronalen Netzwerke in PyTorch. </p>
<p>In der <code>_init_network</code> Methode definieren wir die einzelnen Schichten des jeweiligen neuronalen Netzwerks. Hierzu fügen wir die einzelnen Klassen der Liste mit dem Namen <code>layer</code> hinzu. Anschließend übergeben wir diese an einen <code>Sequential</code>-Container. Dieser ermöglicht einen einfachen Aufruf der <code>forward()</code>-Methode, da dieser die Ausgaben einer Schicht mit den Eingaben des nachfolgenden Moduls automatisch miteinander "verkettet". Schließlich wird die Ausgabe des letzten Moduls zurückgegeben. </p>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">Generator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generator.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">n_feature_maps</span><span class="p">,</span> <span class="n">n_channels</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Generator</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span> <span class="o">=</span> <span class="n">z_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_feature_maps</span> <span class="o">=</span> <span class="n">n_feature_maps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_channels</span> <span class="o">=</span> <span class="n">n_channels</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_init_network</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_init_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">layer</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># input is Z, going into a convolution</span>
        <span class="n">layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feature_maps</span> <span class="o">*</span> <span class="mi">8</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">))</span>

        <span class="n">layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feature_maps</span> <span class="o">*</span> <span class="mi">8</span><span class="p">))</span>
        <span class="n">layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>

        <span class="c1"># state size. ``(ngf*8) x 4 x 4``</span>
        <span class="n">layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feature_maps</span> <span class="o">*</span> <span class="mi">8</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feature_maps</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">))</span>

        <span class="n">layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feature_maps</span> <span class="o">*</span> <span class="mi">4</span><span class="p">))</span>
        <span class="n">layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>

        <span class="c1"># state size. ``(ngf*4) x 8 x 8``</span>
        <span class="n">layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feature_maps</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feature_maps</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">))</span>

        <span class="n">layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feature_maps</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>

        <span class="c1"># state size. ``(ngf*2) x 16 x 16``</span>
        <span class="n">layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feature_maps</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feature_maps</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">))</span>

        <span class="n">layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feature_maps</span><span class="p">))</span>
        <span class="n">layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>

        <span class="c1"># state size. ``(ngf) x 32 x 32``</span>
        <span class="n">layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feature_maps</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">))</span>

        <span class="n">layer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">())</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">main</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layer</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward operation for the generator network.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">main</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>
<p>Mit der nächsten Klasse implementieren wir das Diskriminator-Netzwerk. Das vorgehen ist identisch mit dem des Generator-Netzwerks von oben.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">Discriminator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Discriminator.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_channels</span><span class="p">,</span> <span class="n">n_feature_maps</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Discriminator</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_channels</span> <span class="o">=</span> <span class="n">n_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_feature_maps</span> <span class="o">=</span> <span class="n">n_feature_maps</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_init_network</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_init_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># input is ``(nc) x 64 x 64``</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_channels</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_feature_maps</span><span class="p">,</span>
            <span class="mi">4</span><span class="p">,</span>
            <span class="mi">2</span><span class="p">,</span>
            <span class="mi">1</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">))</span>

        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

        <span class="c1"># state size. ``(ndf) x 32 x 32``</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_feature_maps</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_feature_maps</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span>
            <span class="mi">4</span><span class="p">,</span>
            <span class="mi">2</span><span class="p">,</span>
            <span class="mi">1</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">))</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feature_maps</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

        <span class="c1"># state size. ``(ndf*2) x 16 x 16``</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_feature_maps</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_feature_maps</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span>
            <span class="mi">4</span><span class="p">,</span>
            <span class="mi">2</span><span class="p">,</span>
            <span class="mi">1</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">))</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feature_maps</span> <span class="o">*</span> <span class="mi">4</span><span class="p">))</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

        <span class="c1"># state size. ``(ndf*4) x 8 x 8``</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_feature_maps</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_feature_maps</span> <span class="o">*</span> <span class="mi">8</span><span class="p">,</span>
            <span class="mi">4</span><span class="p">,</span>
            <span class="mi">2</span><span class="p">,</span>
            <span class="mi">1</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">))</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_feature_maps</span> <span class="o">*</span> <span class="mi">8</span><span class="p">))</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

        <span class="c1"># state size. ``(ndf*8) x 4 x 4``</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_feature_maps</span> <span class="o">*</span> <span class="mi">8</span><span class="p">,</span>
            <span class="mi">1</span><span class="p">,</span>
            <span class="mi">4</span><span class="p">,</span>
            <span class="mi">1</span><span class="p">,</span>
            <span class="mi">0</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">))</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">())</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">main</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward Operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">main</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>
<h5 id="fazit">Fazit</h5>
<p>Verschiedene Varianten der GAN-basierten Methode können im Bereich Face Aging die plausibelsten und realistischsten Bilder erzeugen, die aufgrund des Alters nur schwer von echten Daten zu unterscheiden sind. Allerdings nutzen sie die sequentiellen Daten nicht vollständig aus. Diese Methoden können die Übergangsmuster, die als Korrelationen der Gesichtsmerkmale zwischen verschiedenen Altersgruppen für eine Person definiert sind, nicht explizit berücksichtigen. Daher sind ihre Ergebnisse meist nicht in der Lage, die Gesichtsidentität beizubehalten oder die Übergangsregeln zwischen verschiedenen Altersgruppen zu berücksichtigen.</p>
<p>Um diese Probleme zu lösen und den Alterungsprozess von Personen mit Hilfe von GANs noch detaillierter und realistischer darzustellen, wurden verschiedene Variationen von diesen Netzwerken entwickelt.</p>
<h4 id="conditional-generative-adversiral-networks">Conditional Generative Adversiral Networks</h4>
<p>Ebenfalls im Jahr 2014 führten die Autoren Mehdi Mirza und Simon Osindero das <a href="https://arxiv.org/abs/1411.1784">Conditional Generative Adversarial Network</a> ein. Bei einem nicht-konditionierten GAN gibt es keine Kontrolle über die Modi der erzeugten Daten. Durch die Konditionierung des Modells auf zusätzliche Informationen ist es jedoch möglich, den Datengenerierungsprozess zu steuern. Eine solche Konditionierung könnte auf Klassenlabels, auf einem Teil der Daten für Inpainting oder sogar auf Daten aus verschiedenen Modalitäten basieren. </p>
<p>GANs können zu einem konditionalen Modell (conditional GAN) erweitert werden, wenn sowohl der Generator als auch der Diskriminator durch eine zusätzliche Information konditioniert werden. Die Konditionierung kann durchgeführt werden, indem die Information sowohl in den Diskriminator als auch in den Generator als zusätzliche Eingabeschicht eingespeist wird.</p>
<figure>
<p><img alt="c-GAN" src="../img/Face%20Aging/Structure_of_Conditional_Adversarial_Net.png" width="400" />
  </p>
<figcaption>Conditional Adversarial Net</figcaption>
</figure>
<p>Die Ergebnisse des Papers demonstrierten das Potenzial von solchen Netzwerken.  </p>
<h4 id="face-aging-with-conditional-generative-adversarial-networks">Face Aging with Conditional Generative Adversarial Networks</h4>
<p>Auch mit dem nächsten Paper - <a href="https://arxiv.org/abs/1702.01983">"Face Aging with Conditional Generative Adversarial Networks"</a> -, aus dem Jahr 2017, wurde versucht, die Probleme mit dem Verlust der Identität der ursprünglichen Person in modifizierten Bildern zu minimieren. Daher konzentriert sich diese Studie auf die identitätserhaltende Gesichtsalterung. Dafür entwickelten die Autoren das Age-cGAN (Age Conditional Generative Adversarial Network). Es sollte das erste GAN sein, das qualitativ hochwertige synthetische Bilder innerhalb der geforderten Alterskategorien erzeugt. Zusätzlich schlug das Team einen neuartigen Ansatz zur Optimierung latenter Vektoren vor, der es Age-cGAN ermöglichte, ein eingegebenes Gesichtsbild zu rekonstruieren, ohne die Identität der ursprünglichen Person zu verändern.</p>
<p>Ein Conditional GAN (cGAN) erweitert das GAN-Modell und ermöglicht die Erzeugung von Bildern mit bestimmten Bedingungen (Attributen). Diese Bedingungen können jede beliebige Information beinhalten, die sich auf das Zielgesichtsbild bezieht. Zum Beispiel Beleuchtungsniveau, Gesichtshaltung oder Gesichtsattribute. Die bedingte Information wird dabei in den Eingang des Generators und in die erste Faltungsschicht (Convolution Layer) von dem Discriminator Netzwerk injiziert.</p>
<p>Conditional GANs verfügen herkömmlicherweise nicht über einen expliziten Mechanismus zur inversen Abbildung eines Eingangsbildes <span class="arithmatex">\(x\)</span> mit Attributen <span class="arithmatex">\(y\)</span> auf einen latenten Vektor <span class="arithmatex">\(z\)</span>, der für die Bildrekonstruktion notwendig ist. Die Autoren umgehen dieses Problem, indem ein Encoder <span class="arithmatex">\(E\)</span> trainiert wird. Bei diesen handelt es sich um ein neuronales Netz, das die inverse Abbildung annähert. Dieser erzeugt anfängliche latente Approximationen, die gut genug sind, um als Initalisierungen für den Optimierungsalgorithmus zu dienen. Das Team nutzt einen neuartigen "identitätserhaltenden Ansatz" (Identity-Preserving) zur Optimierung latenter Vektoren. Hierbei lautet der Grundgedanke folgendermaßen: Bei einem neuronalen Netz zur Gesichtserkennung (Face Recognition), das in der Lage ist, die Identität einer Person in einem eingegebenen Gesichtsbild zu erkennen, kann der Unterschied zwischen den Identiäten in den ursprünglichen und rekonstruiertern Bildern als euklidischer Abstand zwischen den entsprechenenden Einbettungen ausgedrückt werden. Daher sollte die Minimierung dieses Abstands die Erhaltung der Identität im rekonstruierten Bild verbessern.</p>
<figure>
<p><img alt="Age-c-GAN" src="../img/Face%20Aging/Age-c-GAN.png" width="800" />
  </p>
<figcaption>Verwendete Methode. (a) Approximative Gesichtsrekonstruktion mit Age-cGAN (b) Wechseln der Altersbedingung am Eingang des Generators G, um die Gesichtsalterung durchzuführen.</figcaption>
</figure>
<p>In der folgenden Abbildung werden Beispiele für die Rekonstruktion und Alterung von Gesichtern dargestellt. </p>
<ul>
<li>(a) Zeigt die originalen Testbilder</li>
<li>(b) Zeigt die rekonstruierten Bilder</li>
<li>(c) Zeigt die rekonstruierten Bilder, die inklusive den "pixelweisen" und "identitätserhaltenden" Methoden generiert wurden</li>
<li>(d) Zeigt die rekonstruierten Bilder, die unter Verwendung der identitätserhaltenden Approximationen und konditioniert auf die jeweilige Alterskategorie (eine pro Spalte) generiert wurden</li>
</ul>
<figure>
<p><img alt="Results Age-c-GAN" src="../img/Face%20Aging/Age-c-GAN_Results.png" width="800" />
  </p>
<figcaption>Beispiele von generierten Bildern druch das Age-c-GAN</figcaption>
</figure>
<p>Die Autoren kamen zu dem Schluss, dass die Gesichtsrekonstruktion mit ihrer Methode weiter verbessert werden kann, indem "pixelweise" (pixelwise) und "identitätserhaltende" (Identity-Preserving) Ansätze in einem Optimizerungsziel kombiniert werden.</p>
<h4 id="face-aging-with-contextual-generative-adversarial-networks">Face Aging with Contextual Generative Adversarial Networks</h4>
<p>Im Jahr 2018 veröffentlichten eine Autorengruppe um Liu et. al. ein Paper mit dem Titel <a href="https://arxiv.org/abs/1802.00237">"Face Aging with Contextual Generative Adversarial Nets"</a>. Im Gegensatz zu traditionellen GANs, die nur die reale Datenverteilung jedes einzelnen Alters modellieren, konzentrierte sich das Team auf die altersübergreifenden Korrelationen höherer Ordnung, die die Ergebnisse der Gesichtsalterung attraktiver machen sollten. Um dies zu realisieren, schlugen die Autoren die Umsetzung der Gesichtsalterung mittels eines "Contextual Generative Adversarial Networks (C-GANs)" vor.</p>
<figure>
<p><img alt="c-GAN" src="../img/Face%20Aging/c-GANs.png" width="400" />
  </p>
<figcaption>Vorgeschlagener C-GANs-Algorithmus für die Gesichtsalterung.</figcaption>
</figure>
<p>Conextual-GANs bestehen aus drei neuronalen Netzwerken. Um sicherzustellen, dass die erzeugten Bilder echt wirken, werden zwei diskriminierende Netzwerke verwendet, um die Verteilung jeder einzelnen Altersgruppe sowie die Übergangspaare von zwei benachbarten Gruppen zu modellieren. Das altersdiskriminierende Netz (Age Discriminative Network) hilft bei der Erzeugung von Bildern, die von den echten nicht zu unterscheiden sind. Das bedingte Transformationsnetzwerk (Conditional Transformation Network) transformiert das Gesicht der Eingabe in das gewünschte Alter. Das diskriminierende Netz für Übergangsmuster (Transition Pattern Discriminative Network) reguliert die erzeugten Bilder, damit sie den den Alterungsregeln entsprechen.</p>
<figure>
<p><img alt="Structure C-GAN" src="../img/Face%20Aging/Structure_c-GANs.png" width="400" />
  </p>
<figcaption>Struktur des vorgeschlagenen C-GANs.</figcaption>
</figure>
<p>In der nächsten Abbildung werden die generierten Gesichter qualitativ verglichen mit der Grundwahrheit dargestellt. In jedem Triplett sind das erste und dritte Bild die Grundwahrheiten, jeweils in Altersgruppe 1 und Altersgruppe 2, während das zweite Bild das generierte Alterungsergebnis ist.</p>
<figure>
<p><img alt="Results Contextual GAN" src="../img/Face%20Aging/Context-GAN_Results.png" width="800" />
  </p>
<figcaption>Ergebnisse Contextual GANs</figcaption>
</figure>
<h4 id="face-aging-with-identity-preserved-conditional-generative-adversarial-networks">Face Aging With Identity-Preserved Conditional Generative Adversarial Networks</h4>
<p>In diesem <a href="https://ieeexplore.ieee.org/document/8578926">Paper</a>, welches 2018 veröffentlicht wurde, schlug das Autorenteam ein Identity-Preserved Conditional Generative Adversarial Network (IPCGAN) - zu Deutsch: identitätserhaltendes bedingtes generatives adversariales Netzwerk - für die Gesichtsalterung vor. Dieses besteht aus drei Modulen. Einem Conditional Generative Adversarial Network (CGAN), einem identitätserhaltenden (Identity-Preserved) Modul und einem Altersklassifikator.</p>
<p>Als Eingabe für den Generator im IPCGAN wird ein Eingabebild und eine Zielaltersklasse verwendet. Es wird versucht ein Gesicht mit dem Alter in der Zielaltersklasse zu erzeugen. Das generierte Gesicht soll sich nicht von realen Gesichtern in der Zielaltersgruppe unterscheiden. Um die Identitätsinformationen zu erhalten, wird ein Wahrnehmungsverlust (Perceptual Loss) eingeführt und um zu garantieren, dass die synthetisierten Gesichter in die Zielaltersgruppe fallen, werden die erzeugten gealterten Gesichter an einen vortrainiertern Alterklassifikator übergeben und ein Altersklassifikationsverlust hinzugefügt.   </p>
<figure>
<p><img alt="IPCGAN" src="../img/Face%20Aging/IPCGAN.png" width="600" />
  </p>
<figcaption>Struktur des vorgeschlagenen IPCGAN.</figcaption>
</figure>
<p>Das folgende Bild zeigt ein generiertes Beispielbild des IPCGAN Ansatzes.</p>
<figure>
<p><img alt="IPCGAN" src="../img/Face%20Aging/IPCGAN_Result.png" width="600" />
  </p>
<figcaption>Generiertes Beispiel des IPCGAN</figcaption>
</figure>
<h4 id="learning-face-age-progression-a-pyramid-architecture-of-gans">Learning Face Age Progression: A Pyramid Architecture of GANs</h4>
<p>2019 veröffentlichten die Autoren Yang et. al. ein Paper mit dem Titel <a href="https://arxiv.org/abs/1711.10352">"Learning Face Age Progression: A Pyramid Architecture of GANs"</a>, in dem ein neuartiger Ansatz zur Alterung von Gesichtern vorgeschlagen wurde. Hierbei werden die Vorteile von Generative Adversarial Networks (GAN) bei  der Synthese visuell plausibler Bilder mit Vorwissen über die menschliche Alterung verbunden. Die Autoren versprechen, dass ihr Modell im Vergleich zu bestehenden Methoden in der Literatur besser in der Lage ist, die beiden kritischen Anforderungen bei der Altersentwicklung zu erfüllen, d.h. Identitätsbeständigkeit und Alterungsgenauigkeit.</p>
<p>In dieser Methode nimmt der Convolutional Neural Network (CNN) basierte Generator junge Gesichter als Input und lernt eine Zuordnung zu einem Bereich, der älteren Gesichtern entspricht. Um Alterungseffekte zu erzielen und gleichzeitig personenspezifische Informationen beizubehalten, wird ein zusammengesetzter Loss verwendet.</p>
<figure>
<p><img alt="Pyramid GAN" src="../img/Face%20Aging/Pyramid_GAN.png" width="600" />
  </p>
<figcaption>Framework der vorgeschlagenen Methode der Altersprogression.</figcaption>
</figure>
<p>Beispiele für die Ergebnisse der Altersentwicklung sind in der nächsten Abbildung zu sehen. Es werden visuell plausible und überzeugende Alterungseffekte erzielt, obwohl die Beispiele eine breite Palette von Personen in Bezug auf Ethnie, Geschlecht, Pose, Make-up und Ausdruck abdecken.</p>
<figure>
<p><img alt="Results Pyramid GAN" src="../img/Face%20Aging/Pyramid-GAN_Results.png" width="800" />
  </p>
<figcaption>Alterungseffekte, die für die CACD (die ersten beiden Zeilen) und MORPH (die letzten beiden Zeilen) Datensätze für 12 verschiedene Personen generiert wurden.</figcaption>
</figure>
<h4 id="triple-gan-progressive-face-aging-with-triple-translation-loss">Triple-GAN: Progressive Face Aging with Triple Translation Loss</h4>
<p>Mit dem Triple-GAN aus dem Jahr 2020 wollten die Autoren die Probleme bei der Gesichtsalterung mittels Deep-Learning-Methoden lösen. Konkret wollten sie die nicht zufriedenstellenden Ergebnisse in der gewünschten Altersgruppe, die Ignorierung der fortschreitenden Veränderung der Altersmuster und die Identitätserhalutng im synthetisierten Bild lösen. Dazu erforschten sie, wie man verschiedene Altersmuster gleichzeitig übersetzen kann, um mehrere Trainingsphasen für kontradiktorisches Lernen zu erhalten. Das Discriminator-Netzwerk wurde so angepasst, dass es nicht nur auf der Ebene von echt und falsch unterscheidet, sondern auch effiziente Zuordnungen zwischen Mustern und Bezeichnungen erstellt, indem verschiedene Altersmuster gemeinsam erlernt wurden. Um die Altersbeziehungen zwischen den verschiedenen Altersgruppen zu modellieren, wurde die Leistung des Generators verbessert und zusätzlich eine dreifache Überstzung (Triple-Translation) hinzugefügt. Diese hilft dabei, das synthetisierte Gesicht eines bestimmten Alters in ein anderes Alter zu übersetzern. </p>
<figure>
<p><img alt="Triple GAN" src="../img/Face%20Aging/Triple_GAN.png" width="400" />
  </p>
<figcaption>Pipeline für die dreifache Überstzung (Triple Translation)</figcaption>
</figure>
<p>Durch die Verwendung eines dreifachen Übersetzungsverlust (Triple Translation Loss) werden verschiedene synthetisierte Gesichter der gleichen Zielaltersgruppe gezwungen, hohe Ähnlichkeit aufzuweisen. So kann die Übersetzung von Altersmustern korreliert werden, um progressive und kontinuierliche Veränderungen in der Gesichtsalterung besser zu simulieren.</p>
<p>Das Framework der Autoren beinhaltet vier Komponenten:</p>
<ol>
<li>Generator Netzwerk</li>
<li>Pre-Trained Identity-Preserved Netzwerk</li>
<li>Pre-Trained Age Classification Netzwerk</li>
<li>Discriminator Netzwerk</li>
</ol>
<figure>
<p><img alt="Framework Triple GAN" src="../img/Face%20Aging/Triple_GAN_Framework.png" width="400" />
  </p>
<figcaption>Framework des vorgeschlagenen Triple-GAN für Gesichtsalterung</figcaption>
</figure>
<p>Die Ergebnisse des Triple-GANs in der folgenden Abbildung zeigen, dass die generiertern Bilder einen offensichtlichen Alterungseffekt und eine gut erhaltene Identität vorweisen können.</p>
<figure>
<p><img alt="Results Triple GAN" src="../img/Face%20Aging/Triple-GAN_Results.png" width="800" />
  </p>
<figcaption>Ergebnisse Triple-GAN</figcaption>
</figure>
<h4 id="only-a-matter-of-style-age-transformation-using-a-style-based-regression-model">Only a Matter of Style: Age Transformation Using a Style-Based Regression Model</h4>
<p>Mit dem Paper <a href="https://arxiv.org/abs/2102.02754">Only a Matter of Style: Age Transformation Using a Style-Based Regression Model</a> stellten die Autoren eine Implementierung namens SAM -Style-based Age Manipulation - vor. Hierbei versuchten sie die gewünschte Altersveränderung zu erfassen und gleichzeitig die Identität zu bewahren. Die Gesichtsalterung wurde dabei durch eine Bild-zu-Bild Übersetzung (Image-to-Image Translation) versucht gelöst zu werden. Zu dieser Technik gehören auch die Conditional GANs, die wir weiter oben bereits erwähnt haben. In der Forschungsarbeit wird ein vortrainierter (pre-trained) StyleGAN-Generator mit einer Encoder-Architektur kombiniert. Der Encoder hat die Aufgabe, ein Gesichtsbild als Eingabe direkt in eine Reihe von Stilvektoren zu kodieren, die der gewünschten Altersveränderung unterliegen. Diese Vektoren werden anschließend an StyleGAN übergeben, um das Ausgangsbild zu erzeugen, das die gewünschte Altersveränderung darstellt. Der Encoder wird bei der Generierung durch ein vortrainiertes Alterregressionsnetzwerk während des Trainingsprozesses als zusätzliche Einschränkung angeleitet. SAM betrachtet die menschliche Alterung also als ein Regressionsproblem auf das gewünschte Zielalter hin.</p>
<figure>
<p><img alt="SAM Architecture" src="../img/Face%20Aging/SAM_Architecture.png" width="600" />
  </p>
<figcaption>Architektur des SAM Netzwerks</figcaption>
</figure>
<figure>
<p><img alt="SAM Results" src="../img/Face%20Aging/SAM_Results.png" width="600" />
  </p>
<figcaption>Mit SAM erzeugte Alterungsergebnisse</figcaption>
</figure>
<p>Die Ergebnisse dieser Methode werden durch die Style-Repräsentation bestimmt. D.h. sie ist auf Bilder beschränkt, die genau in den latenten Raum von StyleGAN eingebettet werden können. Die Modellierung von Gesichtern, die außerhalb des StyleGAN-Bereichs liegen, kann daher eine Herausforderung darstellen. Ebenso kann es durch die Einbettung eines Bildes in eine Reihe von Vektoren schwieriger werden, die Eingangsmerkmale wie den Bildhintergrund originalgetreu zu erhalten. In den Evaluierungen wurde gezeigt, dass die vorgeschlagene Methode das Alter und andere Merkmale wie Haarfarbe und Frisur erfolgreich voneinander trennt. Allerdings ändern sich solche Attribute natürlich mit dem Alter. Um diese Veränderungen zu modellieren, wurden daher zwei Bearbeitungstechniken zur Kontrolle globaler Veränderungen (z. B. Haarfarbe) und lokaler Veränderungen (z. B. das Vorhandensein von Brillen und Gesichtsbehaarung) vorgeschlagen. Die Erfassung komplexerer Veränderungen, wie z. B. zurückweichende Haarlinien und Veränderungen der Hautfarbe, ist mit diesert Methode aber nach wie vor eine Herausforderung.</p>
<h4 id="pfa-gan-progressive-face-aging-with-generative-adversarial-network">PFA-GAN: Progressive Face Aging With Generative Adversarial Network</h4>
<p>Die Autoren von diesem <a href="https://arxiv.org/abs/2012.03459">Paper</a> nutzen die Tatsache, dass Gesichter im Laufe der Zeit fortlaufend altern und modellieren den Alterungsprozess im Gesicht daher in einer progressiven Weise mit ihrem neuen progressive Face-Aging-Framework, das auf einem GAN basiert (Progressive Face Aging with Generative Adversarial Network - PFA-GAN). Dieses besteht aus mehreren kleinen Generator-Subnetzwerken, die sich jeweils nur  mit spezifischen Alterungseffekten zwischen zwei angrenzenden Altersgruppen befassen. Der Hauptunterschied zu anderen GAN-basierten Methoden besteht darin, dass das PFA-GAN die Subnetze gleichzeitig trainiert. Frühere GAN-Varianten trainierten verschiedene Netzwerke unabhängig voneinander. </p>
<p>Die Autoren heben dabei die Bedeutung der folgenden vier Aspekte für eine progressive Modellierung der Gesichtsalterung hervor:</p>
<ol>
<li>Konzentration auf die Modellierung von Gesichtsalterungseffekten zwischen zwei angrenzenden Altersgruppen</li>
<li>Fortlaufende Alterungsergebnisse durch das durchgängige Trainieren der progressiven Gesichtsalterung</li>
<li>Verbesserung der Alterungsglätte durch eine ordinale Beziehung zwischen den Altersgruppen</li>
<li>Die Leistung der Cross-Age-Verifizierung kann verbessert werden</li>
</ol>
<p>Die folgende Abbildung zeigt, dass das <span class="arithmatex">\(i\)</span>-te Teilnetzwerk <span class="arithmatex">\(G_i\)</span> dazu dient, Gesichter von der Altersgruppe <span class="arithmatex">\(i\)</span> zur Gruppe <span class="arithmatex">\(i + 1\)</span> zu altern.</p>
<figure>
<p><img alt="PFA-GAN Subnetworks" src="../img/Face%20Aging/PFA-GAN_Subnetworks.png" width="800" />
  </p>
<figcaption>Der vorgeschlagene PFA-GAN für die Gesichtsalterung mit 4 Altersgruppen</figcaption>
</figure>
<p>Der progressive Alterungsrahmen von der Ausgangsaltersgruppe bis zur Zielaltersgruppe <span class="arithmatex">\(t\)</span> lässt sich wie folgt formulieren:</p>
<div class="arithmatex">\[X_t = \overline{G}_{t-1} \circ \overline{G}_{t-2} \circ \dots \circ \overline{G}_{s}(X_s)\]</div>
<p>In den Generator-Netzwerken werden zusätzlich <a href="https://arxiv.org/abs/1512.03385">Residual-Skip-Verbindung</a> genutzt. Diese verhindern, dass die exakte Kopie des Gesichtes des Eingabebilds über mehrere Subnetze hinweg gespeichert wird. Durch die Einführung der Skip-Verbindung kann die Zielaltersgruppe leicht in eine Sequenz von binären Gattern umgewandelt werden, die den Alterungsfluss steuern. </p>
<p>Die Veränderungen von der Altersgruppe <span class="arithmatex">\(i\)</span> zu <span class="arithmatex">\(i + 1\)</span> lässt sich mathematisch wie folgt beschreiben:</p>
<div class="arithmatex">\[ X_{t+1} = \overline{G}_{i}(X_i) = X_i + \lambda_i G_i (X_i) \]</div>
<p>Somit besteht ein Generator-Subnetzwerk aus einer Resudial-Skip-Verbindung, einem binären Gatter und dem Netzwerk an sich. Bei <span class="arithmatex">\(\lambda_i \in \{0,1\}\)</span> handelt es sich um das binäre Gatter, welches kontrolliert ob das Subnetzwerke <span class="arithmatex">\(G_i\)</span> in den Alterungsprozess zum jeweiligen Zielalter mit einbezogen wird.</p>
<p>Mit dem vorgeschlagenen Framework lässt sich die Altersprogression, z.B. von der Altersgruppe 1 bis 4 wie in der obigen Abbildung dargestellt, wie folgt ausdrücken:</p>
<div class="arithmatex">\[
\begin{equation}
X_4 = X_3 + \underbrace{\lambda_3 G_3(X_3)}_\text{Alterseffekte Gruppe 3 bis 4} \\
= X_2 + \underbrace{\lambda_2 G_2(X_2) + \lambda_3 G_3(X_3)}_\text{Alterseffekte Gruppe 2 bis 4}
\end{equation} \\
= X_1 + \underbrace{\lambda_1 G_1(X_1) + \lambda_2 G_2(X_2) + \lambda_3 G_3(X_3)}_\text{Alterseffekte Gruppe 1 bis 4}
\]</div>
<p>Wenn jetzt die Alterung von Gruppe 2 nach Gruppe vorhergesagt werden soll, so reduziert sich die obige Gleichung auf <span class="arithmatex">\(X_3 = X_2 + G_2(X_2)\)</span>. Der Vektor für <span class="arithmatex">\(\lambda\)</span> für diese Generierung lautet folgendermaßen <span class="arithmatex">\(\begin{pmatrix}0 &amp; 1 &amp; 0\end{pmatrix}\)</span> und somit werden die Subnetze <span class="arithmatex">\(G_1\)</span> und <span class="arithmatex">\(G_3\)</span> bei der Berechnung außenvorgelassen.</p>
<p>Schließlich können wir den Alterungsprozess vom einem Eingabe-Gesicht <span class="arithmatex">\(X_s\)</span> von einer gegebenen Altersgruppe <span class="arithmatex">\(s\)</span> hin zu einer Zielaltersgruppe <span class="arithmatex">\(t\)</span> wie folgt formulieren:</p>
<div class="arithmatex">\[X_t = G(X_s, \lambda_{s:t})\]</div>
<p><span class="arithmatex">\(G = \overline{G}_{N-1} \circ \overline{G}_{N-2} \circ \dots \circ \overline{G}_{1}\)</span> beschreibt das progressive Gesichtsalterungsnetzwerk. <span class="arithmatex">\(\lambda_{s:t}\)</span> kontrolliert den Alterungsprozess.</p>
<p>Zusätzlich zum Generator und Diskriminator Netzwerk, die Hauptbestandteile von GANs sind, wird im PFA-GAN noch ein weiteres Netzwerk verwendet. Hierbei handelt es sich um ein Altersschätzungsnetzwerk (Age Estimation Network). Es dient dazu, die Gesichtsaltersverteilung für eine verbesserte Altersgenauigkeit besser zu charakterisieren. In früheren Arbeiten wurde in der Regel entweder die Alterklassifikation oder die Altersregression verwendet, um zu überprüfen ob das erzeugte Gesicht zur Zielaltersgruppe gehört. In dieser Implementierung verwendeten die Autoren den <a href="https://link.springer.com/article/10.1007/s11263-016-0940-3">Deep Expectation (DEX)</a> Ansatz. Das Altersschätzungsnetzwerk wurde vortrainiert und die erzielten Gewichte eingefroren. Es reguliert den Generator für eine verbesserte Alterungsgenauigkeit.</p>
<p>Die folgende Abbildung zeigt die komplette Architektur des vorgeschlagenen PFA-GANs.</p>
<figure>
<p><img alt="PFA-GAN" src="../img/Face%20Aging/GAN_Framework_for_PFA-GAN.png" width="600" />
  </p>
<figcaption>Das GAN Framework für das PFA-GAN</figcaption>
</figure>
<p>Bei diesem Ansatz werden verschiedene Losses (Verlustberechnungen) kombiniert, um die folgenden Anforderungen für die Gesichtsalterung berücksichtigen:</p>
<ol>
<li>Adversarial Loss zielt darauf ab, qualitativ hochwertige, gealterte Gesichter zu erzeugen, die nicht von echten zu unterscheiden sind</li>
<li>Der Verlust der Altersschätzung soll die Alterungsgenauigkeit verbessern</li>
<li>Der Verlust der Identitätskonsistenz zielt darauf ab, die gleiche Identität zu bewahren</li>
</ol>
<p>Bei einem jungen Gesicht <span class="arithmatex">\(X_s\)</span> aus der Altersgruppe <span class="arithmatex">\(s\)</span> ist das Ergebnis von <span class="arithmatex">\(G\)</span> von <span class="arithmatex">\(s\)</span> zu einer alten Altersgruppe <span class="arithmatex">\(t\)</span> <span class="arithmatex">\(G(X_s, \lambda_{s:t})\)</span>. Im Kontext von GANs der kleinsten Quadrate ist der gegnerische Verlust für den Generator <span class="arithmatex">\(G\)</span> somit definiert als:</p>
<div class="arithmatex">\[L_{\text{adv}} = \frac{1}{2} \mathbb{E}_{X_{s}} [D([G(X_s,\lambda_{s:t});C_t]) - 1]^2\]</div>
<p>Der Altersschätzverlust zwischen dem geschätzten Alter <span class="arithmatex">\(\hat{y}\)</span> und dem Zielalter <span class="arithmatex">\(y\)</span> für den Generator <span class="arithmatex">\(G\)</span> ist definiert als:</p>
<div class="arithmatex">\[L_{\text{age}} = \mathbb{E}_{X_{s}} [ || y - \hat{y} ||_2 + l(A(X)W, c_t) ]\]</div>
<p><span class="arithmatex">\(W \in \mathbb{R}^{101 \times N}\)</span> bezeichnet die letzte vollständig verbundene Schicht für das Altersgruppenklassifizierungsnetzwerks und <span class="arithmatex">\(l\)</span> ist der Verlust der Kreuzentropie für die Altersgruppenlkassifizierung.</p>
<p>Um die identitätsbezogenen Informationen des Gesichts zu bewahren und die identitätsirrelevanten Informationen wie den Hintergrund unverändert zu lassen, wird ein gemischter Identitätskonsistenzverlust zwischen dem Eingabegesicht und dem generierten Gesicht verwendet. Hierzu zählen:</p>
<ul>
<li>ein pixelweiser Verlust (pixel-wise loss)</li>
<li>ein Verlust für die strukturelle Ähnlichkeit (<a href="https://ieeexplore.ieee.org/document/1284395">Structural Similarity (SSIM) loss</a>)</li>
<li>ein Feature-Level Loss</li>
</ul>
<p>Diese drei sind wie folgt definiert:</p>
<div class="arithmatex">\[L_{\text{pix}} = \mathbb{E}_{X_{s}} | G(X_s, \lambda_{s:t}) - X_s |\]</div>
<div class="arithmatex">\[L_{\text{ssim}} = \mathbb{E}_{X_{s}} [ 1- \text{SSIM}(G(X_s, \lambda_{s:t}), X_s]\]</div>
<div class="arithmatex">\[L_{\text{fea}} = \mathbb{E}_{X_{s}} || \phi(G(X_s, \lambda_{s:t})) - \phi(X_s) ||_{F}^2 \]</div>
<p>Schließlich kann der Identitätskonsistenz Verslust (Identity Consistency Loss) für das Generator-Netzwerk definiert werden als:</p>
<div class="arithmatex">\[L_{\text{fea}} = (1 - \alpha_{\text{ssim}}) * L_{\text{pix}} + \alpha_{\text{ssim}} * L_{\text{ssim}} + \alpha_{\text{fea}} * L_{\text{fea}}\]</div>
<p><span class="arithmatex">\(\alpha_{\text{ssim}}\)</span> und <span class="arithmatex">\(\alpha_{\text{fea}}\)</span> sind Hyperparameter, die dazu dienen die Balance zwischen den drei Verlusten zu kontrollieren.</p>
<p>Der finale Verlust für den Generator ergibt sich aus der Zusammensetzung aller einzelnen Verlustberechnungen:</p>
<div class="arithmatex">\[L_G = \lambda_{\text{adv}} L_{\text{adv}} + \lambda_{\text{age}} L_{\text{age}} + \lambda_{\text{ide}} L_{\text{ide}}\]</div>
<p>Die einzelnen <span class="arithmatex">\(\lambda\)</span> dienen ebenfalls wieder als Hyperparameter.</p>
<p>Die folgende Darstellung zeigt Beispielergebnisse zur Gesichtsalterung und -verjüngung durch Anwendung des PFA-GANs auf drei externe Datensätze:</p>
<ul>
<li><a href="https://yanweifu.github.io/FG_NET_data/">FG-NET</a></li>
<li><a href="https://ieeexplore.ieee.org/document/7410782">CelebA</a></li>
<li><a href="https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/">IMDB-WIKI</a></li>
</ul>
<p>Die roten Kästschen kennzeichenen die Einbagebilder.</p>
<figure>
<p><img alt="Results PFA-GAN" src="../img/Face%20Aging/PFA-GAN_Results.png" width="800" />
  </p>
<figcaption>Generierte Gesichter durch das PFA-GAN</figcaption>
</figure>
<p>Trotz qualitativer und quantitativer Überlegenheit des PFA-GANs gegenüber vorhergehenden Methoden, bestehen auch einige Einschränkungen:</p>
<ul>
<li>Die Haupteinschränkung der GAN-basierten Methoden besteht im Vergleich zu cGAN-basierten Methoden darin, dass die Netzwerke als Eingabe die Alterskennzeichnung der Quelle benötigen</li>
<li>Das PFA-GAN muss ein zweites, umgekehrtes Modell für die Gesichtsverjüngung trainieren, währed andere Methoden das gleiche Netzwerk für Alterung und Verjüngung nutzen können</li>
<li>mit mehr Altersgruppen oder einer kleinen Zeitspanne in jeder Altersgruppe wird es bei der Gesichtsalterung schwieriger, ein Gesichtsalterungsmodell zu trainieren, und die Muster zwischen zwei benachbarten Altersgruppen werden weniger klar, was fast alle Methoden gleichermaßen vor Herausforderungen stellt</li>
</ul>
<h2 id="anwendungen">Anwendungen</h2>
<p>Die meisten Anwendungen findet man aktuell im Bereich der Unterhaltung. Man findet unzählige Apps für das Smartphone, sowohl für iOS als auch für Android, welche ein älteres Bild einer Person generieren lassen.</p>
<h3 id="smokerface-app">Smokerface App</h3>
<p>Aber auch sinnvollere Anwendungen wurden schon umgesetzt. In der Studie <a href="https://pubmed.ncbi.nlm.nih.gov/30111525/">A Face-Aging App for Smoking Cessation in a Waiting Room Setting: Pilot Study in an HIV Outpatient Clinic</a> wurde eine Face-Aging App zur Intervention und Raucherentwöhnung entwickelt. Hintergrund zu dieser Studie war, dass die Einführung von Technologien zur Raucherentwöhnung in ambulanten Wartezimmern eine wirksame Strategie für eine Veränderung sein kann, die das Potenzial hat, fast alle Patienten die einen Gesundheitsdienstleister aufsuchen, zu erreichen, ohne dass der Arzt vorher tätig werden muss. Das Ziel der Studie war es, eine Intervention zur Raucherentwöhnung zu entwickeln, die Patienten während Wartezeiten passiv einer Tablet-basierten App mit Gesichtsveränderung und öffentlichem Morphing aussetzt. Diese Intervention wurde in einem Wartezimmer einer HIV-Ambulanz getestet und die Wahrnehmung dieser Intervention unter rauchenden und nicht rauchenden HIV-Patienten gemessen. Dabei entwickelte das Team eine Kioskversion der dreidimensionalen Gesichtsalterungs-App Smokerface, die dem Benutzer zeigt, wie sein Gesicht mit oder ohne Zigarettenrauchen in 1 bis 15 Jahren aussehen würde. Es wurde ein Tablet mit der App auf einem Tisch in der Mitte des Wartezimmers platziert, verbunden mit einem großen Monitor, der an einer gegenüberliegenden Wand angebracht war. Ein Forscher notierte alle Patienten, die den Warteraum nutzten. Wenn ein Patient die App nicht innerhalb von 30 Sekunden nach Betreten nutzte, forderte der Forscher ihn auf, dies zu tun. Die Nutzer wurden danach gebeten, einen Fragebogen auszufüllen. Die Studie kam zum Schluss, dass eine im Wartezimmer implementierte Face-Aging-App eine neuartige Möglichkeit bietet, Patienten, die einen Gesundheitsdienstleister aufsuchen, dazu zu motivieren, mit dem Rauchen aufzuhören, die Raucherentwöhnung bei ihrem nächsten Termin anzusprechen und dadurch die ärztlich verordnete Raucherentwöhnung zu fördern.</p>
<p>Nachfolgend der Link um die App auf dem eigenen Gerät zu testen:</p>
<ul>
<li><a href="https://play.google.com/store/apps/details?id=com.agt.smokerface&amp;hl=de">Android</a></li>
<li><a href="https://apps.apple.com/de/app/smokerface/id946861642">iOS</a></li>
</ul>
<h3 id="sunface-uv-selfie">Sunface - UV-Selfie</h3>
<p>In einer weiteren Studie mit dem Titel <a href="https://pubmed.ncbi.nlm.nih.gov/32374352/">Effect of a Face-Aging Mobile App-Based Intervention on Skin Cancer Protection Behavior in Secondary Schools in Brazil: A Cluster-Randomized Clinical Trial</a> wurde untersucht, wie sich eine kostenlose mobile Gesichtsalterungs-App mit den Namen Sunface auf das Hautkrebsschutzverhalten von Jugendlichen auswirkt. Da der Kontakt mit UV-Strahlung in jungen Jahren ein wichtiger Risikofaktor für die Entstehung von Melanomen ist, ist die Reduzierung der UV-Exposition bei Kindern und Jugendlichen von größter Bedeutung. Das primäre Ziel der Studie war der Unterschied in der täglichen Verwendung von Sonnenschutzmitteln bei der Nachbeobachtung nach 6 Monaten. Zu den sekundären Zielen gehörten der Unterschied bei der täglichen Verwendung von Sonnenschutzmitteln nach 3 Monaten Nachbeobachtung, mindestens eine Selbstuntersuchung der Haut innerhalb von 6 Monaten und mindestens eine Bräunungssitzung in den vorangegangenen 30 Tagen. Alle Analysen wurden im Voraus festgelegt und basierten auf der Absicht, die Studie zu behandeln. Clustereffekte wurden berücksichtigt. Die Ergebnisse dieser Studie deuten darauf hin, dass Interventionen auf der Grundlage von Apps zur Gesichtsalterung das Hautkrebsschutzverhalten brasilianischer Jugendlicher verbessern können.</p>
<p>Auch diese App kann selbst getestet werden:</p>
<ul>
<li><a href="https://play.google.com/store/apps/details?id=com.agt.sunface&amp;hl=de">Android</a></li>
<li><a href="https://apps.apple.com/de/app/sunface-uv-selfie/id1226606410?l=en">iOS</a></li>
</ul>
<h3 id="aprilage-inc">AprilAge Inc.</h3>
<p>Auch das Unternehmen <a href="https://aprilage.com/">AprilAge</a> entwickelt Gesichts- und Körpervisualisierungssoftware für verschiedene Unternehmen, die Menschen dazu bewegen und motivieren sollen, riskante Lebensgewohnheiten zu ändern, die zu chronischen Krankheiten und hohen Behandlungskosten führen. Die Software zeigt den Einfluss von Rauchen, erhöhter Sonnenbestrahlung und Übergewicht auf den Alterungsprozess des Gesichtes.</p>
<h2 id="fazit_1">Fazit</h2>
<p>Generative Adversarial Networks in verschiedenen Implementierungen lösten die herkömmlichen Methoden, physikalisch-modellbasierte und prototypbasierte, ab. Mit den neuen neuronalen Netzwerken benötigte man nicht mehr die große Menge an kostspielig zu erfassenden Datensätze. Der datengetriebene Ansatz konnte mit den Alterungsverläufen besser umgehen.</p>
<p>Im Zuge der Forschung in anderen Bereichen, z.B. Face Recognition, wurden verschiedene Datensätze weiterentwickelt und neu erstellt. Diese können problemlos für das Thema Face Aging verwendet werden. </p>
<p>Seit der Einführung der GANs im Jahr 2014 wurden unterschiedliche Implementierungen auf deren Basis umgesetzt. Sie versuchten Bilder zu generieren, welche die Identität der Person bei der Alterung erhalten sollen. Dazu wurden verschiedene Techniken verwendet. Die generierten Ergebnisse der neuen neuronalen Netzwerke auf den Testdaten waren kaum von echten Bildern zu unterscheiden. Trotzdem wird die Erzeugung qualitativ hochwertiger Bilder bei extremen Posen, anspruchsvollen Ausdrücken und/oder Accessoires in Bildern unabhängig von den Trainings- oder Testdaten erschwert.</p>
<p>Theoretische Bereiche der Anwendung gibt es zahlreiche. Wirklich viele Anwendungen findet man im Bereich der Unterhaltung. In verschiedenen Apps wird Face Aging als lustiger Filter angeboten, um Freunden das ältere Ich als kleiner Scherz für Zwischendurch zu zeigen. Zwei Studien und eine Firma mit Anwendungen im Bereich der Medizin haben wir im Report vorgestellt. Weitere Applikationen z.B. bei der Hilfe der Bekämpfung des Menschenhandels wären wünschenswert.</p>
<p>Die Forschung auf diesem Gebiet bleibt spannend. Neuartige Methoden im Bereich des maschinellen Lernens könnten die Generierung qualitativ hochwertiger Bilder für die Gesichtsalterung weiterhin verbessern. </p>
<h2 id="weiterfuhrendes-material">Weiterführendes Material</h2>
<h3 id="talk">Talk</h3>
<p>Hier einfach Youtube oder THD System embedden.</p>
<h3 id="demo">Demo</h3>
<p>Hier Link zum Demo Video.</p>
<p>Hier der Link zum <a href="https://github.com/julian-steiner-ai/face-aging">GitHub Repository</a> der Code-Demonstration.</p>
<h2 id="literaturliste">Literaturliste</h2>
<h3 id="datensatze_1">Datensätze</h3>
<ul>
<li><a href="https://ieeexplore.ieee.org/document/1613043">K. Ricanek and T. Tesafaye, "MORPH: a longitudinal image database of normal adult age-progression," 7th International Conference on Automatic Face and Gesture Recognition (FGR06), Southampton, UK, 2006, pp. 341-345, doi: 10.1109/FGR.2006.78.</a></li>
<li><a href="https://ieeexplore.ieee.org/document/879790">P. J. Phillips, Hyeonjoon Moon, S. A. Rizvi and P. J. Rauss, "The FERET evaluation methodology for face-recognition algorithms," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no. 10, pp. 1090-1104, Oct. 2000, doi: 10.1109/34.879790.</a></li>
<li><a href="https://yanweifu.github.io/FG_NET_data/">FG-NET dataset by Yanwei Fu</a></li>
<li><a href="https://bcsiriuschen.github.io/CARC/">Bor-Chun Chen, , Chu-Song Chen, and Winston H. Hsu. "Cross-Age Reference Coding for Age-Invariant Face Recognition and Retrieval." . In Proceedings of the European Conference on Computer Vision (ECCV).2014.</a></li>
<li><a href="https://ieeexplore.ieee.org/document/993553">A. Lanitis, C. J. Taylor and T. F. Cootes, "Toward automatic simulation of aging effects on face images," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 24, no. 4, pp. 442-455, April 2002, doi: 10.1109/34.993553.</a></li>
<li><a href="https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/">IMDB-WIKI – 500k+ face images with age and gender labels</a></li>
<li><a href="https://vis-www.cs.umass.edu/lfw/">Gary B. Huang, , Manu Ramesh, Tamara Berg, and Erik Learned-Miller. 2007. "Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments." University of Massachusetts, Amherst, Invalid Date 2007.</a></li>
</ul>
<h3 id="physikalische-modellbasierte-methoden_1">Physikalische modellbasierte Methoden</h3>
<ul>
<li><a href="https://ieeexplore.ieee.org/document/4782970">J. Suo, S. -C. Zhu, S. Shan and X. Chen, "A Compositional and Dynamic Model for Face Aging," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 3, pp. 385-401, March 2010, doi: 10.1109/TPAMI.2009.39.</a></li>
<li><a href="https://link.springer.com/article/10.1007/s11042-013-1399-7">Tsai, MH., Liao, YK. &amp; Lin, IC. Human face aging with guided prediction and detail synthesis. Multimed Tools Appl 72, 801–824 (2014). https://doi.org/10.1007/s11042-013-1399-7</a></li>
<li><a href="https://www.jstor.org/stable/24966262">Todd, James T., Leonard S. Mark, Robert E. Shaw, and John B. Pittenger. “The Perception of Human Growth.” Scientific American 242, no. 2 (1980): 132–45. http://www.jstor.org/stable/24966262.</a></li>
<li><a href="https://pubmed.ncbi.nlm.nih.gov/22997125/">Suo, J., Chen, X., Shan, S., Gao, W., &amp; Dai, Q. (2012). A concatenational graph evolution aging model. IEEE transactions on pattern analysis and machine intelligence, 34(11), 2083–2096. https://doi.org/10.1109/TPAMI.2012.22</a></li>
<li><a href="https://dl.acm.org/doi/10.1145/280814.280823">Douglas DeCarlo, Dimitris Metaxas, and Matthew Stone. 1998. An anthropometric face model using variational techniques. Proceedings of the 25th annual conference on Computer graphics and interactive techniques. Association for Computing Machinery, New York, NY, USA, 67–74. https://doi.org/10.1145/280814.280823</a></li>
<li><a href="https://ieeexplore.ieee.org/document/1640784">N. Ramanathan and R. Chellappa, "Modeling Age Progression in Young Faces," 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), New York, NY, USA, 2006, pp. 387-394, doi: 10.1109/CVPR.2006.187.</a></li>
<li><a href="https://ieeexplore.ieee.org/document/993553">A. Lanitis, C. J. Taylor and T. F. Cootes, "Toward automatic simulation of aging effects on face images," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 24, no. 4, pp. 442-455, April 2002, doi: 10.1109/34.993553.</a></li>
</ul>
<h3 id="prototypbasierte-methoden_1">Prototypbasierte Methoden</h3>
<ul>
<li><a href="https://ieeexplore.ieee.org/document/6909822">I. Kemelmacher-Shlizerman, S. Suwajanakorn and S. M. Seitz, "Illumination-Aware Age Progression," 2014 IEEE Conference on Computer Vision and Pattern Recognition, Columbus, OH, USA, 2014, pp. 3334-3341, doi: 10.1109/CVPR.2014.426.</a></li>
<li><a href="https://ieeexplore.ieee.org/document/946630">B. Tiddeman, M. Burt and D. Perrett, "Prototyping and transforming facial textures for perception research," in IEEE Computer Graphics and Applications, vol. 21, no. 5, pp. 42-50, July-Aug. 2001, doi: 10.1109/38.946630.</a></li>
<li><a href="https://arxiv.org/abs/1510.06503">Xiangbo Shu, , Jinhui Tang, Hanjiang Lai, Luoqi Liu, and Shuicheng Yan. "Personalized Age Progression with Aging Dictionary." (2015).</a></li>
<li><a href="https://ieeexplore.ieee.org/document/7442560">H. Yang, D. Huang, Y. Wang, H. Wang and Y. Tang, "Face Aging Effect Simulation Using Hidden Factor Analysis Joint Sparse Representation," in IEEE Transactions on Image Processing, vol. 25, no. 6, pp. 2493-2507, June 2016, doi: 10.1109/TIP.2016.2547587.</a></li>
</ul>
<h3 id="deep-generative-networks_1">Deep Generative Networks</h3>
<ul>
<li><a href="https://arxiv.org/abs/2012.03459">Zhizhong Huang, , Shouzhen Chen, Junping Zhang, and Hongming Shan. "PFA-GAN: Progressive Face Aging With Generative Adversarial Network".IEEE Transactions on Information Forensics and Security 16 (2021): 2031–2045.</a></li>
<li><a href="https://ieeexplore.ieee.org/document/5406526">Fu Y, Guo G, Huang TS. Age synthesis and estimation via faces: a survey. IEEE Trans Pattern Anal Mach Intell. 2010 Nov;32(11):1955-76. doi: 10.1109/TPAMI.2010.36. PMID: 20847387.</a></li>
<li><a href="https://arxiv.org/abs/1406.2661">Ian J. Goodfellow, , Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. "Generative Adversarial Networks." (2014).</a></li>
<li><a href="https://arxiv.org/abs/1802.00237">Si Liu, , Yao Sun, Defa Zhu, Renda Bao, Wei Wang, Xiangbo Shu, and Shuicheng Yan. "Face Aging with Contextual Generative Adversarial Nets." (2018).</a></li>
<li><a href="https://arxiv.org/abs/1702.01983">Grigory Antipov, , Moez Baccouche, and Jean-Luc Dugelay. "Face Aging With Conditional Generative Adversarial Networks." (2017). </a></li>
<li><a href="https://ieeexplore.ieee.org/document/9151060">H. Fang, W. Deng, Y. Zhong and J. Hu, "Triple-GAN: Progressive Face Aging with Triple Translation Loss," 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), Seattle, WA, USA, 2020, pp. 3500-3509, doi: 10.1109/CVPRW50498.2020.00410.</a></li>
<li><a href="https://ieeexplore.ieee.org/document/8578926">X. Tang, Z. Wang, W. Luo and S. Gao, "Face Aging with Identity-Preserved Conditional Generative Adversarial Networks," 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 2018, pp. 7939-7947, doi: 10.1109/CVPR.2018.00828.</a></li>
<li><a href="https://arxiv.org/abs/1711.10352">Hongyu Yang, , Di Huang, Yunhong Wang, and Anil K. Jain. "Learning Face Age Progression: A Pyramid Architecture of GANs." (2019). </a></li>
<li><a href="https://arxiv.org/abs/2102.02754">Yuval Alaluf, , Or Patashnik, and Daniel Cohen-Or. "Only a Matter of Style: Age Transformation Using a Style-Based Regression Model." (2021). </a></li>
<li><a href="https://arxiv.org/abs/1411.1784">Mehdi Mirza, , and Simon Osindero. "Conditional Generative Adversarial Nets." (2014).</a></li>
<li><a href="https://arxiv.org/abs/1512.03385">Kaiming He, , Xiangyu Zhang, Shaoqing Ren, and Jian Sun. "Deep Residual Learning for Image Recognition." (2015).</a></li>
<li><a href="https://link.springer.com/article/10.1007/s11263-016-0940-3">Rothe, R., Timofte, R. &amp; Van Gool, L. Deep Expectation of Real and Apparent Age from a Single Image Without Facial Landmarks. Int J Comput Vis 126, 144–157 (2018). https://doi.org/10.1007/s11263-016-0940-3</a></li>
<li><a href="https://ieeexplore.ieee.org/document/1284395">Zhou Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli, "Image quality assessment: from error visibility to structural similarity," in IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600-612, April 2004, doi: 10.1109/TIP.2003.819861.</a></li>
<li><a href="https://ieeexplore.ieee.org/document/7410782">Z. Liu, P. Luo, X. Wang and X. Tang, "Deep Learning Face Attributes in the Wild," 2015 IEEE International Conference on Computer Vision (ICCV), Santiago, Chile, 2015, pp. 3730-3738, doi: 10.1109/ICCV.2015.425.</a></li>
</ul>
<h3 id="anwendungen_1">Anwendungen</h3>
<ul>
<li><a href="https://pubmed.ncbi.nlm.nih.gov/30111525/">Brinker, T. J., Brieske, C. M., Esser, S., Klode, J., Mons, U., Batra, A., Rüther, T., Seeger, W., Enk, A. H., von Kalle, C., Berking, C., Heppt, M. V., Gatzka, M. V., Bernardes-Souza, B., Schlenk, R. F., &amp; Schadendorf, D. (2018). A Face-Aging App for Smoking Cessation in a Waiting Room Setting: Pilot Study in an HIV Outpatient Clinic. Journal of medical Internet research, 20(8), e10976. https://doi.org/10.2196/10976</a></li>
<li><a href="https://pubmed.ncbi.nlm.nih.gov/32374352/">Brinker, T. J., Faria, B. L., de Faria, O. M., Klode, J., Schadendorf, D., Utikal, J. S., Mons, U., Krieghoff-Henning, E., Lisboa, O. C., Oliveira, A. C. C., Lino, H. A., &amp; Bernardes-Souza, B. (2020). Effect of a Face-Aging Mobile App-Based Intervention on Skin Cancer Protection Behavior in Secondary Schools in Brazil: A Cluster-Randomized Clinical Trial. JAMA dermatology, 156(7), 737–745. https://doi.org/10.1001/jamadermatol.2020.0511</a></li>
<li><a href="https://aprilage.com/">AprilAge - Face Aging and Body Visualization Software</a></li>
</ul>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.expand", "navigation.indexes"], "search": "../../assets/javascripts/workers/search.74e28a9f.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.220ee61c.min.js"></script>
      
        
          <script src="../../javascripts/katex.js"></script>
        
      
        
          <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.js"></script>
        
      
        
          <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/contrib/auto-render.min.js"></script>
        
      
    
  </body>
</html>