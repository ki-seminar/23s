{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Seminar Aktuelle Themen der k\u00fcnstlichen Intelligenz","text":"<p>Herzlich Willkommen auf der Kurswebseite des Kurses \"Aktuelle Themen der k\u00fcnstlichen Intelligenz\" des Studiengangs K\u00fcnstliche Intelligenz der Technischen Hochschule Deggendorf. </p> <p>Im Seminar aktuelle Themen der KI erarbeiten Studierende zu einem Thema ein Dreiergruppen</p> <ul> <li>eine Podcastepisode f\u00fcr fachfremdes Publikum</li> <li>einen Konferenzvortrag</li> <li>eine Live Demo</li> </ul> <p>zu einem aktuellen Thema im Bereich k\u00fcnstliche Intelligenz. Am Ende werden alle Ergebnisse zusammen mit dem Bericht hier ver\u00f6ffentlicht.</p> <p>Die Ausarbeitungen werden voraussichtlich ab Mitte Juli 2023 hier einsehbar sein.</p>"},{"location":"impressum/","title":"Impressum","text":""},{"location":"impressum/#angaben-gema-5-tmg","title":"Angaben gem\u00e4\u00df \u00a7 5 TMG","text":"<p>Prof. Dr. Florian Wahl Lederergasse 31 94032 Passau</p>"},{"location":"impressum/#postadresse","title":"Postadresse","text":"<p>Prof. Dr. Florian Wahl Technologie Campus Grafenau Hauptstr. 3 94481 Grafenau</p>"},{"location":"impressum/#kontakt","title":"Kontakt","text":"<p>Telefon: +49(0)991/36 15 8257 E-Mail: florian.wahl@th-deg.de</p>"},{"location":"impressum/#redaktionell-verantwortlich","title":"Redaktionell verantwortlich","text":"<p>Prof. Dr. Florian Wahl</p> <p>Quelle: eRecht24</p>"},{"location":"Themen/Einf%C3%A4rben%20von%20Bildern/","title":"Einf\u00e4rben von Bildern - Zwei Ans\u00e4tze","text":"<p>von Simon Drasch, Florian Eder &amp; Moritz Enderle</p>"},{"location":"Themen/Einf%C3%A4rben%20von%20Bildern/#abstract","title":"Abstract","text":"<p>In der heutigen Zeit, in welcher gro\u00dfe Mengen an Bildern leicht zug\u00e4nglich sind, gewinnen die Nutzung und Verarbeitung dieser Daten mit Hilfe von KI an Bedeutung. Wir stellen zwei Methoden vor um das zu erreichen, pix2pix und Deoldify. Wir gliedern unsere Arbeit in drei Bereiche: Podcast, Pr\u00e4sentation mit Code-Demonstration und dieser schriftlichen Ausarbeitung.</p> <p>In unserem Podcast bieten wir einen oberfl\u00e4chlichen \u00dcberblick \u00fcber die verschiedenen Methoden und Anwendungsgebiete der Bildkolorierung. Dabei erkl\u00e4ren wir die Mechaniken so einfach und verst\u00e4ndlich wie m\u00f6glich, um auch Zuh\u00f6rer ohne Vorkenntnisse im Bereich der k\u00fcnstlichen Intelligenz anzusprechen.</p> <p>In der Pr\u00e4sentation bieten wir tiefgehende Einblicke in den Aufbau und die Merkmale von Bildern. Zuerst stellen wir statistische Ans\u00e4tze wie den Mean StD Transfer und Lab Mean Transfer kurz vor. Dar\u00fcber hinaus gehen wir intensiv auf die KI-Systeme Pix2pix und DeOldify ein. Dabei werden die mathematischen Grundlagen hinter den Algorithmen erkl\u00e4rt. Der Fachvortrag richtet sich an ein Fachpublikum mit fortgeschrittenen Kenntnissen im Bereich der k\u00fcnstlichen Intelligenz und liefert detaillierte Informationen \u00fcber die verschiedenen Ans\u00e4tze zur Bildkolorierung. </p> <p>Anschlie\u00dfend wird in der Code Demonstration anhand eines Beispieldatensatzes gezeigt, wie verschiedene Methoden zur Bildkolorierung angewendet werden Dabei wird der Einsatz von Pix2pix und DeOldify demonstriert, die Qualit\u00e4t der Ergebnisse miteinander verglichen und anschlie\u00dfend eine Einsch\u00e4tzung der Effektivit\u00e4t und Genauigkeit der verschiedenen Ans\u00e4tze dargestellt.</p>"},{"location":"Themen/Einf%C3%A4rben%20von%20Bildern/#motivation","title":"Motivation","text":"<p>Unter Bildkolorierung versteht man die Methode, Schwarz-Wei\u00df-Bildern Farben hinzuzuf\u00fcgen. Dadurch kann man ihnen neues Leben einzuhauchen und alte Bilder vollautomatisch und realistisch restaurieren. Sie erm\u00f6glicht es uns, visuelle Informationen und Details zu erfassen, die in Schwarz-Wei\u00df-Bildern verborgen sind, vorallem da die Einbeziehung von Farben die Art und Weise, wie wir Bilder wahrnehmen und interpretieren, erheblich ver\u00e4ndert. Das Hinzuf\u00fcgen von Farbe auf Schwarz-Wei\u00df-Bildern erm\u00f6glicht somit eine Vielzahl von Anwendungen, von der Restaurierung alter Fotografien bis hin zur Verbesserung der visuellen Datenanalyse.</p> <p>Ein Beispiel f\u00fcr den Einsatz von KI-Systemen in der Bildrestaurierung ist die Website Myheritage, welche Ahnenforschung, Stammbaumerstellung und genetische Genealogie betreibt. Dar\u00fcber hinaus bieten sie zus\u00e4tzlich an, alte Familienbilder einzuf\u00e4rben und diese mit anderen zu teilen, damit die Familiengeschichte nicht in Vergessenheit ger\u00e4t. </p> <p>Herausforderungen bei der Bildkolorierung liegen vor allem in der genauen Reproduktion von Farben und der Beibehaltung des urspr\u00fcnglichen Bildcharakters. Neben technische F\u00e4higkeiten muss auch \u00e4sthetisches Verst\u00e4ndnis vorhanden sein, um qualitativ hochwertige Ergebnisse garantieren zu k\u00f6nnen.</p> <p>Um solche Ergebnisse selbst erzeugen zu k\u00f6nnen, werden in den folgenden Abschnitten verschiedene Ans\u00e4tze zur Bildkolorierung vorgestellt, darunter sowohl klassische Methoden als auch moderne KI-Systeme.</p>"},{"location":"Themen/Einf%C3%A4rben%20von%20Bildern/#methoden","title":"Methoden","text":"<p>In der Bildkolorierung werden verschiedene Methoden eingesetzt, die auf statistischen Ans\u00e4tzen und KI-Systemen basieren.</p>"},{"location":"Themen/Einf%C3%A4rben%20von%20Bildern/#statistische-ansatze","title":"Statistische Ans\u00e4tze","text":"<p>Statistische Ans\u00e4tze zur Bildkolorierung verwenden mathematische Modelle, um Farben zu Schwarz-Wei\u00df-Bildern hinzuzuf\u00fcgen. Einf\u00e4rben mit Hilfe von statistischen Modellen ist nicht m\u00f6glich, jedoch wird die Farb\u00fcbertragung eines Referenzbildes auf ein Schwarz-Wei\u00df-Bild oft mit Einf\u00e4rben betitlelt. Drei solcher statistischen Methoden sind:</p> <ul> <li>Mean StD Transfer: In dieser Methode wird das Helligkeits-und Farbniveau des Referenzbildes auf eine normalisierte Version des Schwarz-Wei\u00df-Bildes \u00fcbertragen. Dies wird mit der folgenden mathematischen Formel erreicht:</li> </ul> \\[ \\text{Output} = \\frac{\\text{Input - mean(Input)}}{\\text{std(Input)}} \\times \\text{std(Reference) + mean(Reference)} \\] <ul> <li> <p>Dieser Ansatz ist zwar schnell und funktioniert einigerma\u00dfen okay, wenn man farbige Bilder umf\u00e4rben m\u00f6chte, jedoch wird bei Schwarz-Wei\u00df-Bildern nur die Durchschnittsfarbe des Referenzbildes projeziert.</p> </li> <li> <p>Lab Mean Transfer: Diese Methode funktioniert gleich wie der Mean StD Transfer, allerdings wird zuvor der Farbraum von RGB in Lab \u00fcbertragen. Dadurch werden schon etwas bessere Ergebnisse erzeugt, allerdings sind auch diese bei Schwarz-Wei\u00df-Bildern genau so schlecht. </p> </li> <li> <p>Probability Density Function (PDF) Transfer: In diesem komplexen mathematischen Verfahren werden f\u00fcr beide Bilder ein normalisiertes Histrogramm erstellt, welche dann genutzt werden um die Wahrscheinlichkeitsverteilung von der Farbpalette des Schwarz-Wei\u00df-Bildes (unterschiedliche Graut\u00f6ne) auf die Farbpalette des Referenzbildes zu \u00fcbertragen. Dieser Ansatz liefert die besten Ergebnisse der drei verschiedenen statistischen Ans\u00e4tze, ist jedoch auch der komplexeste und rechenintensivste. Die Ergebnisse sind zwar besser als bei den anderen beiden Ans\u00e4tzen, jedoch sind sie immer noch nicht zufriedenstellend.</p> </li> </ul>"},{"location":"Themen/Einf%C3%A4rben%20von%20Bildern/#ergebnisse","title":"Ergebnisse:","text":"Schwarz-Wei\u00df-Bild Referenzbild Mean StD Transfer Lab Mean Transfer PDF Transfer"},{"location":"Themen/Einf%C3%A4rben%20von%20Bildern/#ki-systeme","title":"KI-Systeme","text":""},{"location":"Themen/Einf%C3%A4rben%20von%20Bildern/#conditional-adversarial-networks-cgan","title":"Conditional Adversarial Networks (cGAN)","text":"<p>Conditional Adversarial Networks (cGAN) sind eine Art von generativen Modellen, die auf dem Konzept der generativen adversariellen Netzwerke (GAN) basieren. GANs bestehen aus zwei neuronalen Netzen, die gegeneinander trainiert werden. Der Generator G versucht, Bilder zu erzeugen, die von einem menschlichen Betrachter nicht von echten Bildern unterschieden werden k\u00f6nnen. Der Diskriminator D versucht, die vom Generator erzeugten Bilder von echten Bildern zu unterscheiden. Sie teilen sich eine Lossfunktion, die den Generator dazu zwingt, bessere Bilder zu erzeugen, und den Diskriminator dazu zwingt, bessere Entscheidungen zu treffen. Sie sieht wie folgt aus:</p> \\[ \\displaystyle \\min_G \\max_D \\text{V(D, G)} = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))] \\] <p>Dabei hat der Generator die Aufgabe, den zweiten Teil der Lossfunktion zu minimieren, w\u00e4hrend der Diskriminator versucht, die vollst\u00e4ndige Funktion zu maximieren.</p> <p>Bei conditional Adversarial Networks wird der Generator und/oder Discriminator zus\u00e4tzlich mit einem Konditionierungsterm erweitert. Dieser Term wird dem Generator als zus\u00e4tzlicher Input \u00fcbergeben und kann beispielsweise ein Label, ein Bild oder eine Zahl bzw Vektor sein. Dadurch kann der Generator Bilder erzeugen, die zu einem bestimmten Label passen. Die Lossfunktion sieht dann wie folgt aus:</p> \\[ \\displaystyle \\min_G \\max_D \\text{V(D, G)} = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x|y)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z|y)))] \\] <p>Hier die beiden Teile der Lossfunktion erkl\u00e4rt: </p> \\[ \\displaystyle \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x|y)] \\] <p>\u27f6 Der Erwartungswert des Diskriminators, dass ein echtes Bild mit Label y als solches klassifiziert wird: D(x|y) soll gegen 1 gehen.</p> \\[ \\displaystyle \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z|y)))] \\] <p>\u27f6 Der Erwartungswert des Diskriminators, dass ein generiertes Bild mit Label y als solches klassifiziert wird: D(G(z)) soll gegen 0 gehen.</p> <p>Man betrachtet also die Wahrscheinlichkeit, dass der Diskriminator ein echtes Bild mit dem Label y von einem generierten Bild mit dem Label y unterscheiden kann. Der Generator versucht, diese Wahrscheinlichkeit zu minimieren, w\u00e4hrend der Diskriminator versucht, sie zu maximieren. Dar\u00fcber hinaus wird der Generator trainiert, Bilder abh\u00e4ngig von einem Label zu erzeugen, daher der Begriff conditional.</p>"},{"location":"Themen/Einf%C3%A4rben%20von%20Bildern/#convolutional-neural-networks-cnn","title":"Convolutional Neural Networks (CNN)","text":"<p>CNNs sind darauf spezialisiert, matrixartige Topologien, wie zum Beispiel Bilder, zu verarbeiten. Sie bestehen haupts\u00e4chlich aus Convolutional Layern, die jeweils aus mehreren Convolutional Filtern bestehen. Diese Filter sind kleine Matrizen, die \u00fcber das Bild geschoben werden und dabei jeweils einen Teil des Bildes betrachten. Die Werte der Filter werden dabei trainiert, um bestimmte Muster zu erkennen. Die Filter werden dann auf das gesamte Bild angewendet und erzeugen eine neue Matrix, die sogenannte Feature Map. Diese Feature Map enth\u00e4lt Informationen \u00fcber die Muster, die der Filter erkannt hat. Die Feature Map wird dann an den n\u00e4chsten Convolutional Layer weitergegeben, der wiederum neue Muster erkennt. Auf diese Weise k\u00f6nnen CNNs komplexe Muster erkennen und klassifizieren. Weiterhin sind Pooling Layer, welche die Feature Maps verkleinern und somit genauere Muster erkennen k\u00f6nnen, und Fully Connected Layer, welche die Feature Maps in einen Vektor umwandeln um sie z.B. f\u00fcr eine Klassifizierung zu nutzen. </p> <p>Das U-Net CNN ist ein Beispiel von CNNs, welches auf einem vortrainierten Residual Neural Network aufbaut. Der Input wird zuerst auf 256x256 Pixel reshaped und normalisiert. Danach wird das vortrainierte Residual Neural Network geladen und die ersten 18 Layer werden eingefroren. Die restlichen Layer werden dann durch Convolutional Layer ersetzt, die die Feature Maps erweitern. Die Feature Maps werden dann durch Upsampling Layer vergr\u00f6\u00dfert und mit den Feature Maps der vorherigen Layer konkateniert. Dadurch werden die Feature Maps verfeinert und die Aufl\u00f6sung erh\u00f6ht. Am Ende wird ein Convolutional Layer mit einem Filter der Gr\u00f6\u00dfe 1x1 angewendet, um die Feature Maps auf die Anzahl der Klassen zu reduzieren. Die Feature Maps werden dann durch einen Softmax Layer klassifiziert.</p>"},{"location":"Themen/Einf%C3%A4rben%20von%20Bildern/#anwendungen","title":"Anwendungen","text":""},{"location":"Themen/Einf%C3%A4rben%20von%20Bildern/#pix2pix","title":"Pix2Pix","text":"<p>Das Pix2Pix Modell haben wir f\u00fcr die Code demo in PyTorch implementiert. PyTorch ist ein Open-Source-Deep-Learning-Framework, das von Facebook AI Research entwickelt wurde. Es bietet eine umfassende Plattform zur Entwicklung und Umsetzung von neuronalen Netzen in Python. </p> <p>Das Pix2Pix Model basiert auf einem GAN Ansatz, wobei sowohl Generator als auch Diskriminator auf einer U-Net Struktur basieren. Ein U-Net ist ein tiefes neuronales Netzwerk mit einer U-f\u00f6rmigen Architektur. Es nutzt Skip Connections, um globale und lokale Informationen zu kombinieren und genaue Farbinformationen zu erzeugen. Diese werden durch Anh\u00e4ngen der letzten Conv2D Schicht des Encoders an die erste Conv2D Schicht des Decoders erzeugt. Dadurch soll der Diskriminator nicht nur Abnormalit\u00e4ten erkennen, sondern auch die genaue Position der Abnormalit\u00e4t im Bild bestimmen k\u00f6nnen. Dar\u00fcber hinaus basiert der Decoder auf dem PatchGAN Diskriminator, welcher das Bild in (hier: 70x70 Pixel) Patches aufteilt und f\u00fcr jedes Patch eine Wahrscheinlichkeit ausgibt, ob es sich um ein echtes oder generiertes Bild handelt. Dadurch erh\u00e4lt man sch\u00e4rfere Features und eine h\u00f6here Genauigkeit. Zudem hat diese Diskriminator Architektur weniger Parameter als ein normaler Diskriminator, was das Training beschleunigt.</p> <p>In dieser cGAN Archtiktur bekommt nur der Diskriminator ein Label, das Schwarz-Wei\u00df-Bild. Der Generator bekommt jediglich das Schwarz-Wei\u00df-Bild als Input und soll ein Bild erzeugen, das zu diesem passt.</p> <p>Der Diskriminator wird sowohl mit dem generierten Bild als auch mit dem Originalbild getestet, um zu sehen, wie gut er die beiden unterscheiden kann. Dabei wird der Binary Cross Entropy Loss verwendet. Dieser ist definiert als:</p> \\[ \\displaystyle \\text{BCE}(x, y) = -\\frac{1}{N} \\sum_{i=1}^N y_i \\log(x_i) + (1 - y_i) \\log(1 - x_i) \\] <p>N ist in unserem Beispiel die Anzahl der Pixel im Bild. x ist der Output des Diskriminators, y ist die tats\u00e4chliche Wahrheit. 0 steht f\u00fcr ein generiertes Bild, 1 f\u00fcr ein echtes Bild. Der Loss wird dann f\u00fcr beide Bilder berechnet und anschlie\u00dfend der Durchschnitt gebildet.</p> <p>Der Generator wird mit dem generierten Bild getestet. Dabei wird der L1 Loss verwendet. Dieser ist definiert als:</p> \\[ \\displaystyle \\text{L1}(x, y) = \\frac{1}{N} \\sum_{i=1}^N |x_i - y_i| \\] <p>N ist in unserem Beispiel die Anzahl der Pixel im Bild. x ist der Output des Diskriminators, y ist das Originalbild.  Zus\u00e4tzlich wird der geupdatete Diskriminator verwendet, um zu sehen, wie gut der Generator die beiden Bilder unterscheiden kann. Dabei wird der Binary Cross Entropy Loss mit folgenden Einstellungen verwendet:</p> <ul> <li>x ist der Output des Diskriminators auf Fake Bild mit Schwarz-Wei\u00df-Bild als Label</li> <li>y ist das Originalbild   Dadurch sagt die Lossfunktion aus, wie sehr der Diskrimator glaubt, dass das generierte Bild zu dem Schwarz-Wei\u00df-Bild nicht passt.</li> </ul> <p></p> <p>Zum Trainieren des Modells haben wir den COCO-Datensatz verwendet, der 123.287 Bilder enth\u00e4lt. Aufgrund unserer begrenzten Trainingsressourcen haben wir nur 10k zuf\u00e4llig ausgew\u00e4hlte Bilder verwendet. </p> <p>Die Bilder wurden auf 256x256 Pixel skaliert und in den Lab-Farbraum konvertiert. Der L-Kanal wurde als Eingabe f\u00fcr den Generator und die Ab-Kan\u00e4le als Ziel verwendet. Der Diskriminator wurde mit dem L-Kanal des Eingangsbildes und den Ab-Kan\u00e4len des Zielbildes trainiert. </p> <p> </p> <p>Das Modell wurde f\u00fcr 70 Epochen mit einer Stapelgr\u00f6\u00dfe von 32 trainiert. Die Verlustfunktion war eine Kombination aus dem L1-Verlust und dem kontradiktorischen Verlust. Der L1-Verlust ist ein einfacher mittlerer absoluter Fehler zwischen dem vorhergesagten und dem Zielbild. Der gegnerische Verlust ist der bin\u00e4re Kreuzentropieverlust zwischen dem vorhergesagten und dem Zielbild. Der adversarial loss wird zum Trainieren des Diskriminators und des Generators verwendet. Der L1-Verlust wird nur f\u00fcr das Training des Generators verwendet. </p> <p>Zum optimieren des Modells brauchen wie die jeweilige Backward Funktion f\u00fcr den Generator und den Diskriminator:</p> <pre><code>def backward_D(self):\n\"\"\"\n    Backward pass for the discriminator.\n    \"\"\"\n    fake_image = torch.cat([self.L, self.fake_color], dim=1)\n    fake_preds = self.net_D(fake_image.detach())\n    self.loss_D_fake = self.GANcriterion(fake_preds, False)\n\n    real_image = torch.cat([self.L, self.ab], dim=1)\n    real_preds = self.net_D(real_image)\n    self.loss_D_real = self.GANcriterion(real_preds, True)\n\n    self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n    self.loss_D.backward()\n</code></pre> <pre><code>def backward_G(self):\n\"\"\"\n    Backward pass for the generator.\n    \"\"\"\n    fake_image = torch.cat([self.L, self.fake_color], dim=1)\n    fake_preds = self.net_D(fake_image)\n    self.loss_G_GAN = self.GANcriterion(fake_preds, True)\n\n    self.loss_G_L1 = self.L1criterion(self.fake_color, self.ab) * self.lambda_L1\n\n    self.loss_G = self.loss_G_GAN + self.loss_G_L1\n    self.loss_G.backward()\n</code></pre> <p>W\u00e4hrend des Training Vorgangs werden nun diese beiden Funktionen hergenommen um in der <code>optimize()</code> Funktion die Gewichte zu trainieren:</p> <pre><code>def optimize(self):\n\"\"\"\n    Optimize the model.\n    \"\"\"\n    self.forward()\n\n    self.net_D.train()\n    self.set_requires_grad(self.net_D, True)\n    self.opt_D.zero_grad()\n    self.backward_D()\n    self.opt_D.step()\n\n    self.net_G.train()\n    self.set_requires_grad(self.net_D, False)\n    self.opt_G.zero_grad()\n    self.backward_G()\n    self.opt_G.step()\n</code></pre> <p>Um das trainierte Modell f\u00fcr eine Prediction zu nutzen, k\u00f6nnen wir Bilder in den L-Space konvertieren und den Generator nutzen um eine Farbvorhersage zu treffen:</p> <pre><code>batch_prp = preprocess(batch)\nfake_imgs = predict(model, batch_prp)\nfake_imgs = [Image.fromarray((img * 255).astype(np.uint8)) for img in fake_imgs]\n</code></pre> <p>Da unser Modell nur sehr begrenzt trainiert wurde, sind die Ergebnisse nicht mit dem aktuellen Stand der Technik vergleichbar. Dennoch sind die Ergebnisse f\u00fcr ein so einfaches Modell sehr gut.</p> <p></p>"},{"location":"Themen/Einf%C3%A4rben%20von%20Bildern/#deoldify","title":"DeOldify","text":"<p>DeOldify ist im Gegensatz zu unserem selbst-trainierten Modell ein bereits kommerziell etablierted Modell und bietet daher auch schon vortrainierte Gewichte an. Diese k\u00f6nnen wir nutzen um unsere Bilder zu f\u00e4rben. </p> <p>Was DeOldify besonders macht ist unter Anderem der NoGAN Ansatz. Das bedeutet in diesem Fall, dass wir Generator und Diskriminator seperat voneinander trainieren und erst nachdem beide vollst\u00e4ndig trainiert wurden, zusammenf\u00fchren. Der Diskriminator wird dabei nur f\u00fcr das Training des Generators verwendet und \u00fcbertr\u00e4gt sein Wissen sehr schnell an den Generator. </p> <p>Die Lossfunktion des Models besteht aus zwei Teilen, den Farbloss und dem Kontentloss. Beim Farbloss wird der L1-Unterschied zwischen den RGB-Werten des Originalbildes und des gef\u00e4rbten Bildes berechnet. Das Kontentloss ist ein Featureloss, das die Unterschiede zwischen den Featuremaps des VGG16-Netzwerks berechnet.</p> <p>M\u00f6chte man das ganze selber ausprobieren, muss man folgenden Code ausf\u00fchren:</p> <pre><code>for image in images_real_bw:\n    deoldify_colorized.append(\n        colorizer.filter.filter(\n            image, image, render_factor=35, post_process=True\n        )\n    )\n</code></pre> <p></p> <p>Eine St\u00e4rke, die sich durch den NoGAN Ansatz herauskristallisiert, ist die F\u00e4higkeit, Videos einzuf\u00e4rben. Diese sind deutlich Farbintensiver und beinhalten weniger Farbflickern als mit herk\u00f6mmlichen GANs.</p> <p></p>"},{"location":"Themen/Einf%C3%A4rben%20von%20Bildern/#fazit","title":"Fazit","text":"<p>Die Bildkolorierung ist ein aktives Forschungsgebiet, das sowohl statistische Ans\u00e4tze als auch KI-Systeme umfasst. Statistische Methoden wie der Mean StD Transfer und der Lab Mean Transfer bieten schnelle und effiziente L\u00f6sungen, haben aber ihre Grenzen, insbesondere bei der Verarbeitung von Schwarz-Wei\u00df-Bildern. Auf der anderen Seite bieten KI-Systeme wie Pix2pix und DeOldify neue M\u00f6glichkeiten zur Verbesserung der Bildkolorierung. Diese Systeme verwenden lernf\u00e4hige Modelle, die sich an verschiedene Arten von Bildern anpassen k\u00f6nnen, und bieten daher das Potenzial f\u00fcr verbesserte Genauigkeit und Vielseitigkeit. Trotz der Fortschritte in diesem Bereich gibt es immer noch Herausforderungen und Raum f\u00fcr Verbesserungen.</p>"},{"location":"Themen/Einf%C3%A4rben%20von%20Bildern/#materialien","title":"Materialien","text":""},{"location":"Themen/Einf%C3%A4rben%20von%20Bildern/#podcast","title":"Podcast","text":"<p>Der Campus Talk \u2013 Silicon Forest \u2013 Folge 5</p>"},{"location":"Themen/Einf%C3%A4rben%20von%20Bildern/#talk","title":"Talk","text":"<p>Hier einfach Youtube oder THD System embedden.</p>"},{"location":"Themen/Einf%C3%A4rben%20von%20Bildern/#demo","title":"Demo","text":"<p>Link zum Repository:  https://mygit.th-deg.de/me04536/recolor</p>"},{"location":"Themen/Einf%C3%A4rben%20von%20Bildern/#literaturliste","title":"Literaturliste","text":"<ul> <li>DeOldify Paper</li> <li>Pix2Pix Paper</li> </ul>"},{"location":"Themen/Einf%C3%A4rben%20von%20Bildern/#autoren","title":"Autoren","text":"<ul> <li>Florian Eder</li> <li>Moritz Enderle</li> <li>Simon Drasch</li> </ul>"},{"location":"Themen/Empfehlungssysteme/","title":"Empfehlungssysteme","text":"<p>von Olha Solodovnyk, Daria Likhacheva und Zi Xun Tan</p>"},{"location":"Themen/Empfehlungssysteme/#abstract","title":"Abstract","text":"<p>Diese Arbeit behandelt das Thema Empfehlungssysteme und pr\u00e4sentiert es in verschiedenen Formaten, darunter einem Podcast, einem Vortrag und einer Code-Demo.</p> <p>Der Podcast bietet eine einfache und unkomplizierte Einf\u00fchrung in Empfehlungssysteme. Es werden verschiedene Arten von Empfehlungssystemen behandelt, darunter inhaltsbasierte, kollaborative, kontext-basierte und hybride Ans\u00e4tze. Der Podcast richtet sich an ein allgemeines Publikum und soll Zuh\u00f6rer einbeziehen und ihnen einen \u00dcberblick \u00fcber die Funktionsweise von Empfehlungssystemen und -anwendungen geben.</p> <p>In dem Vortrag wird hingegen n\u00e4her auf Empfehlungssysteme eingegangen. Die Grundlagen und Funktionsweise inhaltsbasierter, kollaborativer und hybrider Empfehlungssysteme werden erl\u00e4utert und die Vor- und Nachteile dieser Ans\u00e4tze diskutiert. Der Vortrag richtet sich an ein technisch versiertes Publikum und vermittelt ein tieferes Verst\u00e4ndnis der Konzepte und Algorithmen hinter Empfehlungssystemen.</p> <p>Die Code-Demo zeigt eine praktische Implementierung eines einfachen Filmempfehlungssystems. Es werden verschiedene Algorithmen und Techniken pr\u00e4sentiert, einschlie\u00dflich inhaltsbasierter, kollaborativer und hybrider Filter. Durch die Demonstration des Codes wird gezeigt, wie ein Datensatz verarbeitet und Empfehlungen generiert werden, die sowohl auf Inhaltsinformationen als auch auf Benutzerinteraktionen basieren. Der bereitgestellte Code ist frei verf\u00fcgbar, sodass jeder die Implementierung von Empfehlungssystemen besser verstehen und damit experimentieren kann.</p>"},{"location":"Themen/Empfehlungssysteme/#1-einleitung-motivation","title":"1 Einleitung / Motivation","text":"<p>In der heutigen digitalen Welt werden die Empfehlungssysteme fast jeden Tag benutzt. Sie haben einen sehr gro\u00dfen Einfluss auf unsere t\u00e4glichen Aktivit\u00e4ten. Von Online-Shopping-Plattformen bis hin zu Streaming-Dienste (wie Spotify oder Netflix) werden die Empfehlungen benutzt, um unsere Entscheidungen zu beeinflussen und unsere Erfahrungen zu personalisieren. Was bedeuten aber diese Empfehlungssysteme und wie funktionieren sie?</p> <p>Wir werden immer mit relevanten Informationen, Produkten und Dienstleistungen versorgt, da die Empfehlungssysteme daf\u00fcr verantwortlich sind. F\u00fcr die Empfehlungssysteme zu funktionieren, brauchen wir sehr viele Daten, die aus unseren Vorlieben, Nutzerverhalten und Aktivit\u00e4ten im Netz gesammelt und analysiert werden. Mit st\u00e4ndigen Empfehlungen von neuen Filmen, B\u00fccher, Musik, Produkte und Dienstleistungen haben die Empfehlungssysteme das Potenzial, unser Leben immer mehr einfacher und bequemer zu gestalten. Sie verbessern aber nicht nur das Leben von Nutzern, sondern auch den Unternehmen und K\u00fcnstlern. Indem sie ihnen eine M\u00f6glichkeit bieten, ihre Werke einem breiteren Publikum zu pr\u00e4sentieren oder \u00fcber ihre Dienstleistungen zu informieren, k\u00f6nnen sie dazu beitragen, die Sichtbarkeit von K\u00fcnstlern, Autoren, Filmemachern, Dienstleistungsanbieter und Unternehmen zu erh\u00f6hen.</p> <p>Dennoch begegnen uns in der modernen Welt auch spezifische Herausforderungen im Zusammenhang mit Empfehlungssystemen. Unter den ethischen Fragen, die ber\u00fccksichtigt werden m\u00fcssen, sind die Privatsph\u00e4re von Kunden, Manipulation und Voreingenommenheit der Empfehlungsalgorithmen. </p> <p>Im Folgenden wollen wir einen Blick auf die Welt der Empfehlungssysteme werfen, die verschiedenen Arten der Empfehlungssysteme uns anschauen, ihre Funktionsweise verstehen, ihre Vor- und Nachteile beleuchten und die Auswirkungen auf unsere Gesellschaft sowie individuellen Entscheidungsprozesse untersuchen. Wenn wir und das Thema genauer anschauen, k\u00f6nnen wir besser verstehen, wie Empfehlungssysteme unseren Alltag verbessern und wie wir als Nutzerinnen und Nutzer bewusste Entscheidungen treffen k\u00f6nnen.</p>"},{"location":"Themen/Empfehlungssysteme/#2-methoden","title":"2 Methoden","text":""},{"location":"Themen/Empfehlungssysteme/#21-inhaltsbasierte-filterung","title":"2.1 Inhaltsbasierte Filterung","text":"<p>Die Inhaltsfilterung analysiert die Merkmale der Objekte (z. B. Filme, Produkte, Artikel) und Benutzervorlieben und versucht, auf der Grundlage dieser Merkmalen geeignete Empfehlungen zu generieren.</p> <p>Der Prozess der Inhaltsbasierten Filterung beinhaltet in der Regel die folgenden Schritte:</p> <p>Objekt-Profileerstellung: F\u00fcr jedes Objekt werden relevante Merkmale extrahiert und daraus ein oder mehrere Profile erstellt. Bei Filmen k\u00f6nnten dies beispielsweise Genre, Schauspieler, Regisseur oder Handlungsbeschreibung sein. Die Merkmale k\u00f6nnen aus Metadaten oder Inhalten des Objekts (z.B. Textanalyse) gewonnen werden. </p> <p>Benutzer-Profilerstellung: F\u00fcr jeden Benutzer wird ein Profil erstellt, das seine Vorlieben basierend auf seinen vergangenen Bewertungen oder Interaktionen widerspiegelt. Das Profil kann durch Gewichtung der Merkmale entsprechend der Wichtigkeit f\u00fcr den Benutzer erstellt werden.</p> <p>\u00dcbereinstimmungsberechnung: Die \u00c4hnlichkeit zwischen Objektvektoren kann mit drei Methoden berechnet werden: </p> <ul> <li>Euklidischer Abstand</li> <li>Pearson-Korrelation und </li> <li>Kosinus-\u00c4hnlichkeit, die wir in unserer Arbeit genauer betrachten.</li> </ul> <p>Kosinus-\u00c4hnlichkeit ist ein Ma\u00df daf\u00fcr, wie \u00e4hnlich zwei Vektoren sind. Der Kosinus des Winkels zwischen zwei Vektoren wird bestimmt. </p> \\[ Kosinus\u2011\u00c4hnlichkeit(u, i) = cos(u,i)= \\frac{u \\cdot i}{\\|u\\| \\cdot \\|i\\|} \\] <p>Der Nutzervektor \\(u\\) repr\u00e4sentiert die Pr\u00e4ferenzen, Eigenschaften oder Merkmale des Nutzers, w\u00e4hrend der Objektvektor \\(i\\) die Merkmale oder Eigenschaften des Items oder eines anderen Nutzers darstellt. Die Euklidische Norm \\(\u2225u\u2225\\) des Nutzervektors gibt an, wie gro\u00df der Vektor ist, w\u00e4hrend die Euklidische Norm \\(\u2225i\u2225\\) des Objektvektors dessen Gr\u00f6\u00dfe angibt. Das Skalarprodukt \\(u\u22c5i\\) zwischen dem Nutzervektor \\(u\\) und dem Objektvektor \\(i\\) ergibt sich aus der Summe der Produkte der entsprechenden Elemente beider Vektoren.</p> <p>Der Kosinus des eingeschlossenen Nullwinkels ist \\(1\\), was 100 % \u00c4hnlichkeit bedeutet. F\u00fcr jeden anderen Winkel ist der Kosinus des eingeschlossenen Winkels kleiner als \\(1\\).</p> <p>Empfehlungsgenerierung: Basierend auf den \u00c4hnlichkeiten werden Objekte ausgew\u00e4hlt, die dem Benutzer empfohlen werden sollen. Dies k\u00f6nnen Objekte sein, die eine hohe \u00c4hnlichkeit zu den vom Benutzer bevorzugten Merkmalen aufweisen.</p>"},{"location":"Themen/Empfehlungssysteme/#22-kollaborative-filterung","title":"2.2 Kollaborative Filterung","text":"<p>Die kollaborative Filterung ist ein Ansatz, der die vergangenen Bewertungen eines Benutzers nutzt, um eine Datenbank (Benutzer-Objekt-Matrix) von Pr\u00e4ferenzen zu erstellen und Vorhersagen \u00fcber Gegenst\u00e4nde zu treffen, die mit den Vorlieben des Benutzers \u00fcbereinstimmen. Sie wird in zwei Hauptkategorien unterteilt: memory-basierte kollaborative Filterung und modellbasierte kollaborative Filterung.</p>"},{"location":"Themen/Empfehlungssysteme/#221-memory-basierte-kollaborative-filterung","title":"2.2.1 Memory-basierte kollaborative Filterung","text":"<p>Die klassische kollaborative Filterung umfasst den memory-basierten Ansatz. Dieser Ansatz nutzt Informationen \u00fcber die Vergangenheit von Benutzern und/oder Objekten, um Empfehlungen zu generieren. Der Schwerpunkt liegt auf der Speicherung und Auswertung von Informationen \u00fcber Benutzerinteraktionen und Bewertungen, um \u00c4hnlichkeiten zwischen Benutzern oder Objekten zu berechnen. </p> <p>Es gibt zwei Haupttypen memory-basierter Ans\u00e4tze, beide sind k-nearest neighbor Algorithmen, aber sie haben dennoch gro\u00dfe Unterschiede:</p>"},{"location":"Themen/Empfehlungssysteme/#benutzer-basierte-methode","title":"Benutzer-basierte Methode:","text":"<p>Das Benutzer-basierte Verfahren analysiert die \u00c4hnlichkeit zwischen Benutzern. Die Idee ist, dass Benutzer mit \u00e4hnlichen Vorlieben \u00e4hnliche Interaktionen mit Objekten haben. Das Empfehlungssystem sucht nach Benutzern mit \u00e4hnlichem Verhalten und generiert basierend auf deren Bewertungen Empfehlungen f\u00fcr einen bestimmten Benutzer. </p> <p> </p>  Eine Illustration der Benutzer-basierte-Methode."},{"location":"Themen/Empfehlungssysteme/#objekt-basierte-methode","title":"Objekt-basierte Methode:","text":"<p>Das Objekt-basierte Verfahren konzentriert sich auf die \u00c4hnlichkeit zwischen Objekten. Die Hauptidee besteht darin, \u00e4hnliche Objekte anhand von Bewertungen oder Benutzerinteraktionen zu finden. Das Empfehlungssystem vergleicht Benutzer-Bewertung-Modelle f\u00fcr verschiedene Objekte und sucht nach \u00e4hnlichen Objekte. </p> <p> </p>  Illustration der Objekt-basierte-Methode.  <p>In beiden F\u00e4llen kann \u00c4hnlichkeitsma\u00df wie die Kosinus-\u00c4hnlichkeit oder andere verwendet werden, um die \u00c4hnlichkeit zwischen Benutzern oder  Objekten zu berechnen.</p>"},{"location":"Themen/Empfehlungssysteme/#222-modellbasierte-kollaborative-filterung","title":"2.2.2 Modellbasierte kollaborative Filterung","text":"<p>Bei der Verwendung von memory-basierte Methoden entsteht das Problem, dass die Benutzer-Objekt-Matrix aufgrund einer gro\u00dfen Anzahl von fehlenden Bewertungen sehr d\u00fcnn besetzt ist. Diese Datensp\u00e4rlichkeit beeintr\u00e4chtigt die F\u00e4higkeit, Benutzervorlieben genau einzusch\u00e4tzen und zuverl\u00e4ssige Empfehlungen zu geben. </p> <p>Ein L\u00f6sungsansatz f\u00fcr dieses Problem besteht in der Verwendung von modellbasierten Methoden. Im Gegensatz zu memory-basierte Methoden lernen modellbasierte Algorithmen aus den vorhandenen Bewertungen ein Modell, das dann zur Vorhersage fehlender Bewertungen genutzt werden kann. Diese Modelle k\u00f6nnen auf Techniken wie Clustering, Bayes'schen Klassifikatoren oder Matrixfaktorisierung basieren.</p> <p>Singular Value Decomposition (SVD) / Singul\u00e4rwertzerlegung Obwohl es verschiedene Algorithmen gibt, konzentrieren wir uns haupts\u00e4chlich auf die Matrixfaktorisierung mit SVD. Die zentrale Idee bei der SVD-basierten Matrixfaktorisierung besteht darin, eine niedrigdimensionale Approximation der urspr\u00fcnglichen Bewertungsmatrix zu finden. Diese niedrigdimensionale Approximation erm\u00f6glicht es uns, die wichtigsten latenten Faktoren zu erfassen, die zu Benutzervorlieben und Objekteigenschaften beitragen. In einem Filmempfehlungssystem k\u00f6nnten latenten Faktoren beispielsweise zugrunde liegende Attribute wie Genre, Regisseur oder Schauspieler darstellen.</p> <p>Angenommen, wir haben eine Matrix \\(A_{m \\times p}\\), in der die \\(m\\) Zeilen die Benutzer repr\u00e4sentieren und die \\(p\\) Spalten die Objekte darstellen. Das Ziel des SVD-Theorems besteht darin, die hochdimensionale Matrix \\(A_{m \\times p}\\) in drei Matrizen mit geringerer Dimension aufzuteilen. Das Theorem besagt:</p> \\[ A = U\u03a3V^T \\] <p>\\(U\\) steht f\u00fcr die Benutzermatrix, \\(\u03a3\\) ist eine diagonale Matrix mit den Singul\u00e4rwerten und \\(V^T\\)  bezeichnet die Objektmatrix. Die Singul\u00e4rwerte in der \\(\u03a3\\)-Matrix werden in absteigender Reihenfolge sortiert. Durch Auswahl der obersten \\(k\\) Singul\u00e4rwerte und ihrer entsprechenden Spalten in \\(U\\) und \\(V^T\\) reduzieren wir die Dimensionalit\u00e4t der Matrizen. Der Wert von \\(k\\) bestimmt die Anzahl der beibehaltenen latenten Faktoren. </p> <p>Wenn wir uns das Filmempfehlungssystem als Beispiel nehmen, geht dieser Ansatz davon aus, dass es versteckte Beziehungen zwischen Benutzern und Filmen gibt, die sich auf die Bewertung eines Benutzers f\u00fcr einen bestimmten Film auswirken. Konkret wird angenommen, dass es eine Reihe von \\(k\\) Faktoren gibt, die bestimmen, wie ein Benutzer einen Film bewertet, und dass diese Faktoren durch die Rang-\\(k\\) SVD erfasst werden k\u00f6nnen.</p> <p>Training des Modells mit Surprise Library In der Surprise-Bibliothek ist SVD als Empfehlungssystem-Modul implementiert. Im Wesentlichen geht es darum, das Empfehlungsproblem in ein Optimierungsproblem umzuwandeln. Wir k\u00f6nnen die G\u00fcte unserer Vorhersagen f\u00fcr die Nutzerbewertungen von Objekten messen. Eine h\u00e4ufig verwendete Metrik daf\u00fcr ist der Root Mean Squared Error (RMSE). Je niedriger der RMSE, desto besser die G\u00fcte.</p> <p><pre><code>from surprise import SVD\n\n# Create a reader object\nreader = Reader()\n\n# Load the dataset from the DataFrame, specifying the columns for userId, movieId, and rating\ndata = Dataset.load_from_df(df_ratings[[\"userId\", \"movieId\", \"rating\"]], reader)\n\n# Instantiate the SVD algorithm\nsvd = SVD()\n\n# Run 5-fold cross-validation and print results\ncross_validate(svd, data, measures=[\"RMSE\", \"MAE\"], cv=5, verbose=True)\n</code></pre> Um eine Bewertung f\u00fcr einen bestimmten Benutzer vorherzusagen, kann die Methode <code>predict()</code> verwendet werden. Beachten Sie jedoch, dass der Benutzer in Ihrer Datenbank vorhanden sein muss. <pre><code>predicted_rating = svd.predict(user_id=1, movie_id=12)\npredicted_rating.est\n</code></pre> Hyperparameter Tuning Bei der Optimierung des SVD-Modells in der Surprise-Bibliothek gibt es mehrere wichtige Hyperparameter zu beachten:</p> <p><code>n_factors</code>: Dieser Hyperparameter bestimmt die Anzahl der latenten Faktoren oder Dimensionen, die zur Darstellung von Benutzern und Objekten verwendet werden. Durch Erh\u00f6hung der Anzahl der Faktoren k\u00f6nnen m\u00f6glicherweise komplexere Beziehungen erfasst werden, es kann jedoch auch zu Overfitting kommen.</p> <p><code>n_epochs</code>: Dieser Hyperparameter repr\u00e4sentiert die Anzahl der Iterationen oder Epochen, die w\u00e4hrend des Optimierungsprozesses verwendet werden. Durch Erh\u00f6hung der Anzahl der Epochen kann das Modell besser aus den Daten lernen, aber zu viele Epochen k\u00f6nnen ebenfalls zu Overfitting f\u00fchren.</p> <p><code>lr_all</code>: Dieser Hyperparameter steuert die Lernrate, die den Schrittweite im Optimierungsalgorithmus bestimmt. Eine h\u00f6here Lernrate kann zu schnellerer Konvergenz f\u00fchren, aber auch dazu f\u00fchren, dass das optimale Ergebnis \u00fcberschritten wird.</p> <p><code>reg_all</code>: Dieser Hyperparameter steuert den Regularisierungsterm f\u00fcr alle Parameter im Modell. Regularisierung hilft, Overfitting zu verhindern, indem eine Strafe f\u00fcr komplexe Modelle hinzugef\u00fcgt wird. Die Anpassung dieses Hyperparameters kann einen Kompromiss zwischen der Erfassung n\u00fctzlicher Muster und der Vermeidung von Overfitting darstellen.</p>"},{"location":"Themen/Empfehlungssysteme/#23-hybride-empfehlungssysteme","title":"2.3 Hybride Empfehlungssysteme","text":"<p>Sowohl das Inhaltsbasierte als auch das kollaborative Filtermodell haben ihre Einschr\u00e4nkungen. Inhaltsbasierte Empfehlungssysteme haben die Einschr\u00e4nkung, dass sie stark von den verf\u00fcgbaren Merkmalen oder Attributen eines Objekts abh\u00e4ngen, um \u00c4hnlichkeiten zu ermitteln. Dadurch k\u00f6nnen sie Schwierigkeiten haben, komplexe und subtile Zusammenh\u00e4nge zwischen verschiedenen Objekten zu erfassen, die sich nicht einfach durch Attribute beschreiben lassen.</p> <p>Auf der anderen Seite k\u00f6nnen kollaborative Filterungssysteme Einschr\u00e4nkungen aufweisen, wenn es um die Bew\u00e4ltigung des sogenannten \"Cold-Start\"-Problems geht. Dieses Problem tritt auf, wenn ein neuer Benutzer oder ein neues Objekt in das System eingef\u00fchrt wird und keine ausreichenden Informationen \u00fcber die Vorlieben oder \u00c4hnlichkeiten zu anderen Benutzern oder Objekten vorliegen. Dadurch kann die Genauigkeit der Empfehlungen in solchen Situationen beeintr\u00e4chtigt werden.</p> <p>Die Idee hinter hybriden Techniken besteht darin, dass eine Kombination von Algorithmen genauere und effektivere Empfehlungen liefert als ein einzelner Algorithmus, da die Nachteile eines Algorithmus durch einen anderen Algorithmus \u00fcberwunden werden k\u00f6nnen. Indem verschiedene Empfehlungsmethoden miteinander verbunden werden, k\u00f6nnen hybride Filtertechniken die St\u00e4rken der einzelnen Ans\u00e4tze nutzen und gleichzeitig deren Schw\u00e4chen reduzieren. Dadurch erm\u00f6glichen sie eine verbesserte Personalisierung und Pr\u00e4zision bei den Empfehlungen f\u00fcr Benutzer.</p> <p>Das Hybrid-Empfehlungsmodell ist in sieben Typen unterteilt: </p> Typ Beschreibung Vorteile Beispiel Gewichtet Eine Methode, bei der die Gewichtung allm\u00e4hlich angepasst wird, je nachdem, inwieweit die Bewertung eines Gegenstandes durch den Benutzer mit der durch das Empfehlungssystem vorhergesagten Bewertung \u00fcbereinstimmt. Nutzung der St\u00e4rken verschiedener Empfehlungssysteme, einfache Implementierung. P-tango Schalten Wechselt zwischen Empfehlungstechniken basierend auf einer heuristischen Methode, die die F\u00e4higkeit zur Erzeugung guter Bewertungen ber\u00fccksichtigt. L\u00f6st spezifische Probleme einzelner Methoden. Vermeidet Probleme spezifischer Methoden, sensibel f\u00fcr St\u00e4rken und Schw\u00e4chen der Empfehlungssysteme. DailyLearner Kaskade Nach der Erstellung einer Kandidatenliste unter Verwendung eines Empfehlungssystemmodells mit \u00e4hnlichem Geschmack wie der Benutzer kombiniert die Methode das zuvor verwendete Empfehlungssystemmodell mit einem anderen Modell, um die Kandidatenliste nach den f\u00fcr den Benutzer am besten geeigneten Artikeln zu sortieren. Verfeinert Empfehlungen durch Iteration, effizient und tolerant gegen\u00fcber St\u00f6rungen. EntreeC Gemischt Kombiniert Empfehlungsergebnisse verschiedener Techniken gleichzeitig f\u00fcr jeden Artikel und liefert mehrere Empfehlungen. Bietet mehrere Empfehlungen pro Artikel, individuelle Leistungen beeinflussen die Gesamtleistung in einem begrenzten Bereich nicht. PTV-System, Profinder, PickAFlick Merkmalskombination Bezieht sich auf die Integration von Merkmalen, die von einer Empfehlungstechnik erzeugt wurden, in eine andere Technik. Durch die Einbeziehung spezifischer Merkmale oder Bewertungen einer Technik als zus\u00e4tzliche Eingabe f\u00fcr eine andere Technik wird der Empfehlungsprozess verbessert, indem verschiedene Informationsquellen genutzt werden, um die Genauigkeit und Relevanz der Empfehlungen zu steigern. Nutzt kollaborative Daten in Verbindung mit anderen Techniken, reduziert die Abh\u00e4ngigkeit von der kollaborativen Filterung. Pipper Funktionserweiterung Nutzt Bewertungen und zus\u00e4tzliche Informationen, die von vorherigen Empfehlungssystemen generiert werden. Erfordert zus\u00e4tzliche Funktionalit\u00e4t der Empfehlungssysteme. F\u00fcgt dem prim\u00e4ren Empfehlungssystem eine geringe Anzahl von Merkmalen hinzu, verbessert die Genauigkeit der Empfehlungen. Libra-System Meta-Ebene Verwendet das interne Modell einer Technik als Eingabe f\u00fcr eine andere. Bietet umfassendere Informationen im Vergleich zu einzelnen Bewertungen. L\u00f6st das Problem der Datensp\u00e4rlichkeit bei kollaborativen Filterungstechniken, nutzt umfassende Modelle f\u00fcr verbesserte Empfehlungen. LaboUr"},{"location":"Themen/Empfehlungssysteme/#3-anwendungen","title":"3 Anwendungen","text":"<p>Empfehlungssysteme finden in sehr vielen Bereichen Anwendung und tragen dazu bei, die Entscheidungsfindung von Benutzern zu erleichtern. Hier werden einige der Hauptanwendungen von Empfehlungssystemen aufgezeigt:</p> <ul> <li>E-Commerce: Online-Shopping-Plattformen wie Amazon oder Zalando nutzen Empfehlungssysteme, um ihren Kunden Produkte vorzuschlagen, die ihren individuellen Vorlieben entsprechen. Basierend auf dem bisherigen Kaufverhalten, den Produktbewertungen und den Pr\u00e4ferenzen anderen Nutzern werden personalisierte Produktvorschl\u00e4ge gemacht, um die Einkaufserfahrung zu verbessern und die Kundenbindung zu st\u00e4rken.</li> <li>Streaming-Dienste: Plattformen wie Netflix und YouTube verwenden Empfehlungssysteme, um Nutzern Inhalte vorzuschlagen, die ihren Geschmack treffen. Basierend auf dem Seh- und H\u00f6rverhalten sowie den Bewertungen vergangener Inhalte werden individuelle Serienempfehlungen und Musikvorschl\u00e4ge generiert, um das Unterhaltungserlebnis zu personalisieren.</li> <li>Soziale Medien: Hier setzen Facebook, Instagram und Twitter Empfehlungssysteme ein, um Nutzern relevante Inhalte, Beitr\u00e4ge und Kontakte anzuzeigen. Basierend auf dem sozialen Netzwerk, den Interaktionen und den Vorlieben der Nutzer werden Beitr\u00e4ge von Freunden, Seiten oder Themen empfohlen, um das Engagement und die Nutzung der Plattform zu f\u00f6rdern.</li> <li>Musikdienste: Musik-Streaming-Dienste wie Spotify oder Apple Musik nutzen Empfehlungssysteme, um personalisierte Wiedergabelisten zu erstellen. \"Discover Weekly\" und \"Release Radar\" sind Beispiele daf\u00fcr, wie Empfehlungssysteme basierend auf dem individuellen Musikgeschmack der Nutzer neue K\u00fcnstler, Songs und Alben vorschlagen.</li> <li>Nachrichten und Content-Aggregatoren: Empfehlungssysteme werden auch in Nachrichten-Apps und Content-Aggregatoren eingesetzt, um Nutzern relevante Artikel, Blogposts und Nachrichten vorzuschlagen. Basierend auf den Pr\u00e4ferenzen, dem Leseverhalten und den Interessen werden personalisierte Newsfeeds und Inhaltszusammenstellungen erstellt.</li> <li>Reise- und Hotelbuchungen: Empfehlungssysteme kommen auch in der Tourismusbranche zum Einsatz. Plattformen wie Booking.com oder Airbnb nutzen sie, um Nutzern ma\u00dfgeschneiderte Hotelvorschl\u00e4ge, Reiserouten und Attraktionen zu pr\u00e4sentieren, die ihren individuellen Vorlieben und Bed\u00fcrfnissen entsprechen.</li> </ul> <p>Diese Anwendungen von Empfehlungssystemen sind nur einige Beispiele f\u00fcr die vielf\u00e4ltigen Einsatzm\u00f6glichkeiten dieser Technologie. Ob im Bereich des Online-Shoppings, der Unterhaltung, der sozialen Interaktion oder der Informationsbeschaffung, Empfehlungssysteme spielen eine zentrale Rolle dabei, unsere Erlebnisse zu verbessern, relevante Inhalte zu entdecken und unsere Zeit effizienter zu nutzen.</p>"},{"location":"Themen/Empfehlungssysteme/#4-fazit","title":"4 Fazit","text":"<p>Empfehlungssysteme sind zu einem integralen Bestandteil unseres digitalen Lebens geworden und bieten personalisierte und relevante Vorschl\u00e4ge f\u00fcr eine Vielzahl von Produkten, Dienstleistungen und Inhalten. Diese Arbeit behandelt die verschiedenen Arten von Empfehlungssystemen, einschlie\u00dflich inhaltsbasierter, kollaborativer Filterung und hybrider Ans\u00e4tze.</p> <p>Die inhaltsbasierte Filterung nutzt die Merkmale von Objekten und die Benutzerpr\u00e4ferenzen, um Empfehlungen zu generieren. Dieser Ansatz ist effektiv, wenn die Benutzerpr\u00e4ferenzen stark von den Eigenschaften des Inhalts abh\u00e4ngen. Bei der kollaborativen Filterung werden hingegen das Benutzerverhalten und die Interaktionen verwendet, um \u00c4hnlichkeiten zwischen den Benutzern zu identifizieren und Empfehlungen auf der Grundlage dieser \u00c4hnlichkeiten zu geben. Dieser Ansatz ist besonders wertvoll, wenn explizite R\u00fcckmeldungen wie Bewertungen oder Rezensionen verf\u00fcgbar sind. Hybride Empfehlungssysteme kombinieren mehrere Ans\u00e4tze, um deren St\u00e4rken zu nutzen und ihre Einschr\u00e4nkungen zu ber\u00fccksichtigen.</p> <p>Jeder Empfehlungsansatz hat Vor- und Nachteile. Die inhaltsbasierte Filterung kann Schwierigkeiten bei der Merkmalsextraktion und dem m\u00f6glichen \"Filterblasen\"-Effekt haben. Die kollaborative Filterung kann mit Herausforderungen wie dem \"Cold Start\" und der Datensp\u00e4rlichkeit konfrontiert sein. Hybride Systeme m\u00fcssen die Komplexit\u00e4t bew\u00e4ltigen, um verschiedene Ans\u00e4tze effektiv zu kombinieren und qualitativ hochwertige Empfehlungen bereitzustellen.</p> <p>Obwohl Empfehlungssysteme insgesamt betr\u00e4chtliche Vorteile bieten, ist es wichtig zu erkennen, dass sie auch ethische Bedenken aufwerfen. Der Prozess basiert stark auf der Erfassung und Analyse von Benutzerdaten, wodurch erh\u00f6hte Aufmerksamkeit f\u00fcr den Datenschutz und effektive Datenschutzma\u00dfnahmen erforderlich sind. Transparenz und Benutzerkontrolle spielen eine entscheidende Rolle bei der Verwaltung der erfassten Daten und ihrer anschlie\u00dfenden Verwendung. Zus\u00e4tzlich muss die Pr\u00e4senz von Vorurteilen oder Ungerechtigkeiten in Empfehlungsalgorithmen schnell angegangen werden, da solche Empfehlungen bestehende Ungleichheiten verst\u00e4rken und bestimmte Vorlieben festigen k\u00f6nnen.</p> <p>Zusammenfassend ist es unbestreitbar, dass Empfehlungssysteme unsere Konsumgewohnheiten, Entscheidungsf\u00e4higkeiten und Gesamterfahrungen als Benutzer ma\u00dfgeblich ver\u00e4ndert haben. Diese Systeme bieten nicht nur die M\u00f6glichkeit, die Benutzerzufriedenheit zu steigern, sondern leisten auch wichtige Unterst\u00fctzung f\u00fcr Unternehmen. Es ist jedoch entscheidend, Empfehlungssysteme mit einem kritischen Blick zu betrachten und ihre Einschr\u00e4nkungen und ethischen Implikationen zu ber\u00fccksichtigen.</p>"},{"location":"Themen/Empfehlungssysteme/#5-weiterfuhrendes-material","title":"5 Weiterf\u00fchrendes Material","text":""},{"location":"Themen/Empfehlungssysteme/#51-podcast","title":"5.1 Podcast","text":"<p>Hier Link zum Podcast.</p>"},{"location":"Themen/Empfehlungssysteme/#52-talk","title":"5.2 Talk","text":"<p>Hier einfach Youtube oder THD System embedden.</p>"},{"location":"Themen/Empfehlungssysteme/#53-demo","title":"5.3 Demo","text":"<p>Hier Link zum Demo Video Link zum GitHub Repository</p>"},{"location":"Themen/Empfehlungssysteme/#6-literaturliste","title":"6 Literaturliste","text":"<p>Ko, H. Y., Lee, S. Y., Park, Y., &amp; Choi, A. L. (2022). A Survey of Recommendation Systems: Recommendation Models, Techniques, and Application Fields. Electronics; MDPI. https://doi.org/10.3390/electronics11010141</p> <p>Isinkaye, F. O., Folajimi, Y., &amp; Ojokoh, B. A. (2015). Recommendation systems: Principles, methods and evaluation. Egyptian Informatics Journal; Elsevier BV. https://doi.org/10.1016/j.eij.2015.06.005</p> <p>Baptiste Rocca (2019). Introduction to recommender systems Overview of some major recommendation algorithms. https://towardsdatascience.com/introduction-to-recommender-systems-6c66cf15ada</p> <p>G. Adomavicius, A. Tuzhilin. (2005). Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions.  https://ieeexplore.ieee.org/abstract/document/1423975</p>"},{"location":"Themen/Face_Aging/","title":"Face Aging","text":"<p>von Felix R\u00f6sch, Julian Steiner</p>"},{"location":"Themen/Face_Aging/#abstract","title":"Abstract","text":"<p>Bei Face Aging handelt es sich um die technische M\u00f6glichkeit Gesichter von Menschen digital altern zu lassen. Wichtig hierbei ist, dass das zuk\u00fcnftige Aussehen mit nat\u00fcrlichen Alterungseffekten vorhergesagt werden und gleichzeitig die pers\u00f6nlichen Merkmale erhalten bleiben. Neuentwickelte Generative Adversarial Networks (GANs) in verschiedenen Formen erzielten eine bessere Vorhersage der Alterung als herk\u00f6mmliche Methoden. </p> <p>In diesen Report gehen wir auf den aktuellen Stand der Forschung, die Methoden und potentielle Anwendungen ein. Zus\u00e4tzlich zu dieser schriftlichen Ausarbeitung wurde ein Fachvortrag und eine Code-Pr\u00e4sentation erarbeitet.</p> <p>Im Fachvortrag gehen wir detaillierter auf das Paper \"PFA-GAN: Progressive Face Aging With Generative Adversarial Network\" und dessen Generative Adversarial Network zum Thema Face Aging ein. Die Code-Demonstration greift diesen Vortrag auf und implementiert das neuronale Netz in einem Jupyter Notebook.</p>"},{"location":"Themen/Face_Aging/#einleitung-motivation","title":"Einleitung / Motivation","text":"<p>Definition</p> <p>Gesichtsalterung (Face Aging), auch bekannt als Alterssynthese (Age Synthesis) und Altersprogression (Age Progression), wird als \u00e4sthetische Darstellung eines Gesichtsbildes mit nat\u00fcrlichen Alterungs- und Verj\u00fcngungseffekten auf das einzelne Gesicht definiert.</p> <p>Fu Y, Guo G, Huang TS</p> <p>Anwendung finden diese Methoden zum Beispiel in den Bereichen der Unterhaltung, sozialer Sicherheit, alters\u00fcbergreifenden Gesichtserkennung, Forensik und Medizin.</p> <p>M\u00f6gliche Anwendungsbeispiele sind zum einen Applikationen, die das Gesicht einer Person mit einem bestimmten Lebensstil vorhersagen. Solche Anwendungen k\u00f6nnen zum Beispiel dazu genutzt werden, um Personen mit einen \u00fcberm\u00e4\u00dfigen Alkohol- und Nikotinkonsum die Auswirkungen aufzuzeigen um damit den Konsum zu verringern. Des Weiteren k\u00f6nnen solche Technologien helfen den Menschenhandel zu bek\u00e4mpfen und Familien wieder zusammenf\u00fchren, denn das menschliche Gesicht durchl\u00e4uft von der Kindheit bis zum Erwachsenenalter eine deutliche k\u00f6rperliche Ver\u00e4nderung. Deswegen ist es zehn bis 15 Jahre nach der Entf\u00fchrung schwierig, ein verlorenes Kind wiederzuerkennen.</p> <p>Die traditionellen Alterungsmethoden basieren meist auf der mechanischen Modellierung von Falten, Haaren, Textur und Gesichtsausdruck oder nutzen viele Daten, um Prototypen als Altersmuster zu konstruieren. Neue Deep Learning Methoden erzielten gro\u00dfe Erfolge bei dem Thema Face Aging. Durch das Training zum Erlernen spezifischer Altersmuster und Zuordnungen zwischen Eingabegesichtern und Zielaltersbezeichnungen k\u00f6nnen Deep-Learning-Methoden Gesichter einer bestimmten Altersgruppe direkt generieren. Grob k\u00f6nnen die verschiedenen Methoden in drei Kategorien eingeteilt werden:</p> <ol> <li>Physikalische modellbasierte Methoden</li> <li>Prototypbasierte Methoden</li> <li>Deep Generative Networks (Tiefe generative Netzwerke)</li> </ol> <p>Obwohl sich mit Deep-Learning-Methoden die Altersmuster leicht erlernen lassen, k\u00f6nnen sie in der gew\u00fcnschten Altersgruppe keine zufriedenstellenden Ergebnisse erzielen. Die altersgruppenbasierte Synthese teilt den Langzeitverlauf in mehrere unabh\u00e4ngige Gruppen auf und f\u00fcgt Identit\u00e4tserhaltung zwischen Eingabe und Ausgabe hinzu. Jedoch werden die fortschreitende \u00c4nderung des Altersmuster und Identit\u00e4tserhaltung zwischen den synthetisierten Bildern ignoriert. Um diese Probleme zu l\u00f6sen wurden verschiedene Arten von Deep-Learning-Methoden entwickelt.</p> <p>In den folgenden Kapiteln werden wir die Herausforderungen bei der Erstellung der Datens\u00e4tze und der Entwicklung solcher neuronalen Netzwerke in ihren verschiedenen Methoden genauer beleuchten.</p>"},{"location":"Themen/Face_Aging/#stand-der-forschung","title":"Stand der Forschung","text":""},{"location":"Themen/Face_Aging/#datensatze","title":"Datens\u00e4tze","text":"<p>In diesem Abschnitt wollen wir die wichtigsten aktuell verf\u00fcgbaren und verwendeten Datens\u00e4tze f\u00fcr das Thema Face Aging vorstellen. Diese Datens\u00e4tze wurden haupts\u00e4chlich in den Papern verwendet, die im Kapitel Methoden genauer vorgestellt werden. Bei MORPH und CACD handelt es sich dabei um die meistverwendeten Datens\u00e4tze.</p>"},{"location":"Themen/Face_Aging/#morph","title":"MORPH","text":"<p>Im Jahr 2006 gab es nur drei \u00f6ffentlich zug\u00e4ngliche, bekannte Datenbanken, die Doppelbilder einer Person in verschiedenen Altersstufen enthielten. MORPH, FERET und FG-NET.</p> <p>MORPH war zu diesem Zeitpunkt die einzige dieser drei Datenbanken, die die ethnische Zugeh\u00f6rigkeit, die Gr\u00f6\u00dfe, das Gewicht und das Geschlecht der Probanden erfasste. Diese Eigenschaften sind f\u00fcr das Verst\u00e4ndnis der Ver\u00e4nderung des Aussehens des menschlichen Gesichts im Alter von entscheidender Bedeutung. Zus\u00e4tzlich beinhaltete diese Datenbank den gr\u00f6\u00dften Satz \u00f6ffentlich verf\u00fcgbarer Bilder von Personen \u00fcber einen l\u00e4ngeren Zeitraum, von einigen Monaten bis zu einer Zeitspanne von mehreren Jahrzehnten. Bei den damaligen Methoden war diese Eigenschaft Voraussetzung f\u00fcr die Erstellung eines erfolgreichen Modells.</p> <p>Seitdem wird der Datensatz st\u00e4ndig weiterentwickelt. Aktuell gibt es den Datensatz der University of North Carolina Wilmington in drei unterschiedliche Varianten: Dem MORPH Commercial Set, MORPH Academic Set und die MORPH Longitudinal Database. Um den Datensatz zu erhalten, muss die Universit\u00e4t pers\u00f6nlich kontaktiert werden. Anschlie\u00dfend wird eine Lizenz ausgestellt und Zugriff auf die Daten gew\u00e4hrt. Weitere Informationen finden Sie auf dieser Website.</p> <p> </p> Beispiel Bilder MORPH Datensatz"},{"location":"Themen/Face_Aging/#cross-age-celebrity-dataset-cacd","title":"Cross-Age Celebrity Dataset (CACD)","text":"<p>Der CACD Datensatz beinhaltet \u00fcber 160.000 Bilder von \u00fcber 2.000 ber\u00fchmten Pers\u00f6nlichkeiten. Diese Bilder stammen aus dem Internet und wurden automatisch \u00fcber Suchmaschinen gesammelt und gespeichert. Dabei dienten die Namen der Personen und das Jahr (2004-2013) als Schl\u00fcsselw\u00f6rter. Das Alter einer Person wurde bestimmt, indem das Geburtsjahr der Person von dem Jahr subtrahiert wurde, indem das Foto aufgenommen wurde. Zus\u00e4tzliche zu den Bildern gibt es einen Metadatensatz im MATLAB-Format, welcher wichtige Informationen zu den Bildern beinhaltet. Hierbei handelt es sich z.B. um den Namen und das Alter der Person. </p> <p> </p> Beispiel Bilder CACD Datensatz <p>Der Datensatz und die Metadaten k\u00f6nnen auf der Homepage heruntergeladen werden.</p>"},{"location":"Themen/Face_Aging/#fg-net","title":"FG-NET","text":"<p>Eingef\u00fchrt wurde der Datensatz FG-NET mit dem Paper Toward automatic simulation of aging effects on face images. Insgesamt beinhaltet er \u00fcber 1.000 Bilder von 82 Personen. Es wird beim Alter eine Spanne von 0 bis 69 Jahren und ein Altersunterschied von bis zu 45 Jahren abgedeckt. Der Datensatz auf dieser Webseite verf\u00fcgbar.</p>"},{"location":"Themen/Face_Aging/#imdb-wiki","title":"IMDB-WIKI","text":"<p>Im IMDB-WIKI Datensatz wurden eine gro\u00dfe Sammlung von Bildern und Metadten von Ber\u00fchmtheiten zusammengestellt. Hier sammelten die Ersteller automatisch das Geburtsdatum, den Namen, das Geschlecht und alle Bilder der 100.000 beliebtesten Schauspieler auf der IMDb-Website. Zus\u00e4tzlich wurden zu diesen Personen alle Profilbilder von Personenseiten aus Wikipedia mit denselben Metadaten automatische gesammelt. Entfernt wurden die Bilder, die keinen Zeitstempel hatten. Das reale Alter der Person auf einem Bild wurde durch das Geburtsdatum und den Zeitstempel des Bildes errechnet. Der Datensatz beinhaltet \u00fcber 460.000 Gesichtsbilder von mehr als 20.000 Prominenten aus IMDb und mehr als 62.000 Bilder aus Wikipedia. Insgesamt umfasst er \u00fcber 520.000 Bilder.</p> <p> </p> Beispiel Bilder IMDB-WIKI Datensatz <p>Auf der Webseite des Datensatzes k\u00f6nnen die Bilder und Metadaten in verschiedensten Varianten heruntergeladen werden. Auch bereits vortrainierte Gewichte zu implementierten Modellen sind hier zu finden.</p>"},{"location":"Themen/Face_Aging/#generative-adversarial-networks-gans","title":"Generative Adversarial Networks (GANS)","text":"<p>Mit der folgenden Abbildung wollen wir die Entwicklung der GAN-Methoden darstellen, die wir f\u00fcr dieses Projekt betrachtet haben. Die Zeitreihe stellt die jeweiligen Paper mit dem Jahr der Ver\u00f6ffentlich dar.</p> <p> </p> Entwicklung GAN Methoden"},{"location":"Themen/Face_Aging/#methoden","title":"Methoden","text":""},{"location":"Themen/Face_Aging/#physikalische-modellbasierte-methoden","title":"Physikalische modellbasierte Methoden","text":"<p>Physikalische, modellbasierte Methoden befassen sich mit dem Entwurf eines komplexen Modells zur Nachahmung des Gesichtsaussehens und zur Simulation von Alterungsmechanismen in Bezug auf Haare, Muskeln und Haut bei Erwachsenen und mit der Anwendung spezifischer Transformationen auf eine Reihe von Orientierungspunkten oder statistischer Parameter zur Modellierung altersbedingter Formver\u00e4nderungen bei Kindern. F\u00fcr diese Methode muss jedoch ein parametrisches Modell erstellt werden, und es werden viele Gesichter derselben Identit\u00e4ten in verschiedenen Altersstufen ben\u00f6tigt, was rechenintensiv und schwer zu erfassen ist.</p>"},{"location":"Themen/Face_Aging/#prototypbasierte-methoden","title":"Prototypbasierte Methoden","text":"<p>Die prototypenbasierten Methoden verwenden ein nichtparametrisches Modell. Die Gesichter sollten zun\u00e4chst in Gruppen nach verschiedenen Altersgruppen eingeteilt werden. Das durchschnittliche Gesicht jeder Altersgruppe wird als Prototyp und Altersmuster einer bestimmten Altersgruppe bezeichnet. Im Paper Personalized Age Progression with Aging Dictionary hat das Autorenteam eine auf W\u00f6rterb\u00fcchern basierende Alterssynthesemethode vorgeschlagen. Yang et al. haben mit Hilfe der Hidden Factor Analysis eine gemeinsame sp\u00e4rliche Darstellung eingef\u00fchrt. Diese vorgeschlagenen Alterungsmethoden modellieren getrennt die stabilen personenabh\u00e4ngigen Eigenschaften \u00fcber einen relativ langen Zeitraum und die altersabh\u00e4ngigen Informationen, die sich im Laufe der Zeit allm\u00e4hlich \u00e4ndern. Da das Altersmuster jedoch aus dem Durchschnittsgesicht gewonnen wird, tendieren prototypbasierte Methoden dazu die identit\u00e4tsgebenden Merkmale eines speziellen Gesichtes zu verlieren.</p>"},{"location":"Themen/Face_Aging/#deep-generative-networks","title":"Deep Generative Networks","text":"<p>Die beiden obengenannten Ans\u00e4tze erfordern jedoch h\u00e4ufig die Erstellung von Alterungssequenzen derselben Person mit einem breiten Altersspektrum, deren Erfassung sehr schwierig und kostspielig ist. Generative Adversarial Networks (GANs) ben\u00f6tigen keine gepaarten Bilder von Gesichtern und erzielen dabei eine bessere Alterungsleistung als diese Methoden.</p> <p>Bei den nachfolgend vorgestellten Methoden gehen wir ganz grob auf das jeweilige Paper ein. Diese wurden verlinkt und sind auch in der Literaturliste zu finden.</p>"},{"location":"Themen/Face_Aging/#generative-adversarial-networks-gans_1","title":"Generative Adversarial Networks (GANs)","text":"<p>Generative Adversarial Networks sind tiefe neuronale Netzwerke. Sie nutzen unbeaufsichtigtes maschinelles Lernen um Daten zu generieren. Eingef\u00fchrt wurden solche Netze 2014 in einem Paper von Ian Goodfellow, Yoshua Bengio, and Aaron Courville.</p> <p>Ein solches neuronales Netzwerk besteht aus zwei weiteren Netzwerken. Einem Generator Netzwerk und einem Discriminator Netzwerk. Durch mehrere Zyklen von Generierung und Diskriminierung neuer Inhalte trainieren sich beide Netzwerke gegenseitig und versuchen gleichzeitig, sich gegenseitig zu \u00fcberlisten. Das Ziel solcher Netze ist es, Datenpunkte zu generieren, die einigen Datenpunkten im Trainingssatz so stark \u00e4hneln, dass sie vom Discriminator Netz nicht mehr als KI-generiert erkannt werden.</p>"},{"location":"Themen/Face_Aging/#pytorch-example","title":"PyTorch Example","text":"<p>In diesem kurzen Code-Beispiel wollen wir eine einfache Implementierung eines Generative Adversarial Networks mit dem PyTorch-Framework zeigen. Den kompletten Code inklusive dem Laden der Daten und dem Trainieren findet ihr in der GIT Repository unter dem Kapitel <code>01_GAN</code>. </p> <p>Note</p> <p>Wichtig ist hier noch zu nennen, dass diese Implementierung noch nicht den Aspekt Face Aging ber\u00fccksichtigt. Mit diesem GAN k\u00f6nnen nur Gesichter anhand des Trainingdatensatzes generiert werden. Ohne Bedingungen etc.</p> <p>Wir starten mit der Implementierung der Generator-Netzwerks. Hierbei erben wir von der <code>nn.Module</code> Klasse. Dies ist die Basisklasse f\u00fcr alle neuronalen Netzwerke in PyTorch. </p> <p>In der <code>_init_network</code> Methode definieren wir die einzelnen Schichten des jeweiligen neuronalen Netzwerks. Hierzu f\u00fcgen wir die einzelnen Klassen der Liste mit dem Namen <code>layer</code> hinzu. Anschlie\u00dfend \u00fcbergeben wir diese an einen <code>Sequential</code>-Container. Dieser erm\u00f6glicht einen einfachen Aufruf der <code>forward()</code>-Methode, da dieser die Ausgaben einer Schicht mit den Eingaben des nachfolgenden Moduls automatisch miteinander \"verkettet\". Schlie\u00dflich wird die Ausgabe des letzten Moduls zur\u00fcckgegeben. </p> <pre><code>class Generator(nn.Module):\n\"\"\"\n    Generator.\n    \"\"\"\n    def __init__(self, z_dim, n_feature_maps, n_channels):\n        super(Generator, self).__init__()\n\n        self.z_dim = z_dim\n        self.n_feature_maps = n_feature_maps\n        self.n_channels = n_channels\n\n        self._init_network()\n\n    def _init_network(self):\n        layer = []\n\n        # input is Z, going into a convolution\n        layer.append(nn.ConvTranspose2d(\n            in_channels=self.z_dim,\n            out_channels=self.n_feature_maps * 8,\n            kernel_size=4,\n            stride=1,\n            padding=0,\n            bias=False\n        ))\n\n        layer.append(nn.BatchNorm2d(self.n_feature_maps * 8))\n        layer.append(nn.ReLU(True))\n\n        # state size. ``(ngf*8) x 4 x 4``\n        layer.append(nn.ConvTranspose2d(\n            in_channels=self.n_feature_maps * 8,\n            out_channels=self.n_feature_maps * 4,\n            kernel_size=4,\n            stride=2,\n            padding=1,\n            bias=False\n        ))\n\n        layer.append(nn.BatchNorm2d(self.n_feature_maps * 4))\n        layer.append(nn.ReLU(True))\n\n        # state size. ``(ngf*4) x 8 x 8``\n        layer.append(nn.ConvTranspose2d(\n            in_channels=self.n_feature_maps * 4,\n            out_channels=self.n_feature_maps * 2,\n            kernel_size=4,\n            stride=2,\n            padding=1,\n            bias=False\n        ))\n\n        layer.append(nn.BatchNorm2d(self.n_feature_maps * 2))\n        layer.append(nn.ReLU(True))\n\n        # state size. ``(ngf*2) x 16 x 16``\n        layer.append(nn.ConvTranspose2d(\n            in_channels=self.n_feature_maps * 2,\n            out_channels=self.n_feature_maps,\n            kernel_size=4,\n            stride=2,\n            padding=1,\n            bias=False\n        ))\n\n        layer.append(nn.BatchNorm2d(self.n_feature_maps))\n        layer.append(nn.ReLU(True))\n\n        # state size. ``(ngf) x 32 x 32``\n        layer.append(nn.ConvTranspose2d(\n            in_channels=self.n_feature_maps,\n            out_channels=self.n_channels,\n            kernel_size=4,\n            stride=2,\n            padding=1,\n            bias=False\n        ))\n\n        layer.append(nn.Tanh())\n\n        self.main = nn.Sequential(*layer)\n\n    def forward(self, x):\n\"\"\"\n        Forward operation for the generator network.\n        \"\"\"\n        return self.main(x)\n</code></pre> <p>Mit der n\u00e4chsten Klasse implementieren wir das Diskriminator-Netzwerk. Das vorgehen ist identisch mit dem des Generator-Netzwerks von oben.</p> <pre><code>class Discriminator(nn.Module):\n\"\"\"\n    Discriminator.\n    \"\"\"\n    def __init__(self, n_channels, n_feature_maps):\n        super(Discriminator, self).__init__()\n\n        self.n_channels = n_channels\n        self.n_feature_maps = n_feature_maps\n\n        self._init_network()\n\n    def _init_network(self):\n        layers = []\n\n        # input is ``(nc) x 64 x 64``\n        layers.append(nn.Conv2d(\n            self.n_channels,\n            self.n_feature_maps,\n            4,\n            2,\n            1,\n            bias=False\n        ))\n\n        layers.append(nn.LeakyReLU(0.2, inplace=True))\n\n        # state size. ``(ndf) x 32 x 32``\n        layers.append(nn.Conv2d(\n            self.n_feature_maps,\n            self.n_feature_maps * 2,\n            4,\n            2,\n            1,\n            bias=False\n        ))\n        layers.append(nn.BatchNorm2d(self.n_feature_maps * 2))\n        layers.append(nn.LeakyReLU(0.2, inplace=True))\n\n        # state size. ``(ndf*2) x 16 x 16``\n        layers.append(nn.Conv2d(\n            self.n_feature_maps * 2,\n            self.n_feature_maps * 4,\n            4,\n            2,\n            1,\n            bias=False\n        ))\n        layers.append(nn.BatchNorm2d(self.n_feature_maps * 4))\n        layers.append(nn.LeakyReLU(0.2, inplace=True))\n\n        # state size. ``(ndf*4) x 8 x 8``\n        layers.append(nn.Conv2d(\n            self.n_feature_maps * 4,\n            self.n_feature_maps * 8,\n            4,\n            2,\n            1,\n            bias=False\n        ))\n        layers.append(nn.BatchNorm2d(self.n_feature_maps * 8))\n        layers.append(nn.LeakyReLU(0.2, inplace=True))\n\n        # state size. ``(ndf*8) x 4 x 4``\n        layers.append(nn.Conv2d(\n            self.n_feature_maps * 8,\n            1,\n            4,\n            1,\n            0,\n            bias=False\n        ))\n        layers.append(nn.Sigmoid())\n\n        self.main = nn.Sequential(*layers)\n\n    def forward(self, x):\n\"\"\"\n        Forward Operation.\n        \"\"\"\n        return self.main(x)\n</code></pre>"},{"location":"Themen/Face_Aging/#fazit","title":"Fazit","text":"<p>Verschiedene Varianten der GAN-basierten Methode k\u00f6nnen im Bereich Face Aging die plausibelsten und realistischsten Bilder erzeugen, die aufgrund des Alters nur schwer von echten Daten zu unterscheiden sind. Allerdings nutzen sie die sequentiellen Daten nicht vollst\u00e4ndig aus. Diese Methoden k\u00f6nnen die \u00dcbergangsmuster, die als Korrelationen der Gesichtsmerkmale zwischen verschiedenen Altersgruppen f\u00fcr eine Person definiert sind, nicht explizit ber\u00fccksichtigen. Daher sind ihre Ergebnisse meist nicht in der Lage, die Gesichtsidentit\u00e4t beizubehalten oder die \u00dcbergangsregeln zwischen verschiedenen Altersgruppen zu ber\u00fccksichtigen.</p> <p>Um diese Probleme zu l\u00f6sen und den Alterungsprozess von Personen mit Hilfe von GANs noch detaillierter und realistischer darzustellen, wurden verschiedene Variationen von diesen Netzwerken entwickelt.</p>"},{"location":"Themen/Face_Aging/#conditional-generative-adversiral-networks","title":"Conditional Generative Adversiral Networks","text":"<p>Ebenfalls im Jahr 2014 f\u00fchrten die Autoren Mehdi Mirza und Simon Osindero das Conditional Generative Adversarial Network ein. Bei einem nicht-konditionierten GAN gibt es keine Kontrolle \u00fcber die Modi der erzeugten Daten. Durch die Konditionierung des Modells auf zus\u00e4tzliche Informationen ist es jedoch m\u00f6glich, den Datengenerierungsprozess zu steuern. Eine solche Konditionierung k\u00f6nnte auf Klassenlabels, auf einem Teil der Daten f\u00fcr Inpainting oder sogar auf Daten aus verschiedenen Modalit\u00e4ten basieren. </p> <p>GANs k\u00f6nnen zu einem konditionalen Modell (conditional GAN) erweitert werden, wenn sowohl der Generator als auch der Diskriminator durch eine zus\u00e4tzliche Information konditioniert werden. Die Konditionierung kann durchgef\u00fchrt werden, indem die Information sowohl in den Diskriminator als auch in den Generator als zus\u00e4tzliche Eingabeschicht eingespeist wird.</p> <p> </p> Conditional Adversarial Net <p>Die Ergebnisse des Papers demonstrierten das Potenzial von solchen Netzwerken.  </p>"},{"location":"Themen/Face_Aging/#face-aging-with-conditional-generative-adversarial-networks","title":"Face Aging with Conditional Generative Adversarial Networks","text":"<p>Auch mit dem n\u00e4chsten Paper - \"Face Aging with Conditional Generative Adversarial Networks\" -, aus dem Jahr 2017, wurde versucht, die Probleme mit dem Verlust der Identit\u00e4t der urspr\u00fcnglichen Person in modifizierten Bildern zu minimieren. Daher konzentriert sich diese Studie auf die identit\u00e4tserhaltende Gesichtsalterung. Daf\u00fcr entwickelten die Autoren das Age-cGAN (Age Conditional Generative Adversarial Network). Es sollte das erste GAN sein, das qualitativ hochwertige synthetische Bilder innerhalb der geforderten Alterskategorien erzeugt. Zus\u00e4tzlich schlug das Team einen neuartigen Ansatz zur Optimierung latenter Vektoren vor, der es Age-cGAN erm\u00f6glichte, ein eingegebenes Gesichtsbild zu rekonstruieren, ohne die Identit\u00e4t der urspr\u00fcnglichen Person zu ver\u00e4ndern.</p> <p>Ein Conditional GAN (cGAN) erweitert das GAN-Modell und erm\u00f6glicht die Erzeugung von Bildern mit bestimmten Bedingungen (Attributen). Diese Bedingungen k\u00f6nnen jede beliebige Information beinhalten, die sich auf das Zielgesichtsbild bezieht. Zum Beispiel Beleuchtungsniveau, Gesichtshaltung oder Gesichtsattribute. Die bedingte Information wird dabei in den Eingang des Generators und in die erste Faltungsschicht (Convolution Layer) von dem Discriminator Netzwerk injiziert.</p> <p>Conditional GANs verf\u00fcgen herk\u00f6mmlicherweise nicht \u00fcber einen expliziten Mechanismus zur inversen Abbildung eines Eingangsbildes \\(x\\) mit Attributen \\(y\\) auf einen latenten Vektor \\(z\\), der f\u00fcr die Bildrekonstruktion notwendig ist. Die Autoren umgehen dieses Problem, indem ein Encoder \\(E\\) trainiert wird. Bei diesen handelt es sich um ein neuronales Netz, das die inverse Abbildung ann\u00e4hert. Dieser erzeugt anf\u00e4ngliche latente Approximationen, die gut genug sind, um als Initalisierungen f\u00fcr den Optimierungsalgorithmus zu dienen. Das Team nutzt einen neuartigen \"identit\u00e4tserhaltenden Ansatz\" (Identity-Preserving) zur Optimierung latenter Vektoren. Hierbei lautet der Grundgedanke folgenderma\u00dfen: Bei einem neuronalen Netz zur Gesichtserkennung (Face Recognition), das in der Lage ist, die Identit\u00e4t einer Person in einem eingegebenen Gesichtsbild zu erkennen, kann der Unterschied zwischen den Identi\u00e4ten in den urspr\u00fcnglichen und rekonstruiertern Bildern als euklidischer Abstand zwischen den entsprechenenden Einbettungen ausgedr\u00fcckt werden. Daher sollte die Minimierung dieses Abstands die Erhaltung der Identit\u00e4t im rekonstruierten Bild verbessern.</p> <p> </p> Verwendete Methode. (a) Approximative Gesichtsrekonstruktion mit Age-cGAN (b) Wechseln der Altersbedingung am Eingang des Generators G, um die Gesichtsalterung durchzuf\u00fchren. <p>In der folgenden Abbildung werden Beispiele f\u00fcr die Rekonstruktion und Alterung von Gesichtern dargestellt. </p> <ul> <li>(a) Zeigt die originalen Testbilder</li> <li>(b) Zeigt die rekonstruierten Bilder</li> <li>(c) Zeigt die rekonstruierten Bilder, die inklusive den \"pixelweisen\" und \"identit\u00e4tserhaltenden\" Methoden generiert wurden</li> <li>(d) Zeigt die rekonstruierten Bilder, die unter Verwendung der identit\u00e4tserhaltenden Approximationen und konditioniert auf die jeweilige Alterskategorie (eine pro Spalte) generiert wurden</li> </ul> <p> </p> Beispiele von generierten Bildern druch das Age-c-GAN <p>Die Autoren kamen zu dem Schluss, dass die Gesichtsrekonstruktion mit ihrer Methode weiter verbessert werden kann, indem \"pixelweise\" (pixelwise) und \"identit\u00e4tserhaltende\" (Identity-Preserving) Ans\u00e4tze in einem Optimizerungsziel kombiniert werden.</p>"},{"location":"Themen/Face_Aging/#face-aging-with-contextual-generative-adversarial-networks","title":"Face Aging with Contextual Generative Adversarial Networks","text":"<p>Im Jahr 2018 ver\u00f6ffentlichten eine Autorengruppe um Liu et. al. ein Paper mit dem Titel \"Face Aging with Contextual Generative Adversarial Nets\". Im Gegensatz zu traditionellen GANs, die nur die reale Datenverteilung jedes einzelnen Alters modellieren, konzentrierte sich das Team auf die alters\u00fcbergreifenden Korrelationen h\u00f6herer Ordnung, die die Ergebnisse der Gesichtsalterung attraktiver machen sollten. Um dies zu realisieren, schlugen die Autoren die Umsetzung der Gesichtsalterung mittels eines \"Contextual Generative Adversarial Networks (C-GANs)\" vor.</p> <p> </p> Vorgeschlagener C-GANs-Algorithmus f\u00fcr die Gesichtsalterung. <p>Conextual-GANs bestehen aus drei neuronalen Netzwerken. Um sicherzustellen, dass die erzeugten Bilder echt wirken, werden zwei diskriminierende Netzwerke verwendet, um die Verteilung jeder einzelnen Altersgruppe sowie die \u00dcbergangspaare von zwei benachbarten Gruppen zu modellieren. Das altersdiskriminierende Netz (Age Discriminative Network) hilft bei der Erzeugung von Bildern, die von den echten nicht zu unterscheiden sind. Das bedingte Transformationsnetzwerk (Conditional Transformation Network) transformiert das Gesicht der Eingabe in das gew\u00fcnschte Alter. Das diskriminierende Netz f\u00fcr \u00dcbergangsmuster (Transition Pattern Discriminative Network) reguliert die erzeugten Bilder, damit sie den den Alterungsregeln entsprechen.</p> <p> </p> Struktur des vorgeschlagenen C-GANs. <p>In der n\u00e4chsten Abbildung werden die generierten Gesichter qualitativ verglichen mit der Grundwahrheit dargestellt. In jedem Triplett sind das erste und dritte Bild die Grundwahrheiten, jeweils in Altersgruppe 1 und Altersgruppe 2, w\u00e4hrend das zweite Bild das generierte Alterungsergebnis ist.</p> <p> </p> Ergebnisse Contextual GANs"},{"location":"Themen/Face_Aging/#face-aging-with-identity-preserved-conditional-generative-adversarial-networks","title":"Face Aging With Identity-Preserved Conditional Generative Adversarial Networks","text":"<p>In diesem Paper, welches 2018 ver\u00f6ffentlicht wurde, schlug das Autorenteam ein Identity-Preserved Conditional Generative Adversarial Network (IPCGAN) - zu Deutsch: identit\u00e4tserhaltendes bedingtes generatives adversariales Netzwerk - f\u00fcr die Gesichtsalterung vor. Dieses besteht aus drei Modulen. Einem Conditional Generative Adversarial Network (CGAN), einem identit\u00e4tserhaltenden (Identity-Preserved) Modul und einem Altersklassifikator.</p> <p>Als Eingabe f\u00fcr den Generator im IPCGAN wird ein Eingabebild und eine Zielaltersklasse verwendet. Es wird versucht ein Gesicht mit dem Alter in der Zielaltersklasse zu erzeugen. Das generierte Gesicht soll sich nicht von realen Gesichtern in der Zielaltersgruppe unterscheiden. Um die Identit\u00e4tsinformationen zu erhalten, wird ein Wahrnehmungsverlust (Perceptual Loss) eingef\u00fchrt und um zu garantieren, dass die synthetisierten Gesichter in die Zielaltersgruppe fallen, werden die erzeugten gealterten Gesichter an einen vortrainiertern Alterklassifikator \u00fcbergeben und ein Altersklassifikationsverlust hinzugef\u00fcgt.   </p> <p> </p> Struktur des vorgeschlagenen IPCGAN. <p>Das folgende Bild zeigt ein generiertes Beispielbild des IPCGAN Ansatzes.</p> <p> </p> Generiertes Beispiel des IPCGAN"},{"location":"Themen/Face_Aging/#learning-face-age-progression-a-pyramid-architecture-of-gans","title":"Learning Face Age Progression: A Pyramid Architecture of GANs","text":"<p>2019 ver\u00f6ffentlichten die Autoren Yang et. al. ein Paper mit dem Titel \"Learning Face Age Progression: A Pyramid Architecture of GANs\", in dem ein neuartiger Ansatz zur Alterung von Gesichtern vorgeschlagen wurde. Hierbei werden die Vorteile von Generative Adversarial Networks (GAN) bei  der Synthese visuell plausibler Bilder mit Vorwissen \u00fcber die menschliche Alterung verbunden. Die Autoren versprechen, dass ihr Modell im Vergleich zu bestehenden Methoden in der Literatur besser in der Lage ist, die beiden kritischen Anforderungen bei der Altersentwicklung zu erf\u00fcllen, d.h. Identit\u00e4tsbest\u00e4ndigkeit und Alterungsgenauigkeit.</p> <p>In dieser Methode nimmt der Convolutional Neural Network (CNN) basierte Generator junge Gesichter als Input und lernt eine Zuordnung zu einem Bereich, der \u00e4lteren Gesichtern entspricht. Um Alterungseffekte zu erzielen und gleichzeitig personenspezifische Informationen beizubehalten, wird ein zusammengesetzter Loss verwendet.</p> <p> </p> Framework der vorgeschlagenen Methode der Altersprogression. <p>Beispiele f\u00fcr die Ergebnisse der Altersentwicklung sind in der n\u00e4chsten Abbildung zu sehen. Es werden visuell plausible und \u00fcberzeugende Alterungseffekte erzielt, obwohl die Beispiele eine breite Palette von Personen in Bezug auf Ethnie, Geschlecht, Pose, Make-up und Ausdruck abdecken.</p> <p> </p> Alterungseffekte, die f\u00fcr die CACD (die ersten beiden Zeilen) und MORPH (die letzten beiden Zeilen) Datens\u00e4tze f\u00fcr 12 verschiedene Personen generiert wurden."},{"location":"Themen/Face_Aging/#triple-gan-progressive-face-aging-with-triple-translation-loss","title":"Triple-GAN: Progressive Face Aging with Triple Translation Loss","text":"<p>Mit dem Triple-GAN aus dem Jahr 2020 wollten die Autoren die Probleme bei der Gesichtsalterung mittels Deep-Learning-Methoden l\u00f6sen. Konkret wollten sie die nicht zufriedenstellenden Ergebnisse in der gew\u00fcnschten Altersgruppe, die Ignorierung der fortschreitenden Ver\u00e4nderung der Altersmuster und die Identit\u00e4tserhalutng im synthetisierten Bild l\u00f6sen. Dazu erforschten sie, wie man verschiedene Altersmuster gleichzeitig \u00fcbersetzen kann, um mehrere Trainingsphasen f\u00fcr kontradiktorisches Lernen zu erhalten. Das Discriminator-Netzwerk wurde so angepasst, dass es nicht nur auf der Ebene von echt und falsch unterscheidet, sondern auch effiziente Zuordnungen zwischen Mustern und Bezeichnungen erstellt, indem verschiedene Altersmuster gemeinsam erlernt wurden. Um die Altersbeziehungen zwischen den verschiedenen Altersgruppen zu modellieren, wurde die Leistung des Generators verbessert und zus\u00e4tzlich eine dreifache \u00dcberstzung (Triple-Translation) hinzugef\u00fcgt. Diese hilft dabei, das synthetisierte Gesicht eines bestimmten Alters in ein anderes Alter zu \u00fcbersetzern. </p> <p> </p> Pipeline f\u00fcr die dreifache \u00dcberstzung (Triple Translation) <p>Durch die Verwendung eines dreifachen \u00dcbersetzungsverlust (Triple Translation Loss) werden verschiedene synthetisierte Gesichter der gleichen Zielaltersgruppe gezwungen, hohe \u00c4hnlichkeit aufzuweisen. So kann die \u00dcbersetzung von Altersmustern korreliert werden, um progressive und kontinuierliche Ver\u00e4nderungen in der Gesichtsalterung besser zu simulieren.</p> <p>Das Framework der Autoren beinhaltet vier Komponenten:</p> <ol> <li>Generator Netzwerk</li> <li>Pre-Trained Identity-Preserved Netzwerk</li> <li>Pre-Trained Age Classification Netzwerk</li> <li>Discriminator Netzwerk</li> </ol> <p> </p> Framework des vorgeschlagenen Triple-GAN f\u00fcr Gesichtsalterung <p>Die Ergebnisse des Triple-GANs in der folgenden Abbildung zeigen, dass die generiertern Bilder einen offensichtlichen Alterungseffekt und eine gut erhaltene Identit\u00e4t vorweisen k\u00f6nnen.</p> <p> </p> Ergebnisse Triple-GAN"},{"location":"Themen/Face_Aging/#only-a-matter-of-style-age-transformation-using-a-style-based-regression-model","title":"Only a Matter of Style: Age Transformation Using a Style-Based Regression Model","text":"<p>Mit dem Paper Only a Matter of Style: Age Transformation Using a Style-Based Regression Model stellten die Autoren eine Implementierung namens SAM -Style-based Age Manipulation - vor. Hierbei versuchten sie die gew\u00fcnschte Altersver\u00e4nderung zu erfassen und gleichzeitig die Identit\u00e4t zu bewahren. Die Gesichtsalterung wurde dabei durch eine Bild-zu-Bild \u00dcbersetzung (Image-to-Image Translation) versucht gel\u00f6st zu werden. Zu dieser Technik geh\u00f6ren auch die Conditional GANs, die wir weiter oben bereits erw\u00e4hnt haben. In der Forschungsarbeit wird ein vortrainierter (pre-trained) StyleGAN-Generator mit einer Encoder-Architektur kombiniert. Der Encoder hat die Aufgabe, ein Gesichtsbild als Eingabe direkt in eine Reihe von Stilvektoren zu kodieren, die der gew\u00fcnschten Altersver\u00e4nderung unterliegen. Diese Vektoren werden anschlie\u00dfend an StyleGAN \u00fcbergeben, um das Ausgangsbild zu erzeugen, das die gew\u00fcnschte Altersver\u00e4nderung darstellt. Der Encoder wird bei der Generierung durch ein vortrainiertes Alterregressionsnetzwerk w\u00e4hrend des Trainingsprozesses als zus\u00e4tzliche Einschr\u00e4nkung angeleitet. SAM betrachtet die menschliche Alterung also als ein Regressionsproblem auf das gew\u00fcnschte Zielalter hin.</p> <p> </p> Architektur des SAM Netzwerks <p> </p> Mit SAM erzeugte Alterungsergebnisse <p>Die Ergebnisse dieser Methode werden durch die Style-Repr\u00e4sentation bestimmt. D.h. sie ist auf Bilder beschr\u00e4nkt, die genau in den latenten Raum von StyleGAN eingebettet werden k\u00f6nnen. Die Modellierung von Gesichtern, die au\u00dferhalb des StyleGAN-Bereichs liegen, kann daher eine Herausforderung darstellen. Ebenso kann es durch die Einbettung eines Bildes in eine Reihe von Vektoren schwieriger werden, die Eingangsmerkmale wie den Bildhintergrund originalgetreu zu erhalten. In den Evaluierungen wurde gezeigt, dass die vorgeschlagene Methode das Alter und andere Merkmale wie Haarfarbe und Frisur erfolgreich voneinander trennt. Allerdings \u00e4ndern sich solche Attribute nat\u00fcrlich mit dem Alter. Um diese Ver\u00e4nderungen zu modellieren, wurden daher zwei Bearbeitungstechniken zur Kontrolle globaler Ver\u00e4nderungen (z. B. Haarfarbe) und lokaler Ver\u00e4nderungen (z. B. das Vorhandensein von Brillen und Gesichtsbehaarung) vorgeschlagen. Die Erfassung komplexerer Ver\u00e4nderungen, wie z. B. zur\u00fcckweichende Haarlinien und Ver\u00e4nderungen der Hautfarbe, ist mit diesert Methode aber nach wie vor eine Herausforderung.</p>"},{"location":"Themen/Face_Aging/#pfa-gan-progressive-face-aging-with-generative-adversarial-network","title":"PFA-GAN: Progressive Face Aging With Generative Adversarial Network","text":"<p>Die Autoren von diesem Paper nutzen die Tatsache, dass Gesichter im Laufe der Zeit fortlaufend altern und modellieren den Alterungsprozess im Gesicht daher in einer progressiven Weise mit ihrem neuen progressive Face-Aging-Framework, das auf einem GAN basiert (Progressive Face Aging with Generative Adversarial Network - PFA-GAN). Dieses besteht aus mehreren kleinen Generator-Subnetzwerken, die sich jeweils nur  mit spezifischen Alterungseffekten zwischen zwei angrenzenden Altersgruppen befassen. Der Hauptunterschied zu anderen GAN-basierten Methoden besteht darin, dass das PFA-GAN die Subnetze gleichzeitig trainiert. Fr\u00fchere GAN-Varianten trainierten verschiedene Netzwerke unabh\u00e4ngig voneinander. </p> <p>Die Autoren heben dabei die Bedeutung der folgenden vier Aspekte f\u00fcr eine progressive Modellierung der Gesichtsalterung hervor:</p> <ol> <li>Konzentration auf die Modellierung von Gesichtsalterungseffekten zwischen zwei angrenzenden Altersgruppen</li> <li>Fortlaufende Alterungsergebnisse durch das durchg\u00e4ngige Trainieren der progressiven Gesichtsalterung</li> <li>Verbesserung der Alterungsgl\u00e4tte durch eine ordinale Beziehung zwischen den Altersgruppen</li> <li>Die Leistung der Cross-Age-Verifizierung kann verbessert werden</li> </ol> <p>Die folgende Abbildung zeigt, dass das \\(i\\)-te Teilnetzwerk \\(G_i\\) dazu dient, Gesichter von der Altersgruppe \\(i\\) zur Gruppe \\(i + 1\\) zu altern.</p> <p> </p> Der vorgeschlagene PFA-GAN f\u00fcr die Gesichtsalterung mit 4 Altersgruppen <p>Der progressive Alterungsrahmen von der Ausgangsaltersgruppe bis zur Zielaltersgruppe \\(t\\) l\u00e4sst sich wie folgt formulieren:</p> \\[X_t = \\overline{G}_{t-1} \\circ \\overline{G}_{t-2} \\circ \\dots \\circ \\overline{G}_{s}(X_s)\\] <p>In den Generator-Netzwerken werden zus\u00e4tzlich Residual-Skip-Verbindung genutzt. Diese verhindern, dass die exakte Kopie des Gesichtes des Eingabebilds \u00fcber mehrere Subnetze hinweg gespeichert wird. Durch die Einf\u00fchrung der Skip-Verbindung kann die Zielaltersgruppe leicht in eine Sequenz von bin\u00e4ren Gattern umgewandelt werden, die den Alterungsfluss steuern. </p> <p>Die Ver\u00e4nderungen von der Altersgruppe \\(i\\) zu \\(i + 1\\) l\u00e4sst sich mathematisch wie folgt beschreiben:</p> \\[ X_{t+1} = \\overline{G}_{i}(X_i) = X_i + \\lambda_i G_i (X_i) \\] <p>Somit besteht ein Generator-Subnetzwerk aus einer Resudial-Skip-Verbindung, einem bin\u00e4ren Gatter und dem Netzwerk an sich. Bei \\(\\lambda_i \\in \\{0,1\\}\\) handelt es sich um das bin\u00e4re Gatter, welches kontrolliert ob das Subnetzwerke \\(G_i\\) in den Alterungsprozess zum jeweiligen Zielalter mit einbezogen wird.</p> <p>Mit dem vorgeschlagenen Framework l\u00e4sst sich die Altersprogression, z.B. von der Altersgruppe 1 bis 4 wie in der obigen Abbildung dargestellt, wie folgt ausdr\u00fccken:</p> \\[ \\begin{equation} X_4 = X_3 + \\underbrace{\\lambda_3 G_3(X_3)}_\\text{Alterseffekte Gruppe 3 bis 4} \\\\ = X_2 + \\underbrace{\\lambda_2 G_2(X_2) + \\lambda_3 G_3(X_3)}_\\text{Alterseffekte Gruppe 2 bis 4} \\end{equation} \\\\ = X_1 + \\underbrace{\\lambda_1 G_1(X_1) + \\lambda_2 G_2(X_2) + \\lambda_3 G_3(X_3)}_\\text{Alterseffekte Gruppe 1 bis 4} \\] <p>Wenn jetzt die Alterung von Gruppe 2 nach Gruppe vorhergesagt werden soll, so reduziert sich die obige Gleichung auf \\(X_3 = X_2 + G_2(X_2)\\). Der Vektor f\u00fcr \\(\\lambda\\) f\u00fcr diese Generierung lautet folgenderma\u00dfen \\(\\begin{pmatrix}0 &amp; 1 &amp; 0\\end{pmatrix}\\) und somit werden die Subnetze \\(G_1\\) und \\(G_3\\) bei der Berechnung au\u00dfenvorgelassen.</p> <p>Schlie\u00dflich k\u00f6nnen wir den Alterungsprozess vom einem Eingabe-Gesicht \\(X_s\\) von einer gegebenen Altersgruppe \\(s\\) hin zu einer Zielaltersgruppe \\(t\\) wie folgt formulieren:</p> \\[X_t = G(X_s, \\lambda_{s:t})\\] <p>\\(G = \\overline{G}_{N-1} \\circ \\overline{G}_{N-2} \\circ \\dots \\circ \\overline{G}_{1}\\) beschreibt das progressive Gesichtsalterungsnetzwerk. \\(\\lambda_{s:t}\\) kontrolliert den Alterungsprozess.</p> <p>Zus\u00e4tzlich zum Generator und Diskriminator Netzwerk, die Hauptbestandteile von GANs sind, wird im PFA-GAN noch ein weiteres Netzwerk verwendet. Hierbei handelt es sich um ein Alterssch\u00e4tzungsnetzwerk (Age Estimation Network). Es dient dazu, die Gesichtsaltersverteilung f\u00fcr eine verbesserte Altersgenauigkeit besser zu charakterisieren. In fr\u00fcheren Arbeiten wurde in der Regel entweder die Alterklassifikation oder die Altersregression verwendet, um zu \u00fcberpr\u00fcfen ob das erzeugte Gesicht zur Zielaltersgruppe geh\u00f6rt. In dieser Implementierung verwendeten die Autoren den Deep Expectation (DEX) Ansatz. Das Alterssch\u00e4tzungsnetzwerk wurde vortrainiert und die erzielten Gewichte eingefroren. Es reguliert den Generator f\u00fcr eine verbesserte Alterungsgenauigkeit.</p> <p>Die folgende Abbildung zeigt die komplette Architektur des vorgeschlagenen PFA-GANs.</p> <p> </p> Das GAN Framework f\u00fcr das PFA-GAN <p>Bei diesem Ansatz werden verschiedene Losses (Verlustberechnungen) kombiniert, um die folgenden Anforderungen f\u00fcr die Gesichtsalterung ber\u00fccksichtigen:</p> <ol> <li>Adversarial Loss zielt darauf ab, qualitativ hochwertige, gealterte Gesichter zu erzeugen, die nicht von echten zu unterscheiden sind</li> <li>Der Verlust der Alterssch\u00e4tzung soll die Alterungsgenauigkeit verbessern</li> <li>Der Verlust der Identit\u00e4tskonsistenz zielt darauf ab, die gleiche Identit\u00e4t zu bewahren</li> </ol> <p>Bei einem jungen Gesicht \\(X_s\\) aus der Altersgruppe \\(s\\) ist das Ergebnis von \\(G\\) von \\(s\\) zu einer alten Altersgruppe \\(t\\) \\(G(X_s, \\lambda_{s:t})\\). Im Kontext von GANs der kleinsten Quadrate ist der gegnerische Verlust f\u00fcr den Generator \\(G\\) somit definiert als:</p> \\[L_{\\text{adv}} = \\frac{1}{2} \\mathbb{E}_{X_{s}} [D([G(X_s,\\lambda_{s:t});C_t]) - 1]^2\\] <p>Der Alterssch\u00e4tzverlust zwischen dem gesch\u00e4tzten Alter \\(\\hat{y}\\) und dem Zielalter \\(y\\) f\u00fcr den Generator \\(G\\) ist definiert als:</p> \\[L_{\\text{age}} = \\mathbb{E}_{X_{s}} [ || y - \\hat{y} ||_2 + l(A(X)W, c_t) ]\\] <p>\\(W \\in \\mathbb{R}^{101 \\times N}\\) bezeichnet die letzte vollst\u00e4ndig verbundene Schicht f\u00fcr das Altersgruppenklassifizierungsnetzwerks und \\(l\\) ist der Verlust der Kreuzentropie f\u00fcr die Altersgruppenlkassifizierung.</p> <p>Um die identit\u00e4tsbezogenen Informationen des Gesichts zu bewahren und die identit\u00e4tsirrelevanten Informationen wie den Hintergrund unver\u00e4ndert zu lassen, wird ein gemischter Identit\u00e4tskonsistenzverlust zwischen dem Eingabegesicht und dem generierten Gesicht verwendet. Hierzu z\u00e4hlen:</p> <ul> <li>ein pixelweiser Verlust (pixel-wise loss)</li> <li>ein Verlust f\u00fcr die strukturelle \u00c4hnlichkeit (Structural Similarity (SSIM) loss)</li> <li>ein Feature-Level Loss</li> </ul> <p>Diese drei sind wie folgt definiert:</p> \\[L_{\\text{pix}} = \\mathbb{E}_{X_{s}} | G(X_s, \\lambda_{s:t}) - X_s |\\] \\[L_{\\text{ssim}} = \\mathbb{E}_{X_{s}} [ 1- \\text{SSIM}(G(X_s, \\lambda_{s:t}), X_s]\\] \\[L_{\\text{fea}} = \\mathbb{E}_{X_{s}} || \\phi(G(X_s, \\lambda_{s:t})) - \\phi(X_s) ||_{F}^2 \\] <p>Schlie\u00dflich kann der Identit\u00e4tskonsistenz Verslust (Identity Consistency Loss) f\u00fcr das Generator-Netzwerk definiert werden als:</p> \\[L_{\\text{fea}} = (1 - \\alpha_{\\text{ssim}}) * L_{\\text{pix}} + \\alpha_{\\text{ssim}} * L_{\\text{ssim}} + \\alpha_{\\text{fea}} * L_{\\text{fea}}\\] <p>\\(\\alpha_{\\text{ssim}}\\) und \\(\\alpha_{\\text{fea}}\\) sind Hyperparameter, die dazu dienen die Balance zwischen den drei Verlusten zu kontrollieren.</p> <p>Der finale Verlust f\u00fcr den Generator ergibt sich aus der Zusammensetzung aller einzelnen Verlustberechnungen:</p> \\[L_G = \\lambda_{\\text{adv}} L_{\\text{adv}} + \\lambda_{\\text{age}} L_{\\text{age}} + \\lambda_{\\text{ide}} L_{\\text{ide}}\\] <p>Die einzelnen \\(\\lambda\\) dienen ebenfalls wieder als Hyperparameter.</p> <p>Die folgende Darstellung zeigt Beispielergebnisse zur Gesichtsalterung und -verj\u00fcngung durch Anwendung des PFA-GANs auf drei externe Datens\u00e4tze:</p> <ul> <li>FG-NET</li> <li>CelebA</li> <li>IMDB-WIKI</li> </ul> <p>Die roten K\u00e4stschen kennzeichenen die Einbagebilder.</p> <p> </p> Generierte Gesichter durch das PFA-GAN <p>Trotz qualitativer und quantitativer \u00dcberlegenheit des PFA-GANs gegen\u00fcber vorhergehenden Methoden, bestehen auch einige Einschr\u00e4nkungen:</p> <ul> <li>Die Haupteinschr\u00e4nkung der GAN-basierten Methoden besteht im Vergleich zu cGAN-basierten Methoden darin, dass die Netzwerke als Eingabe die Alterskennzeichnung der Quelle ben\u00f6tigen</li> <li>Das PFA-GAN muss ein zweites, umgekehrtes Modell f\u00fcr die Gesichtsverj\u00fcngung trainieren, w\u00e4hred andere Methoden das gleiche Netzwerk f\u00fcr Alterung und Verj\u00fcngung nutzen k\u00f6nnen</li> <li>mit mehr Altersgruppen oder einer kleinen Zeitspanne in jeder Altersgruppe wird es bei der Gesichtsalterung schwieriger, ein Gesichtsalterungsmodell zu trainieren, und die Muster zwischen zwei benachbarten Altersgruppen werden weniger klar, was fast alle Methoden gleicherma\u00dfen vor Herausforderungen stellt</li> </ul>"},{"location":"Themen/Face_Aging/#anwendungen","title":"Anwendungen","text":"<p>Die meisten Anwendungen findet man aktuell im Bereich der Unterhaltung. Man findet unz\u00e4hlige Apps f\u00fcr das Smartphone, sowohl f\u00fcr iOS als auch f\u00fcr Android, welche ein \u00e4lteres Bild einer Person generieren lassen.</p>"},{"location":"Themen/Face_Aging/#smokerface-app","title":"Smokerface App","text":"<p>Aber auch sinnvollere Anwendungen wurden schon umgesetzt. In der Studie A Face-Aging App for Smoking Cessation in a Waiting Room Setting: Pilot Study in an HIV Outpatient Clinic wurde eine Face-Aging App zur Intervention und Raucherentw\u00f6hnung entwickelt. Hintergrund zu dieser Studie war, dass die Einf\u00fchrung von Technologien zur Raucherentw\u00f6hnung in ambulanten Wartezimmern eine wirksame Strategie f\u00fcr eine Ver\u00e4nderung sein kann, die das Potenzial hat, fast alle Patienten die einen Gesundheitsdienstleister aufsuchen, zu erreichen, ohne dass der Arzt vorher t\u00e4tig werden muss. Das Ziel der Studie war es, eine Intervention zur Raucherentw\u00f6hnung zu entwickeln, die Patienten w\u00e4hrend Wartezeiten passiv einer Tablet-basierten App mit Gesichtsver\u00e4nderung und \u00f6ffentlichem Morphing aussetzt. Diese Intervention wurde in einem Wartezimmer einer HIV-Ambulanz getestet und die Wahrnehmung dieser Intervention unter rauchenden und nicht rauchenden HIV-Patienten gemessen. Dabei entwickelte das Team eine Kioskversion der dreidimensionalen Gesichtsalterungs-App Smokerface, die dem Benutzer zeigt, wie sein Gesicht mit oder ohne Zigarettenrauchen in 1 bis 15 Jahren aussehen w\u00fcrde. Es wurde ein Tablet mit der App auf einem Tisch in der Mitte des Wartezimmers platziert, verbunden mit einem gro\u00dfen Monitor, der an einer gegen\u00fcberliegenden Wand angebracht war. Ein Forscher notierte alle Patienten, die den Warteraum nutzten. Wenn ein Patient die App nicht innerhalb von 30 Sekunden nach Betreten nutzte, forderte der Forscher ihn auf, dies zu tun. Die Nutzer wurden danach gebeten, einen Fragebogen auszuf\u00fcllen. Die Studie kam zum Schluss, dass eine im Wartezimmer implementierte Face-Aging-App eine neuartige M\u00f6glichkeit bietet, Patienten, die einen Gesundheitsdienstleister aufsuchen, dazu zu motivieren, mit dem Rauchen aufzuh\u00f6ren, die Raucherentw\u00f6hnung bei ihrem n\u00e4chsten Termin anzusprechen und dadurch die \u00e4rztlich verordnete Raucherentw\u00f6hnung zu f\u00f6rdern.</p> <p>Nachfolgend der Link um die App auf dem eigenen Ger\u00e4t zu testen:</p> <ul> <li>Android</li> <li>iOS</li> </ul>"},{"location":"Themen/Face_Aging/#sunface-uv-selfie","title":"Sunface - UV-Selfie","text":"<p>In einer weiteren Studie mit dem Titel Effect of a Face-Aging Mobile App-Based Intervention on Skin Cancer Protection Behavior in Secondary Schools in Brazil: A Cluster-Randomized Clinical Trial wurde untersucht, wie sich eine kostenlose mobile Gesichtsalterungs-App mit den Namen Sunface auf das Hautkrebsschutzverhalten von Jugendlichen auswirkt. Da der Kontakt mit UV-Strahlung in jungen Jahren ein wichtiger Risikofaktor f\u00fcr die Entstehung von Melanomen ist, ist die Reduzierung der UV-Exposition bei Kindern und Jugendlichen von gr\u00f6\u00dfter Bedeutung. Das prim\u00e4re Ziel der Studie war der Unterschied in der t\u00e4glichen Verwendung von Sonnenschutzmitteln bei der Nachbeobachtung nach 6 Monaten. Zu den sekund\u00e4ren Zielen geh\u00f6rten der Unterschied bei der t\u00e4glichen Verwendung von Sonnenschutzmitteln nach 3 Monaten Nachbeobachtung, mindestens eine Selbstuntersuchung der Haut innerhalb von 6 Monaten und mindestens eine Br\u00e4unungssitzung in den vorangegangenen 30 Tagen. Alle Analysen wurden im Voraus festgelegt und basierten auf der Absicht, die Studie zu behandeln. Clustereffekte wurden ber\u00fccksichtigt. Die Ergebnisse dieser Studie deuten darauf hin, dass Interventionen auf der Grundlage von Apps zur Gesichtsalterung das Hautkrebsschutzverhalten brasilianischer Jugendlicher verbessern k\u00f6nnen.</p> <p>Auch diese App kann selbst getestet werden:</p> <ul> <li>Android</li> <li>iOS</li> </ul>"},{"location":"Themen/Face_Aging/#aprilage-inc","title":"AprilAge Inc.","text":"<p>Auch das Unternehmen AprilAge entwickelt Gesichts- und K\u00f6rpervisualisierungssoftware f\u00fcr verschiedene Unternehmen, die Menschen dazu bewegen und motivieren sollen, riskante Lebensgewohnheiten zu \u00e4ndern, die zu chronischen Krankheiten und hohen Behandlungskosten f\u00fchren. Die Software zeigt den Einfluss von Rauchen, erh\u00f6hter Sonnenbestrahlung und \u00dcbergewicht auf den Alterungsprozess des Gesichtes.</p>"},{"location":"Themen/Face_Aging/#fazit_1","title":"Fazit","text":"<p>Generative Adversarial Networks in verschiedenen Implementierungen l\u00f6sten die herk\u00f6mmlichen Methoden, physikalisch-modellbasierte und prototypbasierte, ab. Mit den neuen neuronalen Netzwerken ben\u00f6tigte man nicht mehr die gro\u00dfe Menge an kostspielig zu erfassenden Datens\u00e4tze. Der datengetriebene Ansatz konnte mit den Alterungsverl\u00e4ufen besser umgehen.</p> <p>Im Zuge der Forschung in anderen Bereichen, z.B. Face Recognition, wurden verschiedene Datens\u00e4tze weiterentwickelt und neu erstellt. Diese k\u00f6nnen problemlos f\u00fcr das Thema Face Aging verwendet werden. </p> <p>Seit der Einf\u00fchrung der GANs im Jahr 2014 wurden unterschiedliche Implementierungen auf deren Basis umgesetzt. Sie versuchten Bilder zu generieren, welche die Identit\u00e4t der Person bei der Alterung erhalten sollen. Dazu wurden verschiedene Techniken verwendet. Die generierten Ergebnisse der neuen neuronalen Netzwerke auf den Testdaten waren kaum von echten Bildern zu unterscheiden. Trotzdem wird die Erzeugung qualitativ hochwertiger Bilder bei extremen Posen, anspruchsvollen Ausdr\u00fccken und/oder Accessoires in Bildern unabh\u00e4ngig von den Trainings- oder Testdaten erschwert.</p> <p>Theoretische Bereiche der Anwendung gibt es zahlreiche. Wirklich viele Anwendungen findet man im Bereich der Unterhaltung. In verschiedenen Apps wird Face Aging als lustiger Filter angeboten, um Freunden das \u00e4ltere Ich als kleiner Scherz f\u00fcr Zwischendurch zu zeigen. Zwei Studien und eine Firma mit Anwendungen im Bereich der Medizin haben wir im Report vorgestellt. Weitere Applikationen z.B. bei der Hilfe der Bek\u00e4mpfung des Menschenhandels w\u00e4ren w\u00fcnschenswert.</p> <p>Die Forschung auf diesem Gebiet bleibt spannend. Neuartige Methoden im Bereich des maschinellen Lernens k\u00f6nnten die Generierung qualitativ hochwertiger Bilder f\u00fcr die Gesichtsalterung weiterhin verbessern. </p>"},{"location":"Themen/Face_Aging/#weiterfuhrendes-material","title":"Weiterf\u00fchrendes Material","text":""},{"location":"Themen/Face_Aging/#talk","title":"Talk","text":"<p>Hier einfach Youtube oder THD System embedden.</p>"},{"location":"Themen/Face_Aging/#demo","title":"Demo","text":"<p>Hier Link zum Demo Video.</p> <p>Hier der Link zum GitHub Repository der Code-Demonstration.</p>"},{"location":"Themen/Face_Aging/#literaturliste","title":"Literaturliste","text":""},{"location":"Themen/Face_Aging/#datensatze_1","title":"Datens\u00e4tze","text":"<ul> <li>K. Ricanek and T. Tesafaye, \"MORPH: a longitudinal image database of normal adult age-progression,\" 7th International Conference on Automatic Face and Gesture Recognition (FGR06), Southampton, UK, 2006, pp. 341-345, doi: 10.1109/FGR.2006.78.</li> <li>P. J. Phillips, Hyeonjoon Moon, S. A. Rizvi and P. J. Rauss, \"The FERET evaluation methodology for face-recognition algorithms,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no. 10, pp. 1090-1104, Oct. 2000, doi: 10.1109/34.879790.</li> <li>FG-NET dataset by Yanwei Fu</li> <li>Bor-Chun Chen, , Chu-Song Chen, and Winston H. Hsu. \"Cross-Age Reference Coding for Age-Invariant Face Recognition and Retrieval.\" . In Proceedings of the European Conference on Computer Vision (ECCV).2014.</li> <li>A. Lanitis, C. J. Taylor and T. F. Cootes, \"Toward automatic simulation of aging effects on face images,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 24, no. 4, pp. 442-455, April 2002, doi: 10.1109/34.993553.</li> <li>IMDB-WIKI \u2013 500k+ face images with age and gender labels</li> <li>Gary B. Huang, , Manu Ramesh, Tamara Berg, and Erik Learned-Miller. 2007. \"Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments.\" University of Massachusetts, Amherst, Invalid Date 2007.</li> </ul>"},{"location":"Themen/Face_Aging/#physikalische-modellbasierte-methoden_1","title":"Physikalische modellbasierte Methoden","text":"<ul> <li>J. Suo, S. -C. Zhu, S. Shan and X. Chen, \"A Compositional and Dynamic Model for Face Aging,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 3, pp. 385-401, March 2010, doi: 10.1109/TPAMI.2009.39.</li> <li>Tsai, MH., Liao, YK. &amp; Lin, IC. Human face aging with guided prediction and detail synthesis. Multimed Tools Appl 72, 801\u2013824 (2014). https://doi.org/10.1007/s11042-013-1399-7</li> <li>Todd, James T., Leonard S. Mark, Robert E. Shaw, and John B. Pittenger. \u201cThe Perception of Human Growth.\u201d Scientific American 242, no. 2 (1980): 132\u201345. http://www.jstor.org/stable/24966262.</li> <li>Suo, J., Chen, X., Shan, S., Gao, W., &amp; Dai, Q. (2012). A concatenational graph evolution aging model. IEEE transactions on pattern analysis and machine intelligence, 34(11), 2083\u20132096. https://doi.org/10.1109/TPAMI.2012.22</li> <li>Douglas DeCarlo, Dimitris Metaxas, and Matthew Stone. 1998. An anthropometric face model using variational techniques. Proceedings of the 25th annual conference on Computer graphics and interactive techniques. Association for Computing Machinery, New York, NY, USA, 67\u201374. https://doi.org/10.1145/280814.280823</li> <li>N. Ramanathan and R. Chellappa, \"Modeling Age Progression in Young Faces,\" 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), New York, NY, USA, 2006, pp. 387-394, doi: 10.1109/CVPR.2006.187.</li> <li>A. Lanitis, C. J. Taylor and T. F. Cootes, \"Toward automatic simulation of aging effects on face images,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 24, no. 4, pp. 442-455, April 2002, doi: 10.1109/34.993553.</li> </ul>"},{"location":"Themen/Face_Aging/#prototypbasierte-methoden_1","title":"Prototypbasierte Methoden","text":"<ul> <li>I. Kemelmacher-Shlizerman, S. Suwajanakorn and S. M. Seitz, \"Illumination-Aware Age Progression,\" 2014 IEEE Conference on Computer Vision and Pattern Recognition, Columbus, OH, USA, 2014, pp. 3334-3341, doi: 10.1109/CVPR.2014.426.</li> <li>B. Tiddeman, M. Burt and D. Perrett, \"Prototyping and transforming facial textures for perception research,\" in IEEE Computer Graphics and Applications, vol. 21, no. 5, pp. 42-50, July-Aug. 2001, doi: 10.1109/38.946630.</li> <li>Xiangbo Shu, , Jinhui Tang, Hanjiang Lai, Luoqi Liu, and Shuicheng Yan. \"Personalized Age Progression with Aging Dictionary.\" (2015).</li> <li>H. Yang, D. Huang, Y. Wang, H. Wang and Y. Tang, \"Face Aging Effect Simulation Using Hidden Factor Analysis Joint Sparse Representation,\" in IEEE Transactions on Image Processing, vol. 25, no. 6, pp. 2493-2507, June 2016, doi: 10.1109/TIP.2016.2547587.</li> </ul>"},{"location":"Themen/Face_Aging/#deep-generative-networks_1","title":"Deep Generative Networks","text":"<ul> <li>Zhizhong Huang, , Shouzhen Chen, Junping Zhang, and Hongming Shan. \"PFA-GAN: Progressive Face Aging With Generative Adversarial Network\".IEEE Transactions on Information Forensics and Security 16 (2021): 2031\u20132045.</li> <li>Fu Y, Guo G, Huang TS. Age synthesis and estimation via faces: a survey. IEEE Trans Pattern Anal Mach Intell. 2010 Nov;32(11):1955-76. doi: 10.1109/TPAMI.2010.36. PMID: 20847387.</li> <li>Ian J. Goodfellow, , Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. \"Generative Adversarial Networks.\" (2014).</li> <li>Si Liu, , Yao Sun, Defa Zhu, Renda Bao, Wei Wang, Xiangbo Shu, and Shuicheng Yan. \"Face Aging with Contextual Generative Adversarial Nets.\" (2018).</li> <li>Grigory Antipov, , Moez Baccouche, and Jean-Luc Dugelay. \"Face Aging With Conditional Generative Adversarial Networks.\" (2017). </li> <li>H. Fang, W. Deng, Y. Zhong and J. Hu, \"Triple-GAN: Progressive Face Aging with Triple Translation Loss,\" 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), Seattle, WA, USA, 2020, pp. 3500-3509, doi: 10.1109/CVPRW50498.2020.00410.</li> <li>X. Tang, Z. Wang, W. Luo and S. Gao, \"Face Aging with Identity-Preserved Conditional Generative Adversarial Networks,\" 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 2018, pp. 7939-7947, doi: 10.1109/CVPR.2018.00828.</li> <li>Hongyu Yang, , Di Huang, Yunhong Wang, and Anil K. Jain. \"Learning Face Age Progression: A Pyramid Architecture of GANs.\" (2019). </li> <li>Yuval Alaluf, , Or Patashnik, and Daniel Cohen-Or. \"Only a Matter of Style: Age Transformation Using a Style-Based Regression Model.\" (2021). </li> <li>Mehdi Mirza, , and Simon Osindero. \"Conditional Generative Adversarial Nets.\" (2014).</li> <li>Kaiming He, , Xiangyu Zhang, Shaoqing Ren, and Jian Sun. \"Deep Residual Learning for Image Recognition.\" (2015).</li> <li>Rothe, R., Timofte, R. &amp; Van Gool, L. Deep Expectation of Real and Apparent Age from a Single Image Without Facial Landmarks. Int J Comput Vis 126, 144\u2013157 (2018). https://doi.org/10.1007/s11263-016-0940-3</li> <li>Zhou Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli, \"Image quality assessment: from error visibility to structural similarity,\" in IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600-612, April 2004, doi: 10.1109/TIP.2003.819861.</li> <li>Z. Liu, P. Luo, X. Wang and X. Tang, \"Deep Learning Face Attributes in the Wild,\" 2015 IEEE International Conference on Computer Vision (ICCV), Santiago, Chile, 2015, pp. 3730-3738, doi: 10.1109/ICCV.2015.425.</li> </ul>"},{"location":"Themen/Face_Aging/#anwendungen_1","title":"Anwendungen","text":"<ul> <li>Brinker, T. J., Brieske, C. M., Esser, S., Klode, J., Mons, U., Batra, A., R\u00fcther, T., Seeger, W., Enk, A. H., von Kalle, C., Berking, C., Heppt, M. V., Gatzka, M. V., Bernardes-Souza, B., Schlenk, R. F., &amp; Schadendorf, D. (2018). A Face-Aging App for Smoking Cessation in a Waiting Room Setting: Pilot Study in an HIV Outpatient Clinic. Journal of medical Internet research, 20(8), e10976. https://doi.org/10.2196/10976</li> <li>Brinker, T. J., Faria, B. L., de Faria, O. M., Klode, J., Schadendorf, D., Utikal, J. S., Mons, U., Krieghoff-Henning, E., Lisboa, O. C., Oliveira, A. C. C., Lino, H. A., &amp; Bernardes-Souza, B. (2020). Effect of a Face-Aging Mobile App-Based Intervention on Skin Cancer Protection Behavior in Secondary Schools in Brazil: A Cluster-Randomized Clinical Trial. JAMA dermatology, 156(7), 737\u2013745. https://doi.org/10.1001/jamadermatol.2020.0511</li> <li>AprilAge - Face Aging and Body Visualization Software</li> </ul>"},{"location":"Themen/LLMs/","title":"Large Language Models","text":"<p>von Richard Kelnhofer, Nick Thomas und Daniel Stoffel</p>"},{"location":"Themen/LLMs/#abstract","title":"Abstract","text":"<p>Das Arbeiten ohne Hilfe von ChatGPT, GitHubCopilot, etc. ist inzwischen f\u00fcr viele Menschen nur schwer vorstellbar. Diese Sogenannten LLMs(Large Language Models) haben innerhalb k\u00fcrzester Zeit nach ihrer Ver\u00f6ffentlichung die Welt im Sturm erobert. Ihre F\u00e4higkeit, Syntax, Zusammenhang und Semantik zu verstehen, macht sie zu unglaublich performanten und unersch\u00f6pflichen Assistenten f\u00fcr jeden, der Zugriff auf das Internet hat. Somit wurde in den letzten Jahren eine neue Generation von Assistenzsystemen geboren, welche einen bedeutenden Wert f\u00fcr die Menschheit bieten. Dieser Bericht umfasst drei unterschiedliche Module, welche sich von oberfl\u00e4chlichem Verst\u00e4ndnis \u00fcber detaillierte Informationen bis zur Anwendung strecken.</p> <p>Der Podcast dieses Beitrags versucht Laien, aber auch Informatiker in den Bereich der LLMs einzuf\u00fchren. Zun\u00e4chst wird ein kleiner Teaser \u00fcber die Funktionsweise der unterliegenden Technologien gegeben. Anschlie\u00dfend wird \u00fcber die Anwendungsgebiete dieser LLMs diskutiert und es wird erl\u00e4utert, welche Modifikationen ChatGPT besonders machen. Am Ende des Podcasts kommen wir zum Resultat, dass LLMs auch Gefahren bergen und so mit verf\u00e4lschten Trainingsdaten (absichtlich oder versehentlich) zur Desinformation der Bev\u00f6lkerung f\u00fchren k\u00f6nnte. Die Kontrolle und Verantwortung liegt hier bei den Unternehmen, die diese Modelle erschaffen.</p> <p>Der Fachvortrag geht \u00e4u\u00dferst detailliert auf alle Aspekte der Transformer-Modelle ein, wie z.B. Tokenisierung, Attention-Mechanismen und die generelle Architektur. Am Ende werden des weiteren Key-Features von ChatGPT, wie das Reward-Model ausf\u00fchrlich erl\u00e4utert.</p> <p>In der Code-Demo wird eine Library namens \"LangChain\" vorgestellt, welche es einfach macht, Das Wissen von LLMs zu erweitern(oder aktualisieren), mit LLMs komplexe Aufgaben zu l\u00f6sen, und diese evtl. sogar in Reihe zu schalten. \"LangChain\" ist in den letzten Jahren zu der bekanntesten Python-Library f\u00fcr das Arbeiten mit LLMs geworden. In der Demo wird die Anwendung einiger Funktionen anhand des Bespiels der Generierung eines Podcast-Scripts gezeigt und hier verwendet um eigene PDFs in die Wissensbasis des Vortrainierten GPT3.5 einf\u00fchren zu lassen.</p>"},{"location":"Themen/LLMs/#1-einleitung-motivation","title":"1 Einleitung / Motivation","text":"<p>Large Language Models(LLMs) sind eine der gr\u00f6\u00dften Neuerungen im Bereich der K\u00fcnstlichen Intelligenz und haben seit ihrer Erscheinung bereits viele Menschen in Angst um ihre Arbeitspl\u00e4tze versetzt. \u00a0Startschuss f\u00fcr diese Flut an LLMs waren die sogenannten \"Transformer\" Modelle, welche erstmals 2017 von Forschern von Google pr\u00e4sentiert wurden und RNNs(Recurrent Neural Networks) in NLP(Natural Language Processing)-Tasks den Kampf ansagten. Im Gegensatz zu RNNs, welche schwer trainierbar auf gro\u00dfe Datenmengen waren, konnten die Transformer problemlos Unmengen an Daten verarbeiten. Sie verf\u00fcgen ebenso \u00fcber sogenannte \"Attention\" und \"Self-Attention\", welche sie \u00e4u\u00dferst passend f\u00fcr sequenzielle Probleme macht.</p> <p>Mit diesen Eigenschaften in ihrer Architektur k\u00f6nnen Transformer, Wortposition, Wortbedeutung und Satz-Syntax selbst ganz ohne Handbenannte Daten lernen. Dies macht sie trainierbar auf exorbitanten Datenmengen, was ihre F\u00e4higkeiten und Bandbreite ihres Wissens im Vergleich zu herk\u00f6mmlichen Methoden enorm steigert.</p> <p>Fast alle modernen LLMs basieren auf genau dieser Transformer-Technologie, erweitern sie aber wie z.B. bei ChatGPT mit handbenannte Daten und sogenannten \"Reward-Models\", welche dem Transformer Feedback geben k\u00f6nnen. ChatGPT ist zwar das bekannteste, aber bei weitem nicht das Einzige LLM. Es gibt zahlreiche kreative Anwendungsm\u00f6glichkeiten, welche auf Transformer bauen, die wir hier kurz pr\u00e4sentieren m\u00f6chten.</p> <p>AutoGPT zum Beispiel ist ein Spezielles Modell, welches versucht eine Aufgabe so gut wie m\u00f6glich zu absolvieren. Das Besondere ist, dass es dabei Google benutzen kann, aber auch Ordner auf dem Computer erstellen kann.</p> <p>MiniGPT ist ein Open-Source ChatBot, welcher \"sehen\" kann und somit auch mit Bildern arbeiten kann.</p> <p>Zu guter Letzt gibt es perplexity.ai, eine kostenlose KI-Suchmaschine, welche bei ihren Antworten auch die Quellen der Informationen angibt.</p> <p>Dies sind nur ein Paar Beispiele, f\u00fcr welche Anwendungen LLMs bereits verwendet werden. Ihre Anzahl wird in den kommenden Jahren wohlm\u00f6glich explodieren.</p>"},{"location":"Themen/LLMs/#2-methoden","title":"2 Methoden","text":"<p>In diesem Abschnitt werden verschiedene wichtige Konzepte und Techniken im Bereich des Natural Language Processing (NLP) behandelt. Es wird auf die grundlegenden Bausteinen des Transformer-Modells eingegangen, das eine Revolution in der NLP-Forschung und -Anwendung darstellt. Der Transformer ist die Grundlage f\u00fcr viele fortschrittliche Modelle, einschlie\u00dflich BERT (Bidirectional Encoder Representations from Transformers) und GPT (Generative Pre-Training). Neben der Beschreibung der Architektur und Funktionsweise von Transformer, BERT und GPT werden auch wichtige Aspekte wie Tokenisierung, Embeddings, Positional Encoding und Fine-Tuning behandelt. Des Weiteren werden Konzepte wie Meta-Learning und Benchmarking erl\u00e4utert, die zur Weiterentwicklung und Evaluierung von NLP-Motdellen beiragen. Somit wird ein umfassender \u00dcberblick \u00fcber die aktuellen Schl\u00fcsselkonzepten und Techniken im Bereich des NLP gegeben.</p>"},{"location":"Themen/LLMs/#21-daten","title":"2.1 Daten","text":"<p>Daten bilden das Herzst\u00fcck der modernen Sprachverarbeitung und intensive Forschung wird in diesem Bereich betrieben, um immer fortschrittlichere Vektorrepr\u00e4sentationen f\u00fcr W\u00f6rter zu entwickeln. Diese repr\u00e4sentativen Vektoren werden auch als Embeddings bezeichnet und bilden die Grundlage f\u00fcr die meisten Modelle im Bereich der nat\u00fcrlichen Sprachverarbeitung (NLP). Dieser Abschnitt widmet sich den zentralen Konzepten und Techniken zur Erstellung von Embeddings \u2013 angefangen bei der Tokenisierung, die den Text in einzelne Token zerlegt, bis hin zu den Embeddings selbst, welche die W\u00f6rter in einem Vektorraum abbilden.</p>"},{"location":"Themen/LLMs/#211-tokenisierung","title":"2.1.1 Tokenisierung","text":"<p>In der Literatur gibt es sehr viele Tokenisierungstechniken. Hier werden nur ein paar davon n\u00e4her betrachtet. Den Anfang machen die regelbasierten Tokenizer, deren Regeln von Menschen erstellt werden. Typische Tokenisierungsregeln sind:</p> <ul> <li> <p>Trennung an Satzzeichen</p> </li> <li> <p>Trennung an Leerzeichen</p> </li> <li> <p>Trennung an Sonderzeichen</p> </li> </ul> <p>Ein Beispiel f\u00fcr einen regelbasierten Tokenizer ist der Penn Treebank Tokenizer. Dieser Tokenizer ist in der Lage, die Tokens eines Textes zu extrahieren. Dabei werden die Tokens an Leerzeichen, Satzzeichen und Sonderzeichen getrennt. Im Kern verwendet er regul\u00e4re Ausdr\u00fccke, um die Tokens zu extrahieren. Weitergehend kann man auch auf Klein- und Gro\u00dfschreibung achten und diese Regeln in den Tokenizer einbauen. Zus\u00e4tzlich kann man auch Lemmatizer und Stemmer verwenden, um die Tokens zu normalisieren.</p> <pre><code>from nltk.tokenize import TreebankWordTokenizer\n\ntokenizer = TreebankWordTokenizer()\ntext = \"I can't wait to visit the U.S. next year.\"\ntokens = tokenizer.tokenize(text)\nprint(tokens)\n</code></pre> <pre><code>['I', 'ca', \"n't\", 'wait', 'to', 'visit', 'the', 'U.S.', 'next', 'year', '.']\n</code></pre> <p>Das Ergebnis eines Tokenizers ist ein Vokabular, aus dem durch Embeddingalgorithmen Vektoren f\u00fcr Sprachmodelle entstehen. Dieses Vokabular ist das Fundament auf dem die Embeddings aufbauen. Es ist wichtig, dass das Vokabular m\u00f6glichst gro\u00df ist, um eine gute Repr\u00e4sentation der Sprache zu erreichen. Ein gro\u00dfes Vokabular hat aber auch Nachteile. Es ist sehr rechenintensiv und ben\u00f6tigt viel Speicher. Deshalb wird das Vokabular meistens auf die 30.000 h\u00e4ufigsten W\u00f6rter beschr\u00e4nkt. Jedoch gibt es auch Modelle mit weitaus gr\u00f6\u00dferem Vokabular. Zus\u00e4tzlich muss das Vokabular nicht nur aus reinen W\u00f6rter bestehen. Was Regeln nicht erreichen, ist eine sinnvolle Trennung des Wortes <code>U.S.</code>. Regeln verstehen die Semantik hinter Abk\u00fcrzungen nicht. Deshalb gen\u00fcgen reine regelbasierte Tokenizer nicht mehr.</p> <p>Ein weiteres Vorgehen, um ein sinnvolles Vokabular zu erzeugen ist Byte-Pair-Encoding. Bevor man BPE verwenden, muss man seine Texte normalisieren. Auf diesen normalisierten Text wird ein Pre-Tokenizer angewendet, um Tokens zu generiern. Danach wird das Vokabular erstellt. Dieses Vokabular besteht aus den einzelnen Zeichen des Textes. Anschlie\u00dfend wird das Vokabular durchlaufen und die h\u00e4ufigsten Zeichenpaare gesucht. Diese werden dann zu einem neuen Zeichen zusammengefasst. Dieser Vorgang wird so oft wiederholt, bis das Vokabular die gew\u00fcnschte Gr\u00f6\u00dfe erreicht hat. Das Ergebnis ist ein Vokabular, das aus Zeichen und Zeichenpaaren besteht. Dieses Vokabular wird dann verwendet, um die Tokens zu generieren.</p> <pre><code>from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n\n# Initialize the BPE tokenizer\ntokenizer = Tokenizer(models.BPE())\n\n# Create and configure the trainer\ntrainer = trainers.BpeTrainer(vocab_size=10000, special_tokens=[\"&lt;s&gt;\", \"&lt;pad&gt;\", \"&lt;/s&gt;\", \"&lt;unk&gt;\"])\n\n# Train the tokenizer on your text data\ntokenizer.train([\"path/to/your/text/file.txt\"], trainer)\n\n# Encode a sentence using the trained tokenizer\nsentence = \"Hello, how are you?\"\nencoded_sentence = tokenizer.encode(sentence)\n\n# Get the tokenized representation\ntokens = encoded_sentence.tokens\nprint(tokens)\n</code></pre>"},{"location":"Themen/LLMs/#212-subword-tokenization","title":"2.1.2 Subword Tokenization","text":"<p>Hier werden WordPiece, Unigram und SentencePiece vorgestellt. Diese Tokenizer sind sehr \u00e4hnlich zu BPE. Dennoch erstellen alle 4 Verfahren andere Vokabelsets. WordPiece ist ein Tokenisierungsalgorithmus, der urspr\u00fcnglich von Google f\u00fcr das NLP-Modell BERT (Bidirectional Encoder Representations from Transformers) entwickelt wurde. Es ist ein statistisches Verfahren, das auf der Idee basiert, h\u00e4ufige Zeichenfolgen von Zeichen in einem Textkorpus zu identifizieren und sie zu einem neuen Wort zusammenzufassen. WordPiece \u00fcberpr\u00fcft, ob es sinvoll ist, k Zeichen zu einem neuen Zeichen zusammenzufassen. Dieser Vorgang wird so oft wiederholt, bis das Vokabular die gew\u00fcnschte Gr\u00f6\u00dfe erreicht hat.</p> \\[ max_{w \\in V} \\frac{count(\\sum\\limits_1^k w_k)}{\\prod\\limits_1^k count(w_k)} \\] <p>Das Unigram Vorgehen ist aus einem bestehenden Vokabular, unn\u00f6tiges zu entfernen. Um das \"Unn\u00f6tige\" zu bestimmen, berechnet Unigram den gesamten loss \u00fcber das Vokabular mit dem Log-Likelihood. Das Vokabular wird dann so lange verkleinert, bis die gew\u00fcnschte Gr\u00f6\u00dfe erreicht ist.</p> \\[  Loss = -\\sum\\limits_{w \\in V} log(P(w)) \\] <p>Zu Letzt wird noch SentencePiece vorgestellt. Dieser Tokenizer ist ein weiteres Verfahren, um Subwords zu generieren. Es ist ein statistisches Verfahren, das auf der Idee basiert, h\u00e4ufige Zeichenfolgen in einem Textkorpus zu identifizieren und sie zu einem neuen Wort zusammenzufassen. Dabei basiert SentencePiece auf BPE, WordPiece, Unigram und vielen anderen.</p> <pre><code>from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n\n# Initialize the WordPiece tokenizer\ntokenizer = Tokenizer(models.WordPiece())\n\n# Initialize the Unigram tokenizer\ntokenizer = Tokenizer(models.Unigram())\n</code></pre> <pre><code>import sentencepiece as spm\n\n# Train SentencePiece tokenizer\nspm.SentencePieceTrainer.train(input='input.txt', model_prefix='spm_model', vocab_size=1000)\n\n# Load trained SentencePiece tokenizer\ntokenizer = spm.SentencePieceProcessor()\ntokenizer.load('spm_model.model')\n</code></pre>"},{"location":"Themen/LLMs/#213-embeddings","title":"2.1.3 Embeddings","text":"<p>In diesem Abschnitt werden die Idee, die Speicherung und verschiedene Algorithmen und Modelle vorgestellt, durch die Embeddings erzeugt werden k\u00f6nnen. Embeddings sind der weitere Schritt in der Datenverarbeitung nach dem Erstellen des Vokabulars durch Tokenizer. Das Ziel ist es die W\u00f6rter in eine Computer verst\u00e4ndliche Form zu bringen. Dies wird erreicht, indem die W\u00f6rter in einen Vektor umgewandelt werden. Dieser Vektor zeigt die Beziehungen zwischen einzelnen W\u00f6rtern an.</p> <p>Definition<p>Die grundlegende Idee hinter Embeddings besteht darin, dass \u00e4hnliche W\u00f6rter \u00e4hnliche Vektoren besitzen, was dazu f\u00fchrt, dass sie im n-dimensionalen Raum r\u00e4umlich nah beieinander liegen.</p> </p> <p>\u201cAn embedding is a way of representing data as points in n-dimensional space so that similar data points cluster together.\u201d</p> <p> </p> Fig. Basic Function <pre><code>import numpy as np\nfrom gensim.models import Word2Vec\nfrom tabulate import tabulate\n\n# Define the word list\nwords = [\"I\", \"love\", \"coding\", \"Python\", \"is\", \"great\", \"Machine\", \"learning\", \"fascinating\"]\n\n# Create a list of sentences\nsentences = [[\"I\", \"love\", \"coding\"],\n             [\"Python\", \"is\", \"great\"],\n             [\"Machine\", \"learning\", \"is\", \"fascinating\"]]\n\n# Train the Word2Vec model\nmodel = Word2Vec(sentences, min_count=1)\n\n# Create a word correlation matrix\ncorrelation_matrix = np.zeros((len(words), len(words)))\n\n# Populate the correlation matrix\nfor i, word1 in enumerate(words):\n    for j, word2 in enumerate(words):\n        correlation_matrix[i, j] = model.wv.similarity(word1, word2)\n\n# Create a table with row and column headers\ntable = [[word] + list(row) for word, row in zip(words, correlation_matrix)]\n\n# Add column headers\ntable.insert(0, [''] + words)\n\n# Print the table\nprint(tabulate(table, headers=\"firstrow\", tablefmt=\"grid\"))\n</code></pre> <pre><code>+-------------+-------------+-------------+-------------+-------------+------------+-------------+------------+-------------+---------------+\n|             |           I |        love |      coding |      Python |         is |       great |    Machine |    learning |   fascinating |\n+=============+=============+=============+=============+=============+============+=============+============+=============+===============+\n| I           |  1          | -0.144546   |  0.00882618 |  0.0605919  |  0.0270575 | -0.0690033  |  0.0749756 |  0.199121   |    0.0336406  |\n+-------------+-------------+-------------+-------------+-------------+------------+-------------+------------+-------------+---------------+\n| love        | -0.144546   |  1          |  0.0048425  | -0.0577458  |  0.0929172 | -0.028491   | -0.0136798 | -0.00275401 |   -0.115555   |\n+-------------+-------------+-------------+-------------+-------------+------------+-------------+------------+-------------+---------------+\n| coding      |  0.00882618 |  0.0048425  |  1          |  0.0191523  |  0.0161347 |  0.0347649  |  0.0415774 |  0.145951   |   -0.114107   |\n+-------------+-------------+-------------+-------------+-------------+------------+-------------+------------+-------------+---------------+\n| Python      |  0.0605919  | -0.0577458  |  0.0191523  |  1          | -0.0598763 |  0.13888    |  0.13149   |  0.0640898  |    0.00939116 |\n+-------------+-------------+-------------+-------------+-------------+------------+-------------+------------+-------------+---------------+\n| is          |  0.0270575  |  0.0929172  |  0.0161347  | -0.0598763  |  1         | -0.0277504  | -0.111671  | -0.0523467  |   -0.0108392  |\n+-------------+-------------+-------------+-------------+-------------+------------+-------------+------------+-------------+---------------+\n| great       | -0.0690033  | -0.028491   |  0.0347649  |  0.13888    | -0.0277504 |  1          | -0.0446171 |  0.170189   |    0.00450302 |\n+-------------+-------------+-------------+-------------+-------------+------------+-------------+------------+-------------+---------------+\n| Machine     |  0.0749756  | -0.0136798  |  0.0415774  |  0.13149    | -0.111671  | -0.0446171  |  1         | -0.0135149  |    0.067976   |\n+-------------+-------------+-------------+-------------+-------------+------------+-------------+------------+-------------+---------------+\n| learning    |  0.199121   | -0.00275401 |  0.145951   |  0.0640898  | -0.0523467 |  0.170189   | -0.0135149 |  1          |   -0.0236717  |\n+-------------+-------------+-------------+-------------+-------------+------------+-------------+------------+-------------+---------------+\n| fascinating |  0.0336406  | -0.115555   | -0.114107   |  0.00939116 | -0.0108392 |  0.00450302 |  0.067976  | -0.0236717  |    1          |\n+-------------+-------------+-------------+-------------+-------------+------------+-------------+------------+-------------+---------------+\n</code></pre> <pre><code># Print the model\nprint(model)\n\nOutput:\nWord2Vec&lt;vocab=9, vector_size=100, alpha=0.025&gt;\n\n\n# Print the word vector for the word \"I\"\nprint(model.wv[\"I\"])\n\nOutput:\n[-9.5785465e-03  8.9431154e-03  4.1650687e-03  9.2347348e-03\n  6.6435025e-03  2.9247368e-03  9.8040197e-03 -4.4246409e-03\n -6.8033109e-03  4.2273807e-03  3.7290000e-03 -5.6646108e-03\n  9.7047603e-03 -3.5583067e-03  9.5494064e-03  8.3472609e-04\n -6.3384566e-03 -1.9771170e-03 -7.3770545e-03 -2.9795230e-03\n  1.0416972e-03  9.4826873e-03  9.3558477e-03 -6.5958775e-03\n  3.4751510e-03  2.2755705e-03 -2.4893521e-03 -9.2291720e-03\n  1.0271263e-03 -8.1657059e-03  6.3201892e-03 -5.8000805e-03\n  5.5354391e-03  9.8337233e-03 -1.6000033e-04  4.5284927e-03\n -1.8094003e-03  7.3607611e-03  3.9400971e-03 -9.0103243e-03\n -2.3985039e-03  3.6287690e-03 -9.9568366e-05 -1.2012708e-03\n -1.0554385e-03 -1.6716016e-03  6.0495257e-04  4.1650953e-03\n -4.2527914e-03 -3.8336217e-03 -5.2816868e-05  2.6935578e-04\n -1.6880632e-04 -4.7855065e-03  4.3134023e-03 -2.1719194e-03\n  2.1035396e-03  6.6652300e-04  5.9696771e-03 -6.8423809e-03\n -6.8157101e-03 -4.4762576e-03  9.4358288e-03 -1.5918827e-03\n -9.4292425e-03 -5.4504158e-04 -4.4489228e-03  6.0000787e-03\n -9.5836855e-03  2.8590010e-03 -9.2528323e-03  1.2498009e-03\n  5.9991982e-03  7.3973476e-03 -7.6214634e-03 -6.0530235e-03\n -6.8384409e-03 -7.9183402e-03 -9.4990805e-03 -2.1254970e-03\n -8.3593250e-04 -7.2562015e-03  6.7870365e-03  1.1196196e-03\n  5.8288667e-03  1.4728665e-03  7.8936579e-04 -7.3681297e-03\n -2.1766580e-03  4.3210792e-03 -5.0853146e-03  1.1307895e-03\n  2.8833640e-03 -1.5363609e-03  9.9322954e-03  8.3496347e-03\n  2.4156666e-03  7.1182456e-03  5.8914376e-03 -5.5806171e-03]\n</code></pre> <p>Im einfachsten Fall, liegt ein Wert pro Spalte zwischen 0 und 1. Dieser Wert gibt an, wie stark das Wort mit dem jeweiligen Merkmal korreliert. Wenn der Wert 0 ist, ist die Korrelation zwischen dem Wort und dem Merkmal nicht vorhanden. Wenn der Wert 1 ist, ist die Korrelation zwischen dem Wort und dem Merkmal sehr stark. In der Praxis werden die Werte jedoch nicht immer zwischen 0 und 1 liegen. Die Werte k\u00f6nnen auch negativ sein oder gr\u00f6\u00dfer als 1 werden.</p> <p>Um bei einen Embedding herauszufinden, wie \u00e4hnlich sich zwei Wortvektoren sind, wird der Cosinus-Abstand, euklidische Abstand oder das Skalarprodukt verwendet. Die Ergebnis des Skalarprodukts ist ein skalarer Wert. Wenn das Skalarprodukt von a \u00b7 b einen Wert nahe der Null hat, sind die Vektoren senkrecht zueinander und haben somit keine Korrelation. Wenn das Skalarprodukt positiv ist, sind die Vektoren \u00e4hnlich oder parallel, w\u00e4hrend ein negatives Skalarprodukt auf eine entgegengesetzte oder divergierende Ausrichtung der Vektoren hinweist.</p> \\[ a \\cdot b = a_1 * b_1 + ... + a_n * b_n \\] <p>Die Aussagekraft des euklidischen Abstandes ist besser als die des Skalarproduktes. Als Ergebnis kommt ein skalarer Wertraus, der die Distanz zwischen den Vektoren angibt. Je kleiner der Wert, desto \u00e4hnlicher sind die Vektoren. Jedoch wird der Wert durch die Komponenten der Vektor stark beeinflusst. An sich wird der Differenzenvektor von a und b berechnet, an dem die euklidische Norm verwendet wird. Die euklidische Norm ist die L\u00e4nge eines Vektors.</p> <p>Den Abstand zweier Vektoren darf man f\u00fcr das \u00c4hnlichkeitsma\u00df nutzen, da folgende Gleichungen erf\u00fcllt sind:</p> <ul> <li> <p>Kommutativgesetz: $ ||x, y|| = ||y, x|| $</p> </li> <li> <p>Hilbert Norm: $ ||x|| = \\sqrt{x \\cdot x} $</p> </li> <li> <p>Cauchy-Schwarz-Ungleichung: $ |x \\cdot y| \\leq ||x|| * ||y|| $</p> </li> <li> <p>Homogenit\u00e4t: $ ||\\alpha * x|| = |\\alpha| * ||x|| $</p> </li> <li> <p>Nichtnegativit\u00e4t: $ ||x|| \\geq 0 $</p> </li> </ul> \\[ d(a,b) = ||a - b||\\_2 = \\sqrt{\\prod\\limits_k^n (a_k - b_k)^2} \\] <p>Das wichtigste Ma\u00df f\u00fcr die Entscheidung \u00fcber \u00c4hnlichkeit ist die Cosinus-\u00c4hnlichkeit. Sie ist definiert als der Kosinus des Winkels zwischen den beiden Vektoren, somit liegen die Werte zwischen -1 und 1, wobei 1 f\u00fcr perfekte \u00c4hnlichkeit steht.</p> \\[ cos(\\theta) = \\frac{a \\cdot b}{||a||_2 * ||b||_2} \\] <p>Die Berechnung der \u00c4hnlichkeit, hilft nicht nur beim trainieren von Sprachmodellen, sonderen ist auch ein effizienter Weg Vektoren zu speichern. F\u00fcr die Speicherung werden Vektor Datenbanken verwendet. Hierbei unterscheident man in eine reine Indeximplementierung wie FAISS oder ein Datenbankmanagement System wie Pinecone. Je nach Anwendungszweck muss entschieden werden, ob Geschwiningkeit oder Sicherheit wichtiger ist. Ein Vektor-Datenbankmanagesystem liefert die g\u00e4ngigen Werkzeuge, um die Speicherung von Vektoren zu verwalten. Dazu geh\u00f6ren die folgenden Funktionen:</p> <ul> <li> <p>Data Management: Speichern, Abrufen, L\u00f6schen und Aktualisieren von Vektoren</p> </li> <li> <p>Indexing: Erstellen von Indizes f\u00fcr die Suche</p> </li> <li> <p>Metadata Management: Speichern von Metadaten zu Vektoren</p> </li> <li> <p>Scalability &amp; Integration: Skalierung und Integration in bestehende Systeme</p> </li> <li> <p>Real-Time updates: Echtzeit-Updates von Vektoren</p> </li> <li> <p>Backups &amp; Collection: Backups und Sammlungen von Vektoren(Window-Functions)</p> </li> <li> <p>Security &amp; access control: Sicherheit und Zugriffskontrolle</p> </li> </ul> <p>Ein Index hat vieles davon nicht. Dementsprechend muss man sicher sehr sicher sein, falls man die Indeximplementierung w\u00e4hlt. Andernfalls werden einzelne Funktionen selbst implementiert, was zu einem Mehraufwand f\u00fchrt.</p> <p> </p> Fig. Vector Database <p>Vektoren werden durch Algorithmen wie Product Quantinization, Locality Sensitive Hashing oder Hierarchical Navigable Small World in einen Index umgewandelt. Dieser Index landet mit dem dazugeh\u00f6rigen Originalvektor in der Vektor Datenbank. Beim Querying wird der Query-Vektor in den Index umgewandelt und mit den Indexvektoren verglichen. Die Ergebnisse werden dann zur\u00fcckgegeben. Schlie\u00dflich kommt es zum Postprocessing, bei dem die Ergebnisse gefiltert und / oder geordnet werden. Somit kommt es zu einer Neubewertung aller Ergebnisse und das beste Ergebnis wird zur\u00fcckgegeben.</p> <p>Product Quantization ist eine Technik, die in der Vektorquantisierung verwendet wird, um hochdimensionale Vektoren durch eine kleinere Menge von Prototypvektoren zu repr\u00e4sentieren und somit zu komprimieren. Das Ziel von Product Quantization besteht darin, den Speicherbedarf hochdimensionaler Vektoren zu reduzieren, w\u00e4hrend ihre wesentlichen Merkmale erhalten bleiben. Somit wird der originale Vektor in mehrere Subvektoren aufgeteilt, die einzeln quantisiert werden. Die Quantisierung wird durch centroids durchgef\u00fchrt. Das sind Vektoren (reproduction values), die sich in einem Subraum befinden, wo die Komponenten der Subvektoren ihre n\u00e4chsten Nachbarn suchen, indem diese k-nearest-neighbours (kNN) verwenden. Folglich wird auf das naheste reproduction value gemappt, um den Vektor zu repr\u00e4sentieren.</p> <p> </p> Fig. Product Quantinization <p>Jetzt wo das sicher der Vektoren klargestellt ist, kann man anfangen Embeddings zu berechnen. Der bekannteste Ansatz ist Word2Vec. Hierzu gibt es zwei Architekturen: Continuous Bag of Words und Skip-Gram. Beide Architekturen sind neural network language models. Das Ziel ist es, die Wahrscheinlichkeit eines Wortes zu berechnen, das auf ein anderes Wort folgt. Die Architektur des CBOW-Modells und des Skip-Gram-Modells ist in der folgenden Abbildung dargestellt.</p> <p> </p> Fig. Word2Vec <p>Das Ziel des CBOW ist es, ein Wort innerhalb eines Kontextes vorherzusagen, w\u00e4hrend das Ziel des Skip-Grams die Vorhersage desKontextes um ein Wort ist. Beide verwenden ein hidden layer ohne Aktivierungsfunktion. Folglich findet eine Projektion statt, indem das Skalarprodukt von Eingabevektor und Gewichtsmatrix gebildet wird. Schlie\u00dflich wird in der Ausgabe Schicht ein hierachischer Softmax verwendet. Er nutzt einen bin\u00e4ren Baum, um die Wahrscheinlichkeit eines Wortes zu berechnen. Anstatt alle W\u00f6rter im Vokabular zu ber\u00fccksichtigen, wird der hierarchische Softmax die Wahrscheinlichkeit schrittweise berechnen, indem er den Baum durchl\u00e4uft. Dieser Baum organisiert die W\u00f6rter hierarchisch, wobei h\u00e4ufigere W\u00f6rter n\u00e4her an der Wurzel und seltenere W\u00f6rter weiter unten im Baum platziert werden. Indem der hierarchische Softmax den bin\u00e4ren Baum verwendet, kann er die Anzahl der Berechnungen reduzieren, da er nur einen Teil des Baums durchlaufen muss, um die Wahrscheinlichkeit eines bestimmten Wortes zu bestimmen. Dies f\u00fchrt zu einer beschleunigten Vorhersagephase und erm\u00f6glicht die effiziente Verarbeitung gro\u00dfer Vokabulare. Zum Vallidieren der Ausgaben wird One-Hot-Encoding verwendet.</p> <p> </p> Fig. hierachischer Softmax <p>Zum Trainieren wird sub sampling als auch negative sampling verwendet. Sub sampling \u00fcberpr\u00fcft von vorne weg, ob ein Wort in das Context Fenster aufgenommen wird oder nicht. Ein Context Fenster entsteht, um ein betrachtetes Wort und seine direkten Nachbarn. Die Wahrscheinlichkeit, dass ein Wort in das Context Fenster aufgenommen wird, ist umgekehrt proportional zu seiner H\u00e4ufigkeit.</p> \\[ P(w_i) = (\\sqrt{\\frac{z(w_i)}{0.001}} + 1) * \\frac{0.001}{z(w_i)} \\] <p>Negative sampling ist eine Technik, die die Trainingszeit verk\u00fcrzt, indem sie nur eine kleine Anzahl von negativen Beispielen und das positive Beispiel ausw\u00e4hlt, um die Gewichte zu aktualisieren. Bei einer gro\u00dfen Anzahl von Perzeptronen in der Eingabeschicht kann das Training lange dauern, wenn man alle Gewichte anpasssen muss, obwohl man nur pro Wort anpasst. Die Anzahl der negativen Beispiele ist ein Hyperparameter, der die Genauigkeit und die Trainingszeit beeinflusst. Die Formel f\u00fcr die Berechnung der Wahrscheinlichkeit eines negativen Beispiels ist wie folgt:</p> \\[ P(w*i) = \\frac{z(w_i)^{3/4}}{\\sum_{j=0}^{n} z(w_j)^{3/4}} \\] <p> </p> Fig. negative sampling <p>Die Erweiterung von Word2Vec ist Fasttext, das von Facebook 2017 ver\u00f6ffentlicht wurde. Es ber\u00fccktsichtigt die Morphologie von W\u00f6rtern. Im Gegensatz zu Word2Vec, das W\u00f6rter als diskrete Einheiten betrachtet, betrachtet Fasttext W\u00f6rter als eine Reihe von Zeichen oder n-Grammen. Somit wird es robuster gegen unbekannte W\u00f6rter. Das Resultat ist eine \"Tasche\" von verschiedenen n-Grammen zu einem Wort. Dies hat zur Folge, dass eine geteilte Repr\u00e4sentation \u00fcber W\u00f6rtern entsteht und somit die Repr\u00e4sentation von seltenen W\u00f6rtern verbessert wird. Die Vorhersage, basiert auf eine Score-Funktion, die die Summe der Vektoren der n-Gramme des Wortes ist. Die Score-Funktion ist wie folgt definiert:</p> \\[ s(w, c) = \\sum\\limits_{g \\in G_w} z_g^T v_c \\] <ul> <li> <p>G ist die Menge der n-Gramme zu einem Wort w</p> </li> <li> <p>z ist der Vektor eines n-Gramms</p> </li> <li> <p>v ist der Vektor des zugeh\u00f6rigen Kontexts</p> </li> </ul> <p>Es k\u00f6nnen auch Language Models f\u00fcr Embeddings verwendet werden. Ein Beispiel ist GloVe. Es verwendet eine Matrix, um die Beziehung zwischen W\u00f6rtern zu erfassen. Hierbei wird betrachtet wie oft W\u00f6rter miteinander in einem Kontext auftauchen. Mit der SVG (Singularwertzelegung) wird die urspr\u00fcngliche Matrix in kleiner Matrizen geteilt. Dabei werden die Wortembeddings erzeugt. Dennoch werden Embeddings auch mit Transformer Modellen wie Bert und GPT erstellt. Diese werden in den n\u00e4chsten Kapitel genauer betrachtet. Doch ein weiteres Modell ist interessant: ELMo(Embeddings from Language Models). ELMo basiert auf einem didirektionalen LSTM. Das hat den Vorteil, dass Sequenzen von beiden Richtungen durchgegangen werden k\u00f6nnen.</p> \\[ ELMo_k^{task} = E(R_k; \\theta^{task}) = \\gamma^{task}\\sum\\limits_{j=0}^L s_j^{task} h_{k, j}^{LM} \\] <ul> <li> <p>\\(\\gamma^{task}\\) ist ein Skalarparameter, der f\u00fcr jede Aufgabe trainiert wird und den ELMo Vektor skaliert</p> </li> <li> <p>\\(s_j^{task}\\) sind softmax-normalisierte Gewichte, die f\u00fcr jede Aufgabe trainiert werden</p> </li> <li> <p>\\(h_{k, j}^{LM}\\) ist der j-te hidden LSTM Zustand f\u00fcr das k-te Token</p> </li> <li> <p>\\(R_k\\) ist die Repr\u00e4sentation des k-ten Tokens</p> </li> <li> <p>\\(\\theta^{task}\\) sind die Parameter der Aufgabe</p> </li> <li> <p>\\(ELMo_k^{task}\\) ein einziger Vektor aus der Reduzierung aller Schichten in R</p> </li> </ul> <p>LSTMs k\u00f6nnen durch ihre Zellen und rekusiver Natur f\u00fcr Sequenzverarbeitung verwendet werden. Die Zellen sind in der Lage, Informationen \u00fcber einen l\u00e4ngeren Zeitraum zu speichern und diese zu filtern. Die rekursive Natur eines LSTM kommt daher, dass es eine Verbesserung des RNNs ist. Das wichtigste f\u00fcr die Embeddinggenerierung ist es, dass die resultierenden Embeddings kontextabh\u00e4ngig sind. Die LSTM-Zellen liefern den Kontext, indem sie die vorherigen Ausgaben ber\u00fccksichtigen. Ein weiterer Vorteil einer LSTM-Architektur ist es, dass pro Zelle \u00fcber den Hidden State eine Vektorrepr\u00e4sentation entsteht. Diese Vektorrepr\u00e4sentationen werden f\u00fcr die Embeddinggenerierung verwendet. Somit glit f\u00fcr die linke Seite: \\(\\vec{h}_{k, j}^{LM}\\) f\u00fcr \\(t_{k+1}\\) und f\u00fcr die rechte Seite gilt: \\(\\overleftarrow{h}_{k, j}^{LM}\\) f\u00fcr \\(t_{k}\\).</p> <p>Um diese Vektorrepr\u00e4sentationen zu erzeugen, muss ELMo trainiert werden. Dies geschieht, indem die Wahrscheinlichkeit f\u00fcr das Vorkommen eines Wortes innerhalb einer Sequenz maximiert wird und die aufkommenden Fehler minimiert werden. Die Wahrscheinlichkeit wird wie folgt berechnet:</p> \\[  max(     \\sum\\limits_{k=1}^{K}         log P(t_k | t_1, ..., t_{k-1}; \\theta_x, \\vec\\theta_{LSTM}, \\theta_s) +         log P(t_t | t_{k+1}, ..., t_K; \\theta_x, \\overleftarrow{\\theta}_{LSTM}, \\theta_s)) \\] <p>Anhand der Gleichung sieht man genau, dass die Sequenzen von beiden Richtungen durchgegangen werden. Um die Wahrscheinlichkeit f\u00fcr das Vorkommen eines Wortes innerhalb einer Sequenz zu berechnen, wird in der Bedingung jeweils die linke oder rechte Seite verwendet. Zus\u00e4tzlich werden noch die zugeh\u00f6rigen Gewichtsmatrizen \\(\\theta_x, \\vec\\theta_{LSTM}, \\overleftarrow{\\theta}_{LSTM}, \\theta_s\\) in die Wahrscheinlichkeitsberechnung mit rein gerechnet. Schlie\u00dflich kann jedes Token als eine Menge von Vektorrepr\u00e4sentaion dargestellt werden. Die Gr\u00f6\u00dfer einer solchen Menge ist abh\u00e4ngig von der Anzahl der LSTM-Zellen, wodruch sich folgende Gleichung ergibt: \\(size(R_t) = 2 * L + 1\\), wobei L die Anzahl der LSTM-Zellen ist. Somit l\u00e4sst sich jedes Token als eine Menge von Vektorrepr\u00e4sentationen darstellen.</p> \\[ R_t = \\{x_k^{LM}, \\vec{h}_{k, j}^{LM}, \\overleftarrow{h}_{k, j}^{LM} | j = 1, ... , L\\} \\] <p> </p> Fig. ELMo <p>Zu guter letzt muss man noch mutli-sense embeddings betrachten. Hier versucht man die Mehrdeutigkeit von W\u00f6rtern abzubilden. M\u00f6gliche Ans\u00e4tze sind:</p> <ul> <li> <p>Sense2Vec</p> </li> <li> <p>ELMo + Clustering</p> </li> <li> <p>Transformer</p> </li> </ul> <p>Speziell benutzt man clustering Methoden, um die Mehrdeutigkeit von W\u00f6rtern abzubilden. Hierbei werden die Embeddings von W\u00f6rtern in Cluster unterteilt, wobei pro Cluster ein neue Repr\u00e4sentation des Wortes entsteht. Um dies zu erreichen, werden cluster Algorithmen ben\u00f6tigt, die mit hochdimensonalen Vektoren umgehen k\u00f6nnen. Beispiele sind: K-Means, DBSCAN, Mean-Shift, Affinity Propagation, Spectral Clustering, Hierarchical Clustering, Gaussian Mixture Models und noch viele mehr. Im folgenden werden die Algorithmen K-Means und Mean-Shift genauer betrachtet.</p> <p>K-Means ist ein iterativer Algorithmus, der versucht die Daten in k Cluster zu unterteilen. Hierbei wird die Distanz zwischen den Datenpunkten und den Clusterzentren minimiert. Die Clusterzentren werden durch den Mittelwert der Datenpunkte gebildet. Der Algorithmus funktioniert wie folgt:</p> <ol> <li> <p>W\u00e4hle k zuf\u00e4llige Punkte als Clusterzentren</p> </li> <li> <p>Berechne die Distanz zwischen den Datenpunkten und den Clusterzentren</p> </li> <li> <p>Weise jeden Datenpunkt dem n\u00e4chstgelegenen Clusterzentrum zu</p> </li> <li> <p>Berechne die neuen Clusterzentren</p> </li> <li> <p>Wiederhole Schritt 2 bis 4, bis sich die Clusterzentren konvergieren</p> </li> </ol> <p>Der Algorithmus ist einfach zu implementieren und schnell in der Ausf\u00fchrung. Doch er hat auch Nachteile. Zum einen muss die Anzahl der Cluster k bekannt sein. Zum anderen ist der Algorithmus anf\u00e4llig f\u00fcr Ausrei\u00dfer, was zu falschen Clusterzentren f\u00fchren kann.</p> <p>Mean-Shift ist ein iterativer Algorithmus, der auf der Idee basiert, dass Datenpunkte zur lokalen Dichtemaxima tendieren. Er kann verwendet werden, um nat\u00fcrliche Cluster in Daten zu finden, ohne die Anzahl der Cluster im Voraus zu kennen. Somit ben\u00f6tig man einen Kernel, den man vorab initialisieren muss. Meistens wird hierf\u00fcr der Gau\u00df-Kernel verwendet. \\(K(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{x^2}{2\\sigma^2}}\\). Danach kann die Suche nach den lokalen Dichtemaxima beginnen. Der Algorithmus funktioniert wie folgt:</p> <ol> <li> <p>W\u00e4hle einen Datenpunkt als Startpunkt</p> </li> <li> <p>Berechne den Kreis mit Radius r um den Startpunkt</p> </li> <li> <p>Berechne den Schwerpunkt der Datenpunkte innerhalb des Kreises</p> </li> <li> <p>Setze den Schwerpunkt als neuen Mittelpunkt</p> </li> <li> <p>Wiederhole Schritt 2 bis 4, bis sich die Mittelpunkte konvergieren</p> </li> </ol> <p>Somit wird der Mittelpunkt immer n\u00e4her an das lokale Dichtemaxima herangef\u00fchrt, da die Datenpunkte innherhalb des Kreises immer mehr werden sollen. Diese Eingenschaft des Algorithmus wird auch als hill climbing bezeichnet.</p> <p> </p> Fig. Mean Shift <p>Ein Vorteil von Mean-Shift-Clustering ist, dass es automatisch die Anzahl der Cluster bestimmt, da die Cluster durch die lokalen Maxima der Datenpunktdichte definiert werden. Der Algorithmus ist auch robust gegen\u00fcber Rauschen und kann Cluster mit komplexen Formen erfassen. Allerdings kann die Performance des Algorithmus bei gro\u00dfen Datens\u00e4tzen beeintr\u00e4chtigt sein, da er eine hohe Rechenleistung erfordert, um die Dichte in einem hochdimensionalen Raum zu berechnen.</p>"},{"location":"Themen/LLMs/#22-transformer","title":"2.2 Transformer","text":"<p>Der Transformer ist ein fortschrittliches neuronales Netzwerkmodell, das in der nat\u00fcrlichen Sprachverarbeitung (NLP) weit verbreitet ist. Er wurde erstmals 2017 in einem bahnbrechenden Paper namens \"Attention is All You Need\" vorgestellt. Im Gegensatz zu traditionellen rekurrenten und faltenden Architekturen hat der Transformer in NLP-Aufgaben wie maschineller \u00dcbersetzung, Textklassifikation und Spracherkennung f\u00fcr Aufsehen gesorgt. Das Kernkonzept des Transformers ist die Verwendung von Self-Attention-Mechanismen, die es dem Modell erm\u00f6glichen, relevante Teile der Eingabe zu identifizieren und deren Beziehungen zu modellieren. Der Transformer nutzt auch andere Techniken wie Positional Encoding, um die Positionsinformationen der Eingabesequenzen zu ber\u00fccksichtigen, und Multi-Head Attention, um verschiedene Aufmerksamkeitsrepr\u00e4sentationen zu erfassen. Dar\u00fcber hinaus umfasst der Transformer Residual Connections und Layer Normalization, um den Trainingsprozess zu stabilisieren, sowie Dropout, um Overfitting zu verhindern. Der Optimizer steuert die Aktualisierung der Modellparameter, w\u00e4hrend die Ausgabeschicht die Vorhersagen des Modells erzeugt. Insgesamt bietet der Transformer eine effiziente Verarbeitung von Sequenzdaten und ist zu einem grundlegenden Modell in der NLP-Forschung und -Anwendung geworden.</p> <p> </p> Fig. Transformer Architektur <p>Wichtig<p>Die Transformer Architektur ist die state of the art Architektur f\u00fcr NLP Aufgaben.</p> </p>"},{"location":"Themen/LLMs/#221-positional-encoding","title":"2.2.1 Positional Encoding","text":"<p>Positional Encoding wird verwendet, um die Positionsinformationen der Eingabesequenzen in den Wortembeddings zu ber\u00fccksichtigen. Da der Transformer keine rekurrenten oder faltenden Schichten enth\u00e4lt, m\u00fcssen die Positionsinformationen explizit in das Modell integriert werden. Das Positional Encoding wird den Wortembeddings hinzugef\u00fcgt und erm\u00f6glicht es dem Transformer, die relative Position der W\u00f6rter in der Eingabesequenz zu ber\u00fccksichtigen.</p> <p>Es besteht aus Sinus- und Kosinus-Funktionen mit unterschiedlichen Frequenzen und Phasen. Die Idee dahinter ist, dass die Kombination dieser Funktionen eine eindeutige Repr\u00e4sentation f\u00fcr jede Position in der Sequenz erzeugt. Das Positional Encoding wird dann elementweise zu den Wortembeddings addiert, um die Information \u00fcber die Positionen in den Gesamtvektor einzuf\u00fcgen.</p> <p>Die Formel f\u00fcr das Positional Encoding lautet:</p> \\[ PE_{(pos, 2i)} = \\sin (\\frac{pos}{10000^{2i/d_{\\text{model}}}})\\] \\[ PE_{(pos, 2i+1)} = \\cos (\\frac{pos}{10000^{2i/d_{\\text{model}}}}) \\] <p>Hierbei steht \\(pos\\) f\u00fcr die Position in der Sequenz, \\(i\\) f\u00fcr die Dimension des Wortembeddings und \\(d_{\\text{model}}\\) f\u00fcr die Gr\u00f6\u00dfe des Wortembeddings. Das Positional Encoding hat die gleiche Dimension wie die Wortembeddings, sodass es direkt zu ihnen addiert werden kann.</p> <p> </p> Fig. positional encoding <p>Durch schafft man es, dass die Position von W\u00f6rtern in unterschiedlichen Satzl\u00e4ngen gleich repr\u00e4sentiert werden. Den sowohl \\(i\\) als auch \\(d_{\\text{model}}\\) sind konstant, sodass die Positionsinformationen von einzelnen W\u00f6rtern an der gleichen Stelle nicht von der L\u00e4nge der Eingabesequenz abh\u00e4ngen.</p> <p> </p> Fig. positional encoding Tabelle: (i, pos)  <p>Durch die Verwendung des Positional Encoding kann der Transformer die Positionsinformationen der W\u00f6rter ber\u00fccksichtigen, ohne dass hierf\u00fcr rekurrente oder faltende Schichten ben\u00f6tigt werden. Dies erm\u00f6glicht dem Modell, sowohl die Bedeutung der W\u00f6rter als auch ihre Position in der Sequenz zu erfassen und komplexe Zusammenh\u00e4nge zwischen ihnen zu erlernen, die auf l\u00e4ngere Sequenzen verallgemeinert werden k\u00f6nnen. Dadurch k\u00f6nnte das Modell effektiv Eingabesequenzen unterschiedlicher L\u00e4ngen verarbeiten. Vereinfacht l\u00e4sst es sich so darstellen:</p> \\[ pe_{k} = \\frac{1}{10000^{2k/d_{\\text{model}}}} \\] \\[\\begin{bmatrix} e_0  \\\\ e_1  \\\\ e_2  \\\\ e_3  \\\\ ...  \\\\ e_n  \\\\ \\end{bmatrix}_{d_n \\times 1} + \\begin{bmatrix} \\sin(pe_1 * 1) \\\\  \\cos(pe_1 * 1) \\\\  \\sin(pe_2 * 1) \\\\  \\cos(pe_2 * 1) \\\\ ... \\\\  \\sin(pe_{n/2} * 1) \\\\  \\cos(pe_{n/2} * 1) \\\\ \\end{bmatrix}_{d_n \\times 1}\\]"},{"location":"Themen/LLMs/#222-architektur","title":"2.2.2 Architektur","text":"<p>Der Encoder schafft ein Verst\u00e4ndnis f\u00fcr die Eingabesequenz, indem er die Embeddings durch mehrere hintereinandere kommende Attenttion und fully connected feed forward Schichten verarbeitet. Ein Encoder-Schicht besteht aus einer multi-head self attention und einem fully connected feed forward network. Diese beiden werden Subschichten genannt. Zwishcen den Subschichten gibt es residual connections und layer normalization, um das Netzwerk zu stabilisieren und die Trainingszeit zu verk\u00fcrzen. Die Aufgabe des Encoders ist es, eine Repr\u00e4sentation der Eingabesequenz zu erzeugen, wo die Bedeutung der W\u00f6rter, ihre Position in der Sequenz, Merkmale und globale Abh\u00e4ngigkeiten ber\u00fccksichtigt werden, damit der Decoder die Ausgabe erzeugen kann.</p> <p> </p> Fig. Encoder - Layer  <p>Der Decoder generiert die Ausgabe des Transformer Modelles. Eine Schicht des Decoders besteht aus einer masked multi-head attention gefolgt von einer mulit-head cross-attention und schlie\u00dflich einem fully connected feed forward network. Diese drei Schichten sind die Subschichten einer Decoder-Schicht. Die masked multi-head attention Schicht ist eine multi-head attention Schicht, die eine Maske verwendet, um die Aufmerksamkeit auf die zuk\u00fcnftigen W\u00f6rter zu beschr\u00e4nken. Die multi-head cross-attention Schicht verwendet die Ausgabe der masked multi-head attention Schicht als Query und die Ausgabe des Encoders als Key und Value. Die Berechnung der masked attention gleicht der, der self attention. Somit wird dem Decoder erm\u00f6glicht, die Ausgabe des Encoders zu verwenden, um die Bedeutung der W\u00f6rter in der Eingabesequenz zu verstehen und die Ausgabe zu erzeugen. Die fully connected feed forward network Schicht ist die gleiche wie im Encoder. Der Decoder verwendet auch residual connections und layer normalization zwischen den Subschichten, um das Modell weiter zu stabilisieren.</p> <p> </p> Fig. Decoder - Layer  <p>Attention ist ein zentrales Konzept in Transformer-Modellen und erm\u00f6glicht es dem Modell, sich auf relevante Teile der Eingabesequenz zu konzentrieren oder relevante Informationen zu extrahieren. Der Sinn von Attention besteht darin, die Verbindungen und Abh\u00e4ngigkeiten zwischen verschiedenen Teilen der Eingabesequenz zu erfassen und diese Informationen in der Modellverarbeitung zu ber\u00fccksichtigen. Die Attention kann in verschiedene Typen unterteilt werden, die in den folgenden Abschnitten beschrieben werden.</p> <p> </p> Fig. Attention Typen"},{"location":"Themen/LLMs/#223-self-attention","title":"2.2.3 Self-Attention","text":"<p>Self-attention wird auch als intra oder scaled dot product attention bezeichnet. Wie der Name schon vorwegnimmt, geht es darum verschiedenste Vektoren durch das Skalarprodukt zu verrechnen, um die Relevanz zwischen einzelnen Positionen zu berechnen. Somit wird jeder Vektor in einen Query, Key und Valuevektor zerlegt, wobei die Dimension dieser drei Vektoren kleiner ist als zum Originalvektor. Um Query, Key und Value Vektoren zu erhalten, muss man den Original Vektor mit drei spezifischen Gewichtsmatrizen multiplizieren.</p> \\[Q = XW^Q; K = XW^K; V = XW^V\\] <p>Die Gewichtsmatrizen werden w\u00e4hrend des Trainings gelernt. Die Dimension der Gewichtsmatrizen ist \\(d_{\\text{model}} \\times d_k\\) f\u00fcr \\(W^Q\\) und \\(W^K\\) und \\(d_{\\text{model}} \\times d_v\\) f\u00fcr \\(W^V\\). Die Dimension des Originalvektors wird als \\(d_{\\text{model}}\\) bezeichnet und \\(d_k\\) als auch \\(d_v\\) sind die Dimensionen der Query, Key und Value Vektoren.</p> <p> </p> Fig. Self-Attention  <p>Als erstes wird das Skalarprodukt zwischen den Query und Key Vektoren berechnet. Das Skalarprodukt wird durch die Division mit \\(\\sqrt{d_k}\\) skaliert. Die Skalierung ist wichtig, da sie bei der Berechnung der Aufmerksamkeitsgewichte hilft, die Varianz der Werte zu reduzieren und die Stabilit\u00e4t der Gradienten zu verbessern. Danach folgt die Anwendung der Softmax Funktion, um die Summe aller positiven Wert auf 1 bringen und damit das Ergebnis zu normalisieren. Schlie\u00dflich werden alle Value Vektoren mit dem Softmax Ergebnis multipliziert, um wichtige Informationen zu verst\u00e4rken und unwichtige Informationen zu unterdr\u00fccken. Das Ergebnis ist die Summe aller gewichteten Value Vektoren. Die Self-Attention Formel kann wie folgt dargestellt werden, wenn man die Matrixmultiplikationen ber\u00fccksichtigt:</p> \\[\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\\] <p>Das Ergebnis der self-attention wird dann angepasst durch residual connections und layer normalization, was dann in das feed forward network eingespeist wird.</p>"},{"location":"Themen/LLMs/#224-cross-attention","title":"2.2.4 Cross-Attention","text":"<p>Cross-attention wird auch als inter attention bezeichnet. Die Brechnung ist gleich der, der self-attention, wobei die Query Vektoren aus dem Decoder kommen und die Key und Value Vektoren aus dem Encoder. Die cross-attention Formel kann wie folgt dargestellt werden:</p> \\[\\text{Attention}(Q_D, K_E, V_E) = \\text{softmax}(\\frac{Q_DK_E^T}{\\sqrt{d_k}})V_E\\] <p>Der Zweck der Cross-Attention besteht darin, dem Decoder Kontextinformationen zu liefern, die auf den Inhalten der Eingabesequenz basieren. Dadurch kann der Decoder relevante Informationen aus der Eingabe extrahieren und sie in den Generierungsprozess der Ausgabe einbeziehen. Dies erm\u00f6glicht dem Modell eine bessere Ber\u00fccksichtigung der globalen Zusammenh\u00e4nge und Kontexte w\u00e4hrend der Generierung und verbessert die Leistungsf\u00e4higkeit des Modells insgesamt.</p>"},{"location":"Themen/LLMs/#225-masked-attention","title":"2.2.5 Masked Attention","text":"<p>Die Idee der Masked Attention ist es, die Aufmerksamkeit auf zuk\u00fcnftige Tokens zu beschr\u00e4nken. Dies wird erreicht, indem die Repr\u00e4sentation der zuk\u00fcnftigen Tokens auf \\(-\\infty\\) gesetzt werden. Dadurch wird die Softmax Funktion dazu gezwungen, die Aufmerksamkeitsgewichte der zuk\u00fcnftigen Tokens auf 0 zu setzen. Die Masked Attention Formel kann wie folgt dargestellt werden:</p> \\[\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^TM}{\\sqrt{d_k}})V\\] <p>Die Maskierung in der maskierten Aufmerksamkeit besteht aus einer Matrix, die \\(-\\infty\\) f\u00fcr zuk\u00fcnftige Positionen und Einsen f\u00fcr aktuelle Positionen enth\u00e4lt. Diese Matrix wird auf die Aufmerksamkeitsgewichte angewendet, um sicherzustellen, dass das Modell nur auf bereits generierte Teile der Ausgabe zugreift. Eine g\u00e4ngige Methode f\u00fcr die Maskierung ist die Dreiecksmatrix. Ein Beispiel daf\u00fcr:</p> \\[M = \\begin{bmatrix} 1 &amp; -\\infty &amp; -\\infty &amp; -\\infty &amp; -\\infty \\\\ 1 &amp; 1       &amp; -\\infty &amp; -\\infty &amp; -\\infty \\\\ 1 &amp; 1       &amp; 1       &amp; -\\infty &amp; -\\infty \\\\ 1 &amp; 1       &amp; 1       &amp; 1       &amp; -\\infty \\\\ 1 &amp; 1       &amp; 1       &amp; 1       &amp; 1       \\\\ \\end{bmatrix}\\]"},{"location":"Themen/LLMs/#226-multi-head-attention","title":"2.2.6 Multi-Head Attention","text":"<p>Die Multihead Attention ist ein Schl\u00fcsselelement in Transformer-Modellen und erm\u00f6glicht es dem Modell, mehrere Aufmerksamkeitsgewichtungen gleichzeitig zu berechnen und verschiedene Aspekte der Eingabesequenz zu erfassen. Sie besteht aus mehreren parallelen Aufmerksamkeitsk\u00f6pfen, die unabh\u00e4ngig voneinander arbeiten. Jeder Aufmerksamkeitskopf hat seine eigenen Parametermatrizen f\u00fcr Query (Q), Key (K) und Value (V). Diese Matrizen werden verwendet, um die Aufmerksamkeitsgewichte f\u00fcr jede Position in der Eingabesequenz zu berechnen. Damit wird erreicht, dass sich das Transformer Modell auf verschiedene Aspekte der Eingabesequenz konzentrieren kann. Die Multihead Attention Formel kann wie folgt dargestellt werden:</p> \\[\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O\\] \\[\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\\] <p>Beim Zusammenf\u00fcgen der verschiedenen attention heads darf kein Durchschnitt gebildet werden, ansonsten w\u00fcrden Informationen verloren gehen, die zuvor berechnet wurden. Deshalb werden die attention heads konkateniert. Schlie\u00dflich muss die Dimensonalit\u00e4t der konkatenierten attention heads reduziert werden, damit die Dimensionen mit der Dimension der Eingabeschicht des feed forward network \u00fcbereinstimmen. Dies wird durch die Matrix \\(W^O\\) erreicht.</p> <p> </p> Fig. Multi-head Attention  <p>An dieser Stelle ist es wichtig zu erw\u00e4hnen, dass die Multihead Attention nicht nur f\u00fcr die self-attention verwendet wird, sondern auch f\u00fcr die cross-attention und die masked attention. Dank der hohen Parallelit\u00e4t, kann das Sprachverstehen im Transformer auf mehreren unterschiedlich tiefen Ebenen stattfinden.</p>"},{"location":"Themen/LLMs/#227-feed-forward-network","title":"2.2.7 Feed Forward Network","text":"<p>Das Feed Forward Network (FFN) besteht aus zwei linearen Transformationen mit einer ReLU Aktivierungsfunktion dazwischen. Das Ziel des FFNs ist es, die Aufmerksamkeitsrepr\u00e4sentationen zu verfeinern und weitere komplexe Zusammenh\u00e4nge \u00fcber die Nicht-Linearit\u00e4t zu erfassen. Die Nicht-Linearit\u00e4t wird durch die Verwendung von Aktivierungsfunktionen wie ReLU erreicht. Die Formel f\u00fcr das FFN kann wie folgt dargestellt werden:</p> \\[\\text{FFN}(x) = \\text{max}(0, xW_1 + b_1)W_2 + b_2\\] <p>Es w\u00e4ren neben der ReLU Aktivierungsfunktion auch andere Aktivierungsfunktionen m\u00f6glich, wie z.B. GELU oder ELU. Die ReLU Aktivierungsfunktion ist jedoch die am h\u00e4ufigsten verwendete Aktivierungsfunktion in der Praxis. Die ReLU Aktivierungsfunktion ist definiert als: \\(f(x) = \\text{max}(0, x)\\) und ist somit effektiv und einfach umzusetzen. GELU - Gaussian Error Linear Unit - eine Variante der ReLU Aktivierungsfunktion ist. Sie wird auch als die gegl\u00e4ttete ReLU-Funktion bezeichnet. Die GELU Aktivierungsfunktion ist definiert als: \\(f(x) = x\\Phi(x)\\), wobei \\(\\Phi(x)\\) die kumulative Verteilungsfunktion der Standardnormalverteilung ist und lautet daher: \\(\\Phi(x) = \\frac{1 + \\text{erf}(x / \\sqrt{2})}{2}\\) mit \\(\\text{erf}(z) = \\frac{2}{\\sqrt{\\Pi}} * \\int_{0}^{z} e^{-t^2} dt\\). Die GELU Aktivierungsfunktion ist eine stetige Funktion, weshalb sie differenzierbar ist und mit negativen Zahlen subtiler verf\u00e4hrt, wodurch - je nach Aufgabe - eine bessere Anpassung des Modelles stattfindet. Die ELU-Funktion - Exponential Linear Unit - ist eine weitere Aktivierungsfunktion und ist definiert als: \\(f(x) = x\\) f\u00fcr \\(x &gt; 0\\) und \\(f(x) = \\alpha(e^x - 1)\\) f\u00fcr \\(x \\leq 0\\), wobei \\(\\alpha\\) eine Konstante ist. Die ELU Aktivierungsfunktion ist eine stetige Funktion, die den gleichen Gedanken verfolgt wie die ReLU-Funktion. Jede dieser Funktionen kann eine Verbesserung des Modells bewirken, somit muss man diese der Aufgabe entsprechend ausw\u00e4hlen. Die GELU-Funktion ist sehr gut bei der Verarbeitung von Texten wie zum Beispiel bei der \u00dcbersetzung von Texten. Die ELU-Funktion wiederum l\u00f6st das Problem der toten Neuronen, die bei der Verwendung der ReLU-Funktion auftreten k\u00f6nnen. Die ELU-Funktion ist jedoch nicht so effizient wie die ReLU-Funktion, da sie exponentielle Funktionen verwendet. </p> \\[\\text{ReLU}(x) = \\begin{cases} x &amp; \\text{if } x &gt; 0 \\\\ x &amp; \\text{if } x \\leq 0 \\end{cases}\\] \\[\\text{GELU}(x) = x\\Phi(x)\\] \\[\\text{ELU}(x) = \\begin{cases} x &amp; \\text{if } x &gt; 0 \\\\ \\alpha(e^x - 1) &amp; \\text{if } x \\leq 0 \\end{cases}\\] <p> </p> Fig. Aktivierungsfunktionen"},{"location":"Themen/LLMs/#228-residual-connections","title":"2.2.8 Residual Connections","text":"<p>Mit den Residual Connections wird die Performance des Transformer Modells verbessert, indem gegen das Problem des Vanishing Gradients vorgegangen wird. Sie sind eine Art von Skip Connections, die es erm\u00f6glichen, die Informationen aus den vorherigen Schichten zu erhalten. Dabei wird die Ausgabe der vorherigen Schicht mit der Ausgabe der aktuellen Schicht addiert. Die Residual Connections werden wie folgt definiert:</p> \\[\\text{LayerNorm}(x + \\text{Sublayer}(x))\\] <p>Diese werden pro Sublayer einmal angewendet und tragen somit zur Stabilisierung des Netzes bei, sowie zur Verbesserung der Erkennung von komplexen Datenmuster.</p>"},{"location":"Themen/LLMs/#229-layer-normalization","title":"2.2.9 Layer Normalization","text":"<p>Die Layer Normalization ist eine Technik, die es erm\u00f6glicht, die Aktivierungen der vorherigen Schicht zu normalisieren um den Mittelwert von 0 mit der Standardabweichung von 1. Daraus folgt, dass die eizelnen Werte weniger streuen und somit die Aktivierungen der vorherigen Schicht weniger variieren. Die Layer Normalization wird wie folgt definiert:</p> \\[\\text{LayerNorm}(x) = \\alpha \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\\] <ul> <li> <p>x die Eingabeaktivierungen der Schicht</p> </li> <li> <p>\\(\\mu\\) ist der Durchschnitt der Aktivierungen \u00fcber die Merkmalsdimension</p> </li> <li> <p>\\(\\sigma\\) ist die Varianz der Aktivierungen \u00fcber die Merkmalsdimension</p> </li> <li> <p>\\(\\epsilon\\) ist ein kleiner Wert zur Stabilit\u00e4t, um eine Division durch Null zu verhindern</p> </li> <li> <p>\\(\\alpha\\) ist der skalare Parameter zur Skalierung der normalisierten Aktivierung</p> </li> <li> <p>\\(\\beta\\) ist der skalare Parameter zur Verschiebung der normalisierten Aktivierung</p> </li> </ul> <p>Die Layer Normalization wird pro Schicht angewendet und tr\u00e4gt somit zur Stabilisierung des Netzes bei, da \u00e4hnlich gro\u00dfe Gradientenschritte vollzogen werden und die einzelnen Wert nicht mehr zustark voneinander variieren.</p> <p> </p> Fig. Feature Dimension"},{"location":"Themen/LLMs/#2210-dropout-optimizer","title":"2.2.10 Dropout &amp; Optimizer","text":"<p>Dropout ist eine Regularisierungstechnik, die versucht zuf\u00e4llig Perzeptronen aka Neuronen zu deaktivieren, um die Generalisierung des Modells zu verbessern. Dies wird erreicht, indem die Gewichte der deaktivierten Neuronen auf 0 gesetzt werden. Die Dropout Rate ist ein Hyperparameter, der die Wahrscheinlichkeit bestimmt, mit der ein Neuron deaktiviert wird. Er liegt zwischen \\(0.1\\) und \\(0.5\\). Es wird eingesetzt, um das Problem des Overfittings zu vermeiden. Dropout l\u00e4sst sich wie folgt definieren: \\(\\frac{EM}{1-p}\\), dabei sind E die Aktivierungen oder Werte der vorherigen Schicht, M eine bin\u00e4re Maske mit derselben Form wie die Eingabe, die zuf\u00e4llig auf 0 oder 1 gesetzt wird und p die Wahrscheinlichkeit, dass eine Aktivierung deaktiviert wird. Die Bin\u00e4rmaske M wird immer wieder neu generiert und ist somit nicht statisch. </p> \\[M = \\begin{cases} 1 &amp; \\text{if } \\text{Bernoulli}(p) == 1 \\\\ 0 &amp; \\text{if } \\text{Bernoulli}(p) == 0 \\end{cases} \\] <p>Der Adam Optimizer - Adaptive Momentum - ist ein Optimierungsalgorithmus, der die Lernrate f\u00fcr jedes Gewicht individuell anpasst. Er wird verwendet, um die Lernrate \\(\\alpha\\) anzupassen, sodass das globale Minimum schnellst m\u00f6glich erreicht wird. Der Adam Optimizer ist eine Kombination aus dem Momentum und der adaptiven Anpassung der Lernrate. Er nutzt die Konzepte der Impulserhaltung des Stochastic Gradient Descent und das des RMSProp. Der Adam Optimizer wird wie folgt definiert:</p> \\[\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t\\] <ul> <li> <p>\\(\\theta_t\\) sind die Gewichte des Netzes</p> </li> <li> <p>\\(\\hat{v}_t\\) ist die gesch\u00e4tzte Varianz des Gradienten: \\(\\hat{v}_t = \\beta_2 v_{t-1} + (1 - \\beta_2)g_t^2\\)</p> </li> <li> <p>\\(\\hat{m}_t\\) ist der gesch\u00e4tzte Mittelwert des Gradienten: \\(\\hat{m}_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t\\)</p> </li> <li> <p>\\(\\beta_1\\) und \\(\\beta_2\\) sind die Exponentialfaktoren f\u00fcr den Mittelwert und die Varianz</p> </li> <li> <p>\\(\\epsilon\\) ist ein kleiner Wert zur Stabilit\u00e4t, um eine Division durch Null zu verhindern</p> </li> </ul> <p> </p> Fig. Gradient Descent"},{"location":"Themen/LLMs/#2211-output-layer","title":"2.2.11 Output Layer","text":"<p>Die Ausgabe des Transformers ist ein Vektor, der die Wahrscheinlichkeit f\u00fcr jedes Token im Vokabular angibt. Somit wird nur das n\u00e4chste Token vorrausgesagt und dann sequentiel folgend der Rest. Die Wahrscheinlichkeit wird mit der Softmax Funktion berechnet. Die Softmax Funktion wird wie folgt definiert:</p> \\[\\text{softmax}(z_{ij}) = \\frac{e^{z_{ij}}}{\\sum_{j=1}^{L} e^{z_{ij}}}\\] <ul> <li> <p>\\(z_{ij}\\) ist die Aktivierung an der Position \\(i\\) und \\(j\\) innerhalb einer Ausgabematrix</p> </li> <li> <p>\\(L\\) ist die Anzahl der Feautures</p> </li> </ul> <p>Die Softmax Funktion wird pro Zeile angewendet und somit wird die Summe der Wahrscheinlichkeiten pro Zeile 1 ergeben.</p> <p> </p> Fig. Softmax"},{"location":"Themen/LLMs/#23-bert","title":"2.3 BERT","text":"<p>Bert, vollst\u00e4ndig bekannt als Bidirectional Encoder Representations from Transformers, ist ein Transformer-Modell, das auf der Encoder-Struktur basiert. Es z\u00e4hlt zur Kategorie der Autoencoder-Modelle und ist darauf spezialisiert, Benutzereingaben zu verstehen und zu verarbeiten.</p> <p> </p> Fig. Bert"},{"location":"Themen/LLMs/#231-architektur","title":"2.3.1 Architektur","text":"<p>BERT - Bidirectional Encoder Representations from Transformers - ist ein Modell f\u00fcr maschinelles Lernen, das Kontextinformationen ber\u00fccksichtigt, indem es die umgebenden W\u00f6rter verwendet, was in der traditionellen Tranformerarchitektur nicht der Fall ist. BERT verwendet eine Kombination aus Masked Language Model (MLM) und Next Sentence Prediction (NSP) Aufgaben, um den Kontext zu erfassen.</p> <p> </p> Fig. Kontext  <p>Die Basisarchitektur von BERT besteht aus einem Stapel von 12 Encoder-Layern. Jeder Encoder-Layer setzt sich aus einer Multi-Head Attention-Schicht und einer Feed Forward-Schicht zusammen, die ihre Verarbeitung der Eingabesquenz an das n\u00e4chste Layer weitergeben. F\u00fcr die Multi-Head Attention wird eine Aufteilung in 12 attention heads vorgenommen. Jeder Attention-Head fokussiert sich auf unterschiedliche Aspekte des Kontexts und erfasst verschiedene Beziehungen zwischen den W\u00f6rtern. Diese Vielfalt an Attention-Heads erm\u00f6glicht es BERT, verschiedene Arten von Abh\u00e4ngigkeiten und Zusammenh\u00e4ngen in der Benutzereingabe zu ber\u00fccksichtigen. Die Gr\u00f6\u00dfe der Embeddings, die BERT verwendet, betr\u00e4gt 768. Das bedeutet, dass jedes Token in der Eingabe in einem Vektorraum mit einer Dimension von 768 repr\u00e4sentiert wird.</p> <p> </p> Fig. Architektur  <p>Bevor die Eingabe an den Encoder weitergegeben wird, wird sie durch einen Tokenizer in einzelne Token aufgeteilt. Diese Tokens werden dann in Embeddings umgewandelt, die die semantische Bedeutung der W\u00f6rter repr\u00e4sentieren. Zus\u00e4tzlich werden Positionsinformationen hinzugef\u00fcgt, um die Reihenfolge und Zugeh\u00f6rigkeit der W\u00f6rter im Satz zu kodieren. Dies ist notwendig, da der Transformer-Algorithmus, auf dem BERT basiert, parallele Strukturen verwendet und die Positionsinformationen helfen, den Kontext korrekt zu erfassen.</p> <p>Die Ausgabe des Encoders wird schlie\u00dflich durch einen Softmax-Layer in Token-Wahrscheinlichkeiten umgewandelt. Diese Wahrscheinlichkeiten geben an, wie wahrscheinlich es ist, dass jedes Token an einer bestimmten Position im Satz auftritt.</p> <p> </p> Fig. Encoding  <p>Bert verwendet einen WordPiece-Tokenizer, der die Eingabes\u00e4tze in einzelne Tokens aufteilt. Jede Sequenz beginnt mit einem speziellen Token [CLS], gefolgt von den Tokens des ersten Satzes, dann einem Trennungs-Token [SEP], und schlie\u00dflich den Tokens des zweiten Satzes, wieder gefolgt von einem Trennungs-Token [SEP].</p> <p>Der Tokenizer kann auch Subword-Aufteilung durchf\u00fchren, wobei ein Token mit dem #-Zeichen gekennzeichnet wird, um anzuzeigen, dass es in mehrere Subtokens aufgeteilt wurde. Dies erm\u00f6glicht eine flexiblere Darstellung von W\u00f6rtern, insbesondere bei seltenen oder unbekannten W\u00f6rtern.</p> <p>Das Vokabular von Bert besteht insgesamt aus 30.000 Tokens, die zur Kodierung von W\u00f6rtern verwendet werden k\u00f6nnen. Jedes Token wird in einem Token-Embedding repr\u00e4sentiert, das die semantische Bedeutung des Tokens erfasst. Zus\u00e4tzlich gibt es ein Segment-Embedding, das die Zugeh\u00f6rigkeit eines Tokens zu einem bestimmten Satz angibt. Dadurch kann Bert zwischen den beiden S\u00e4tzen unterscheiden und den Kontext richtig erfassen. Schlie\u00dflich gibt es ein Position-Embedding, das die Position jedes Tokens im Satz angibt. Dies ist wichtig, da der Transformer-Algorithmus parallele Strukturen verwendet und die Positionsinformationen helfen, den Kontext korrekt zu kodieren.</p> <p>Zusammen bilden das Token-Embedding, das Segment-Embedding und das Position-Embedding das Gesamt-Embedding f\u00fcr die Eingabe von Bert. Dieses Embedding erfasst die Beziehungen zwischen den Tokens, die Zugeh\u00f6rigkeit zu verschiedenen S\u00e4tzen und die Position in den S\u00e4tzen, was zur pr\u00e4zisen Modellierung des Kontexts beitr\u00e4gt.</p>"},{"location":"Themen/LLMs/#232-masked-language-model","title":"2.3.2 Masked Language Model","text":"<p>Beim Masked Language Modeling (MLM) in BERT wird versucht, maskierte W\u00f6rter in der Eingabesequenz vorherzusagen, um dem Modell ein grundlegendes Sprachverst\u00e4ndnis zu erm\u00f6glichen. Dieser Prozess wird durchgef\u00fchrt, indem etwa 15% der Tokens in der Eingabesequenz zuf\u00e4llig durch das [MASK]-Token ersetzt werden. Das Modell versucht dann, das urspr\u00fcngliche Token anhand einer Wahrscheinlichkeitsverteilung des Vokabulars zu bestimmen. Durch das Maskieren und Vorhersagen von W\u00f6rtern wird das Modell dazu angeleitet, den Kontext und die Beziehungen zwischen den W\u00f6rtern zu verstehen. Indem es den umgebenden Kontext verwendet, um die maskierten Tokens zu rekonstruieren, entwickelt das Modell ein Verst\u00e4ndnis f\u00fcr die Bedeutung und Struktur von S\u00e4tzen. Die Wahrscheinlichkeiten f\u00fcr die vorhergesagten W\u00f6rter basieren auf einer Verteilung \u00fcber das Vokabular, die durch den Softmax-Layer im BERT-Modell berechnet wird. Das Modell gibt Wahrscheinlichkeiten f\u00fcr jedes m\u00f6gliche Token im Vokabular aus, und das wahrscheinlichste Token wird als Vorhersage f\u00fcr das maskierte Token genommen.</p> <p> </p> Fig. Masked Language Model"},{"location":"Themen/LLMs/#233-next-sentence-prediction","title":"2.3.3 Next Sentence Prediction","text":"<p>Bei der Next Sentence Prediction (NSP) geht es darum, dass BERT versucht, zu bestimmen, ob ein Satz A auf einen Satz B folgt. Dieser Ansatz erm\u00f6glicht es BERT, das Verst\u00e4ndnis f\u00fcr den Zusammenhang zwischen S\u00e4tzen zu erlernen. W\u00e4hrend des Trainings wird eine Aufteilung von 50/50 verwendet, bei der BERT sowohl positive als auch negative Beispiele f\u00fcr die NSP-Aufgabe erh\u00e4lt. Positive Beispiele bestehen aus aufeinanderfolgenden S\u00e4tzen, w\u00e4hrend negative Beispiele aus zuf\u00e4llig ausgew\u00e4hlten S\u00e4tzen bestehen, die nicht in einem direkten Zusammenhang stehen. Durch diese Aufteilung wird BERT dazu angeleitet, die Beziehungen zwischen S\u00e4tzen zu verstehen und zu lernen, ob ein Satz auf den vorherigen folgt oder nicht. Dies tr\u00e4gt dazu bei, dass BERT ein umfassendes Verst\u00e4ndnis des Kontexts und der Abh\u00e4ngigkeiten zwischen S\u00e4tzen entwickelt.</p> <p> </p> Fig. Next Sentence Prediction  <p>Die Entscheidung, ob zwei S\u00e4tze aufeinander folgen, wird \u00fcber die Sigmoid-Funktion getroffen. Der Klassifikator besteht in der Regel aus einem Fully Connected Layer, gefolgt von einer Sigmoid-Aktivierungsfunktion. Das Fully Connected Layer nimmt die Ausgabe des BERT-Modells und projiziert sie auf eine einzige Ausgabedimension. Diese Ausgabedimension wird dann der Sigmoid-Funktion \u00fcbergeben, die eine Wahrscheinlichkeit zwischen 0 und 1 erzeugt.</p> \\[ \\sigma(W_{NSP} \\cdot [CLS]_{NSP} + b_{NSP}) \\] <p>Hierbei stellt \\(\\sigma\\) die Sigmoid-Funktion dar, \\(W_{\\text{NSP}}\\) sind die Gewichte des Fully Connected Layers f\u00fcr die NSP-Aufgabe und \\(b_{\\text{NSP}}\\) ist der Bias-Term. \\([CLS]_{NSP}\\) ist das Embedding des [CLS]-Tokens, das als Eingabe f\u00fcr den Klassifikator verwendet wird. Es beinhaltet die wichtigsten Informationen \u00fcber die ganze Sequenz, wodurch es in der Lage es ist, die NSP-Aufgabe zu l\u00f6sen.</p>"},{"location":"Themen/LLMs/#234-pre-training","title":"2.3.4 Pre-Training","text":"<p>W\u00e4hrend des Trainings von BERT wird die Ausgabe des Modells f\u00fcr das Masked Language Modeling (MLM) mit One-Hot-kodierten Vektoren validiert, w\u00e4hrend die Ausgabe f\u00fcr die Next Sentence Prediction (NSP) mit einem Bin\u00e4rwert validiert wird. Das Modell wird mithilfe des Cross-Entropy Loss trainiert, wobei die Wahrscheinlichkeitsverteilung der Vorhersagen mit den tats\u00e4chlichen Werten verglichen wird. Der Loss wird nur bei den maskierten Tokens verwendet, um Trainingszeit zu sparen.</p> \\[ L_i = -\\sum_{j=1}^{V} y_{ij} \\log(\\hat{y}_{ij}) \\] <p>Dabei steht \\(y_{ij}\\) f\u00fcr den tats\u00e4chlichen Wert des Labels und \\(\\hat{y}_{ij}\\) f\u00fcr die vorhergesagte Wahrscheinlichkeit des Modells f\u00fcr die jeweilige Klasse oder Kategorie. Der Loss wird f\u00fcr jedes Token oder jeden Satz berechnet und aggregiert. Das Pre-Training von BERT wird mit einer Batch-Gr\u00f6\u00dfe von 256 und einer Lernrate von \\(1e-4\\), die mit dem  Adam Optimizer angepasst wird, durchgef\u00fchrt. Dabei werden Cloud-TPUv3-Ressourcen verwendet.</p>"},{"location":"Themen/LLMs/#235-fine-tuning","title":"2.3.5 Fine-Tuning","text":"Fig. Fine tuning Bert  <p>Nach dem Pre-Training kann BERT f\u00fcr spezifische Aufgaben durch sogenanntes Fine-Tuning weiter optimiert werden. Das initiale BERT-Modell wird dabei als Ausgangspunkt verwendet, da es bereits auf das grundlegende Sprachverst\u00e4ndnis trainiert ist. Beim Fine-Tuning werden die Parameter des vortrainierten Modells an die spezifische Aufgabe angepasst, indem bestimmte Teile des Modells mit einem Aufgabendatensatz neu trainiert werden.</p> <p>Die Ausgabe eines feinabgestimmten BERT-Modells variiert je nach Aufgabe und erfordert die Identifizierung der relevanten Sequenzteile. Dieser Prozess unterscheidet sich von Aufgabe zu Aufgabe. Nehmen wir als Beispiel eine Frage-Antwort-Aufgabe, bei der die Antwort auf eine gestellte Frage gefunden werden soll. In diesem Fall befinden sich die relevanten Informationen, die die Antwort enthalten, hinter dem ersten [SEP]-Token in der Sequenz. Das bedeutet, dass die Tokens nach dem [SEP]-Token die Antwort repr\u00e4sentieren. Bei der Textklassifikation hingegen wird die Ausgabe des Modells durch das erste [SEP]-Token dargestellt, da es die Klassifikation des Textes widerspiegelt. Hierbei ist das erste [SEP]-Token das entscheidende Element, um die Kategorie oder Klasse des Textes zu identifizieren. Die genaue Identifizierung der relevanten Sequenzteile kann je nach Aufgabe unterschiedlich sein und erfordert eine Aufgaben-spezifische Behandlung der Modellausgabe.</p>"},{"location":"Themen/LLMs/#24-fine-tuning","title":"2.4 Fine-Tuning","text":"<p>Das Fine-Tuning-Prozess besteht darin, das vorhandene Wissen und die Sprachrepr\u00e4sentationen, die im Pre-Training gelernt wurden, auf die spezifische Aufgabe anzuwenden. Hierbei werden die Gewichte des Modells aktualisiert, um die Leistung auf der spezifischen Aufgabe zu verbessern. W\u00e4hrend des Fine-Tuning k\u00f6nnen verschiedene Schichten oder Komponenten des Modells, wie die oberen Schichten des Modells oder der Klassifikator, angepasst werden, w\u00e4hrend tiefere schichten des Modells fixiert / gefroren bleiben. Hierbei ist zu beachten, dass in den tieferen Schichten die F\u00e4higkeit des Modelles steckt, Sprache zu verstehen, w\u00e4hrend in den oberen Schichten die Aufgabe spezifisch gelernt wird. Das Fine-Tuning kann mit einem kleineren Datensatz als der des initialen Trainings durchgef\u00fchrt werden, da das Modell bereits ein grundlegendes Sprachverst\u00e4ndnis hat. Dies erm\u00f6glicht es, dass das Modell mit nur wenigen Trainingsbeispielen gute Ergebnisse erzielt.</p>"},{"location":"Themen/LLMs/#241-aufgaben","title":"2.4.1 Aufgaben","text":"Fig. FT-Aufgaben  <p>Die Trainingsstrategie f\u00fcr Sprachmodelle wie BERT besteht darin, zun\u00e4chst ein allgemeines Sprachverst\u00e4ndnis zu erlernen und anschlie\u00dfend spezifische Aufgaben mithilfe von Labeldaten zu trainieren. Die Aufgaben, auf die ein Sprachmodell feinabgestimmt werden kann, sind vielf\u00e4ltig und abh\u00e4ngig von den spezifischen Anforderungen und Zielen. Beispiele f\u00fcr solche Aufgaben k\u00f6nnen in der Abbildung FT-Aufgaben gefunden werden, die verschiedene Anwendungsbereiche wie Textklassifikation, Named Entity Recognition, Sentimentanalyse und vieles mehr umfasst.</p>"},{"location":"Themen/LLMs/#242-overfitting","title":"2.4.2 Overfitting","text":"Fig. Overfitting  <p>Beim fine tuning kann es zu einem Overfitting kommen, wenn das Modell zu stark auf die Trainingsdaten angepasst wird und die F\u00e4higkeit verliert, auf neue Daten zu generalisieren. Dies kann durch die Verwendung von Regularisierungsstrategien verhindert werden. Einige dieser Techniken sind: </p> <ul> <li>Dropout</li> <li>Early Stopping</li> <li>\\(L_1 / L_2\\) Regularisierung</li> <li>Verteilungsanpassung</li> </ul> <p>Dropout ist eine Technik, bei der zuf\u00e4llig ausgew\u00e4hlte Neuronen w\u00e4hrend des Trainings deaktiviert werden, um zu verhindern, dass das Modell zu stark auf die Trainingsdaten angepasst wird. Early Stopping ist eine Technik, bei der das Training gestoppt wird, wenn die Leistung des Modells auf den Validierungsdaten nicht mehr verbessert wird. Dies verhindert, dass das Modell zu stark auf die Trainingsdaten angepasst wird. \\(L_1 / L_2\\) Regularisierung ist eine Technik, bei der die Gewichte des Modells mit einem Strafterm belegt werden, um zu verhindern, dass das Modell zu stark auf die Trainingsdaten angepasst wird. Folglich werden die Gewichtungen gegl\u00e4ttet und das Modell ist stabiler gegen Unbekanntes, Rauschen und Ausrei\u00dfer. Dies dient dazu, dass der Verlustfunktionswert erh\u00f6ht wird, wenn die Gewichte des Modells zu gro\u00df werden. Die \\(L_1 / L_2\\) Regularisierung wird durch die folgenden Gleichungen definiert: \\(L_1 = \\sum_{i=1}^{n} |w_i|\\) und \\(L_2 = \\sum_{i=1}^{n} w_i^2\\), wobei zu beachten ist, dass der Loss auch auf dem Bias berechnet werden muss. Schlie\u00dflich addiert man den Loss der \\(L_1 / L_2\\) Regularisierung mit dem Loss der Daten, um den Gesamtloss zu erhalten.</p> \\[ regularization\\_loss = L_1 + L_2 \\] \\[ L = data\\_loss + regularization\\_loss \\] <p>Das Ziel der Verteilungsanpassung besteht darin, sicherzustellen, dass die Verteilung der Daten, auf die das feinabgestimmte Modell trainiert wird, \u00e4hnlich oder vergleichbar mit der Verteilung des urspr\u00fcnglichen Trainingsdatensatzes ist. Es geht darum, eine konsistente Verteilung der Daten beizubehalten, um sicherzustellen, dass das Modell auf \u00e4hnliche Beispiele und Muster trifft wie w\u00e4hrend des urspr\u00fcnglichen Trainings. Es darf nicht vergessen werden, dass die Daten des fine-tuned Modelles geb\u00fcndelt sind und die des pre-trained Modelles gestreut sind.</p>"},{"location":"Themen/LLMs/#25-generative-pre-training","title":"2.5 Generative Pre-Training","text":"<p>Generative Pre-Training, kurz GPT, ist eine Architektur, die auf der Decoder-Seite von Transformers basiert. GPT z\u00e4hlt zu den sogenannten auto-regressiven Modellen. Seine herausragende F\u00e4higkeit liegt in der Generierung von Texten, wie das aktuelle GPT-4-Modell beeindruckend demonstriert.</p> <p> </p> Fig. CHAT - GPT  <p>Im Folgenden werden wir \u00fcber die drei grundlegenden S\u00e4ulen von GPT sprechen, die es zu einem beeindruckenden Sprachmodell machen: Pre-Training + Fine-Tuning, Reward Model und Reinforcement Learning. Diese drei S\u00e4ulen bilden die Grundlage f\u00fcr das Training des Modells, um Texte zu generieren und weitere NLP-Aufgaben zu meistern. </p> <p>Die erste S\u00e4ule ist das Pre-Training + Fine-Tuning. Beim Pre-Training wird das GPT-Modell auf einer riesigen Menge an Textdaten trainiert. Hierbei wird dem Modell beigebracht, Sprachstrukturen, Zusammenh\u00e4nge und Semantik zu verstehen. Dabei wird ein sogenanntes unsupervised Learning verwendet, bei dem das Modell keine explizite Zielvorgabe erh\u00e4lt, sondern selbstst\u00e4ndig Muster und Regelm\u00e4\u00dfigkeiten im Text erkennt und generalisiert. Das Pre-Training erfolgt normalerweise auf einer gro\u00dfen Textsammlung, wie beispielsweise dem gesamten Internet. Nach dem Pre-Training folgt das Fine-Tuning, bei dem das Modell auf eine spezifische Aufgabe oder Dom\u00e4ne angepasst wird. Hierbei werden Textdaten verwendet, die durch Labeler vorbereitet werden, um das Modell auf spezifische Kontexte, Stile und Vorgaben einzustellen.</p> <p>Die zweite S\u00e4ule ist das Reward Model. Um das Modell auf bestimmte Ziele oder Pr\u00e4ferenzen auszurichten, wird ein sogenanntes Reward Model verwendet. Das Reward Model bewertet die Qualit\u00e4t der generierten Texte anhand vorgegebener Kriterien wie Koh\u00e4renz, Relevanz, Grammatik oder anderen dom\u00e4nenspezifischen Merkmalen. Das Modell wird dann durch Reinforcement Learning darauf trainiert, die jeweiligen NLP-Aufgaben zu meistern, die ein h\u00f6heres Belohnungssignal vom Reward Model erhalten. Dadurch kann das Modell lernen den gew\u00fcnschten Kriterien besser entsprechen.</p> <p>Das Reinforcement Learning erm\u00f6glicht es dem Modell, aus Erfahrung zu lernen und seine Textgenerierung kontinuierlich zu verfeinern. Indem es auf das Feedback des Reward Models reagiert und seine Strategien entsprechend anpasst, kann das Modell immer bessere Texte erzeugen, die den Anforderungen und Zielen gerecht werden.</p>"},{"location":"Themen/LLMs/#251-architektur-pre-training","title":"2.5.1 Architektur / Pre-Training","text":"Fig. Decoder-Layout  <p>Wie schon erw\u00e4hnt, basiert GPT auf dem Decoder Teil des originalen Transformermodelles. Dementsprechend ben\u00f6tigt es, wie auch das Transformermodell, eine Eingabe, die aus einer Sequenz von Token besteht, welche durch ein Embedding-Layer in einen Vektor umgewandelt wird. Dieser Vektor bekommt zus\u00e4tzlich \u00fcber das positional Encoding Informationen \u00fcber die Position der einzelnen Tokens. Schlie\u00dflich wird der Eingabevektor durch mehrere Decoderschichten verarbeitet. </p> \\[ h_0 = UW_e + W_p \\] <ul> <li> <p>\\(U\\) ist der Context Vektor: \\(U = (u_{-k}, ... , u_{-1})\\) bei einem Corpus von \\(C = {u_1, ... , u_n}\\)</p> </li> <li> <p>\\(k\\) ist die Gr\u00f6\u00dfe des Kontextes</p> </li> <li> <p>\\(W_e\\) ist die Embeddingmatrix der Tokens</p> </li> <li> <p>\\(W_p\\) ist die positional encoding Matrix</p> </li> </ul> <p>Diese Gleichung makiert den Startpunkt von GPT, in welcher die Initialisierungsschritte des GPT-Modells dargestellt werden. Der Vektor \\(h_0\\) repr\u00e4sentiert den Anfangszustand des Modells, der durch eine lineare Transformation von zwei Eingabevektoren erzeugt wird. Schlie\u00dflich wandern die Eingaben durch die Transformerbl\u00f6cke.</p> \\[ h_i = TransformerBlock(h_{i-1}) \\forall{i} \\in [1, n]\\] <p>Damit wird durch die ganzen Schichten des GPT-Modelles iteriert. Die Ausgabe des letzten Transformerblocks wird dann durch eine lineare Transformation in die Wahrscheinlichkeitsverteilung der n\u00e4chsten Token umgewandelt.</p> \\[ P(u) = \\text{softmax}(h_nW_e^T) \\] <p>Die Ausgabeformell basiert auf dem Hintergrund, dass die Wahrscheinlichkeit von einem Token \\(u\\) abh\u00e4ngt, welche Tokens im aktuellen Kontex vorausgehen. Die Wahrscheinlichkeit von \\(u\\) ist also abh\u00e4ngig von der Wahrscheinlichkeit der vorherigen Tokens und der Parameter des Modells. </p> \\[P(u_i | u_{i-k}, ... , u_{i-1}; \\theta)\\] <p>Das Modelle optimiert somit die Wahrscheinlichkeit \\(L_1(C) = \\sum_i \\log P(u_i | u_{i-k}, ... , u_{i-1}; \\theta)\\). Die Optimierung erfolgt durch das unsupervised Learning mit dem Ziel, die Wahrscheinlichkeit der n\u00e4chsten Token zu maximieren.</p> <p>Um bessere und nat\u00fcrlichere Ergebnisse zu erhalten, werden zuk\u00fcnftige GPT-Modelle mit sample Algorithmen erweitert, damit die Ausgaben im Wortlaut varrieren und nicht immer die gleichen Texte generiert werden.</p> <ol> <li> <p>Top-K-Verfahren: Die Wahrscheinlichkeiten der Tokens werden sortiert und k Tokens mit der h\u00f6chsten Wahrscheinlichkeit werden ausgew\u00e4hlt. Von denen wird dann zuf\u00e4llig ein Token ausgew\u00e4hlt, welches dann als n\u00e4chstes Token verwendet wird.</p> </li> <li> <p>Nucleus: Dieses Verfahren funktioniert \u00e4hnlich zu Top-K, jedoch wird hier nicht eine feste Anzahl von Tokens ausgew\u00e4hlt, sondern die Tokens werden solange ausgew\u00e4hlt, bis die Summe der Wahrscheinlichkeiten der ausgew\u00e4hlten Tokens einen bestimmten Wert \u00fcberschreitet. Dieser Wert wird als p bezeichnet und ist ein Hyperparameter des Modells.</p> </li> <li> <p>Temperature: Dieses Verfahren wird verwendet, um die Wahrscheinlichkeiten der Tokens zu ver\u00e4ndern. Die Wahrscheinlichkeiten werden durch die Temperatur T geteilt. Je h\u00f6her die Temperatur, desto h\u00f6her ist die Wahrscheinlichkeit, dass ein Token mit einer niedrigen Wahrscheinlichkeit ausgew\u00e4hlt wird. Bei einer niedrigen Temperatur werden die Wahrscheinlichkeiten der Tokens erh\u00f6ht, die bereits eine hohe Wahrscheinlichkeit haben. Die Formell hierf\u00fcr lautet: \\(\\text{softmax}(z_i) = \\frac{e^{z_i/T}}{\\sum_j e^{z_j/T}}\\)</p> </li> <li> <p>Greedy-Verfahren: Dieses Verfahren w\u00e4hlt immer das Token mit der h\u00f6chsten Wahrscheinlichkeit aus.</p> </li> <li> <p>Beam Search: Dieses Verfahren w\u00e4hlt die k wahrscheinlichsten Tokens aus und erzeugt f\u00fcr jedes Token eine neue Sequenz. Diese Sequenzen werden dann wiederum durch die Wahrscheinlichkeiten der n\u00e4chsten Tokens erweitert. Dieser Vorgang wird solange wiederholt, bis die Sequenzen eine bestimmte L\u00e4nge - beam size - erreicht haben. Die Sequenz mit der h\u00f6chsten Wahrscheinlichkeit wird dann als Ausgabe verwendet.</p> </li> </ol> <p>Aktuell verwendet OpenAI das nucleus-Verfahren und das Temperaturverfahren, um die Textgenerierung zu verbessern.</p>"},{"location":"Themen/LLMs/#252-fine-tuning","title":"2.5.2 Fine-Tuning","text":"<p>Im Fine-Tuning muss das Modelle verschiedene Aufgaben erkennen k\u00f6nnen und diese dann l\u00f6sen. Daf\u00fcr wird das Modell mit einem Datensatz trainiert, der aus einer gro\u00dfen Menge von Texten besteht, die mit den jeweiligen Aufgaben verkn\u00fcpft sind. Die Aufgaben k\u00f6nnen beispielsweise darin bestehen, dass das Modell Texte vervollst\u00e4ndigen, Fragen beantworten oder Texte klassifizieren muss. Foglich gibt es ein Label \\(y\\) und ein zugeh\u00f6rige Sequenz an Tokens \\(x^1, \\dots , x^m\\). Um die Wahrscheinlichkeit f\u00fcr das Label \\(y\\) zu berechnen, werden die Aktivierungen des Modelles und die Ausgabematrix ben\u00f6tigt.</p> \\[ P(y|x^1, \\dots , x^m) = \\text{softmax}(h_l^mW_y) \\] <p>Somit ergibt sich ein neues Ziel, das optimiert werden muss, um die Parameter optimal einzustellen: \\(L_2(D) = \\sum_{(x,y) \\in D} \\log P(y|x^1, \\dots , x^m)\\). Die Optimierung erfolgt durch das supervised Learning mit dem Ziel, die Wahrscheinlichkeit des Labels \\(y\\) zu maximieren.</p> <p>Insgesamt kann man das pre-trained Sprachmodell beim fine-tuning mitverwenden, damit die Generalisierung verbessert wird und die Konvergenz beschleunigt wird.</p> \\[ L_3(D) = L_2(D) + \\lambda L_1(D) \\]"},{"location":"Themen/LLMs/#253-reward-model","title":"2.5.3 Reward Model","text":"<p>Das Reward Model basiert auf dem Likert-Scale, der verwendet wird, um die menschlichen Bewertungen der Plausibilit\u00e4t der GPT-Ausgaben zu erfassen. Hierbei werden die generierten Texte von einem Menschen auf einer Skala von 1 bis 7 bewertet. Diese Skala erm\u00f6glicht es, die Qualit\u00e4t und die Plausibilit\u00e4t der Texte abzubilden, wobei eine h\u00f6here Zahl eine bessere Bewertung darstellt. Zus\u00e4tzlich zur Likert-Skala wird der Mensch auch dazu aufgefordert, ja-nein Fragen zu beantworten, um zu erkl\u00e4ren, warum er die bestimmte Bewertung abgegeben hat. Dieses zus\u00e4tzliche Feedback erm\u00f6glicht es, Einblicke in die Entscheidungsfindung des Bewerters zu erhalten und zu verstehen, welche spezifischen Aspekte des Textes zur Bewertung beigetragen haben. Auf Grundlage dieser menschlichen Bewertungen entsteht ein umfangreicher Datensatz, der verwendet wird, um ein kleineres GPT-Modell zu trainieren. Dieses kleinere Modell besitzt in der Regel rund 6 Milliarden Parameter und wird speziell f\u00fcr die Aufgabe der Textbewertung eingesetzt. Das trainierte kleinere GPT-Modell wird anschlie\u00dfend eingesetzt, um die Ausgaben des gro\u00dfen GPT-Modells zu bewerten. Es analysiert die generierten Texte und weist ihnen eine Bewertung zu, die auf den erlernten Kriterien basiert. Diese Bewertung dient dann als R\u00fcckmeldung f\u00fcr das gro\u00dfe GPT-Modell, um seine Textgenerierung kontinuierlich zu verbessern. Indem das kleinere GPT-Modell die Textausgaben des gro\u00dfen Modells bewertet, wird ein iterativer Feedback-Loop geschaffen. Das gro\u00dfe Modell kann durch das erhaltene Feedback lernen und seine Textgenerierungsf\u00e4higkeiten entsprechend optimieren, um qualitativ hochwertigere und plausiblere Texte zu erzeugen.</p> \\[ loss(\\theta) = - \\frac{1}{\\binom{K}{2}} E_{(x, y_w, y_l) \\sim D}[\\log(\\sigma(r_{\\theta}(x, y_w) - r_{\\theta}(x, y_l)))] \\] <ul> <li>\\(D\\): Datensatz mit den menschlichen Bewertungen</li> <li>\\(K\\): Anzahl der Antworten von GPT-Modell</li> <li>\\(x\\): Eingabesquenz</li> <li>\\(y_w\\): pr\u00e4fferierte Ausgabe von GPT</li> <li>\\(y_l\\): Ausgabe von GPT</li> <li>\\(r_{\\theta}\\): Skalar zum Reward-Modell zu den Parametern \\(\\theta\\)</li> <li>\\(\\sigma\\): Sigmoid-Funktion</li> </ul> <p>Die Hauptidee ist, wenn die pr\u00e4ferierte Ausgabe \\(y_w\\) eine h\u00f6here Bewertung als die Ausgabe \\(y_l\\) erh\u00e4lt, dann wird der Verlust minimiert. Wenn die Ausgabe \\(y_l\\) eine h\u00f6here Bewertung als die pr\u00e4ferierte Ausgabe \\(y_w\\) erh\u00e4lt, dann wird der Verlust maximiert. Die Sigmoid-Funktion wird verwendet, um die Wahrscheinlichkeit zu berechnen, dass die pr\u00e4ferierte Ausgabe \\(y_w\\) eine h\u00f6here Bewertung als die Ausgabe \\(y_l\\) erh\u00e4lt. Die Sigmoid-Funktion ist definiert als \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\). Die Sigmoid-Funktion ist eine monoton steigende Funktion, die Werte zwischen 0 und 1 zur\u00fcckgibt. Wenn die Differenz zwischen den Bewertungen \\(r_{\\theta}(x, y_w)\\) und \\(r_{\\theta}(x, y_l)\\) gro\u00df ist, dann ist die Wahrscheinlichkeit, dass die pr\u00e4ferierte Ausgabe \\(y_w\\) eine h\u00f6here Bewertung als die Ausgabe \\(y_l\\) erh\u00e4lt, nahe 1. Wenn die Differenz zwischen den Bewertungen \\(r_{\\theta}(x, y_w)\\) und \\(r_{\\theta}(x, y_l)\\) klein ist, dann ist die Wahrscheinlichkeit, dass die pr\u00e4ferierte Ausgabe \\(y_w\\) eine h\u00f6here Bewertung als die Ausgabe \\(y_l\\) erh\u00e4lt, nahe 0. Der Logarithmus steigert das Verhalten der Sigmoidfunktion ins negative und erm\u00f6glicht die Berechnung des Verlustes, da er f\u00fcr kleine Werte nahe 0 nach \\(- \\infty\\) anstrebt und bei 1 seine Nullstelle hat.</p> <p> </p> Fig. Reward Model with Sigmoid &amp; Logarithm"},{"location":"Themen/LLMs/#254-reinforcement-learning","title":"2.5.4 Reinforcement Learning","text":"Fig. Reinforcement Learning  <p>Reinforcement Learning (RL) ist eine Methode des maschinellen Lernens, bei der ein Agent in einer Umgebung agiert und durch Interaktion mit dieser Umgebung lernen kann, welche Aktionen zu belohnenden Ergebnissen f\u00fchren. Das Ziel des RL besteht darin, eine optimale Handlungsstrategie zu erlernen, um maximale Belohnungen zu erhalten. Der RL-Prozess besteht aus mehreren Iterationen, in denen der Agent versucht, seine Handlungsstrategie zu verbessern. Dies geschieht durch das Lernen aus Erfahrung, indem der Agent wiederholt Aktionen ausf\u00fchrt, den Zustand der Umgebung wahrnimmt und Belohnungen erh\u00e4lt. Der Agent verwendet dann Algorithmen des maschinellen Lernens, um eine Strategie zu entwickeln, die die erwarteten Belohnungen maximiert.</p> <p>Dieser Prozess baut auf folgenden Gleichungen auf:</p> <p>Markov Decision Process (MDP):</p> Gleichungen Symbole \\(M = (S, A, P, R, \\gamma)\\) \\(S\\): Zustandsraum Markov Chain: \\(P_a(s, s')= \\frac{P(s', s, a)}{P(s, a)}\\) \\(A\\): Aktionen \\(R: (S \u00d7 A)\\) \\(P\\): \u00dcbergangsfunktion \\(\\pi: S \\to A\\) \\(R\\): Belohnungsfunktion \\(E_{\\pi, p}[\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, \\pi(s_t))]\\) \\(\\gamma\\): Diskontierungsfaktor \\(0 \\le \\gamma \\le 1\\) \\(\\pi\\): Strategie des Agenten <p>Wertefunktion:</p> <p>Kalkuliert den erwarteten kumulativen Gewinn einer Aktion in einem Zustand</p> Gleichungen Symbole \\(V^{\\pi}(s_t) = E_{\\pi, p}[\\sum_{k=0}^{\\infty} \\gamma^k R(s_{t+k}, \\pi(s_{t+k}))]\\) \\(V^{\\pi}(s_t)\\): Wertefunktion \\(Q^{\\pi}(s_t, a_t) = R(s_t, a_t) + V^{\\pi}(s_t)_{k=1}\\) \\(Q^{\\pi}(s_t, a_t)\\): Aktionenwertefunktion <p>Bellman Gleichung:</p> Gleichungen Symbole \\(V^{\\pi}(s_t) = E_{\\pi, p}[R(s_t, \\pi(s_t)) + \\gamma V^{\\pi}(s_{t+1})]\\) \\(V^{\\pi}(s_t)\\): Wertefunktion \\(Q^{\\pi}(s_t, a_t) = E_{\\pi, p}[R(s_t, a_t) + \\gamma Q^{\\pi}(s_{t+1}, \\pi(s_{t+1}))]\\) \\(Q^{\\pi}(s_t, a_t)\\): Aktionenwertefunktion \\(\\pi^*(s) = \\text{argmax}_{a \\in A}(\\max_{\\pi}Q^{\\pi}(s, a))\\) \\(\\pi^*\\): otimale Strategie des Agenten <p>Policy:</p> <p>Die Policy ist die Strategie des Agenten, die angibt, welche Aktionen in einem bestimmten Zustand ausgef\u00fchrt werden sollen. Die Policy kann deterministisch oder stochastisch sein.</p> Gleichungen Symbole \\(\\pi_{k+1}(s) = \\text{argmax}_aQ^{\\pi_k}(s, a)\\) \\(\\pi(s_t)\\): Policy <p>Aus diesen vier Grundgleichungen entstehen folgende Algorithmen:</p> <ul> <li> <p>Die Hauptidee des Q-Learning besteht darin, mithilfe einer Q-Funktion die optimale Handlungsstrategie eines Agenten in einer Umgebung zu erlernen. Die Q-Funktion gibt den erwarteten Nutzen (Q-Wert) einer Aktion in einem gegebenen Zustand an. Durch das Lernen und Aktualisieren der Q-Werte w\u00e4hrend des Lernprozesses kann der Agent die beste Aktion in jedem Zustand ausw\u00e4hlen und so die Gesamtbelohnung maximieren.</p> </li> <li> <p>Deep Q - Learning (DQL) ist eine Erweiterung des klassischen Q-Learning-Algorithmus, der tiefe neuronale Netzwerke verwendet, um komplexe Zustandsr\u00e4ume zu modellieren und eine optimale Handlungsstrategie zu erlernen. Durch die Integration von Deep Learning-Techniken erm\u00f6glicht DQL den Agenten, in hochdimensionalen Umgebungen mit gro\u00dfen Zustands- und Aktionsr\u00e4umen effektiv zu operieren.</p> </li> <li> <p>REINFORCE ist ein Algorithmus des Reinforcement Learning, der zur L\u00f6sung von Aufgaben des verst\u00e4rkenden Lernens eingesetzt wird. Er basiert auf der Idee, Richtlinien (Policies) zu erlernen, die einem Agenten erm\u00f6glichen, eine optimale Handlungsstrategie in einer gegebenen Umgebung zu entwickeln. Der REINFORCE-Algorithmus verwendet Monte Carlo-Sampling, um die Richtlinie zu verbessern, indem er die Gesamtbelohnung der durchgef\u00fchrten Aktionen maximiert.</p> </li> <li> <p>Proximal Policy Optimization (PPO) wird in den GPT-Nachfolgern verwendet. PPO zielt darauf ab, die Vorz\u00fcge von Policy Gradient-Methoden und Trust Region Policy Optimization (TRPO) zu kombinieren, um ein effizientes und stabiles Lernen zu erm\u00f6glichen und bestm\u00f6gliche Policies f\u00fcr den Agenten zu finden.</p> </li> </ul> <p>Allgemein ist PPO eine stochastic gradient ascent Methode, um das bestm\u00f6gliche Ergebnis zu erreichen. Dabei wird die optimale Policy mit einem Gradientenverfahren gesucht, indem eine Policy zu einem Zeitpunkt \\(t\\) mit der Vorteilsfunktion an dieser Stelle verrechnet wird. Die Standardformel hierf\u00fcr ist:</p> \\[ \\hat{g} = \\frac{1}{T} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) A^{\\pi_{\\theta}}(s_t, a_t) \\] <p>Um die Policy zu verbessern, wird ein sogenanntes surrogate objective verwendet. Dieses ist eine Funktion, die die Policy verbessert, ohne sie zu stark zu ver\u00e4ndern. Es basiert auf dem Ansatz der trust region, der die \u00c4nderung der Policy begrenzt, indem er die Divergenz zwischen der alten und der neuen Policy begrenzt. Dies wird erreicht, indem die Ver\u00e4nderungen der Policy mit einem Faktor \\(\\epsilon\\) geklippt werden. Die Verlustfunktion, die f\u00fcr dieses Ziel verwendet wird, lautet:</p> \\[ L^{CLIP}(\\theta) = \\hat{E}_t [min(r_t(\\theta) \\hat{A}_t, clip(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t)]; \\] \\[ r_t(\\theta)=\\frac{\\pi_{\\theta}(a_t, s_t)}{\\pi_{\\theta_{old}}(a_t, s_t)} \\] <p>Hierbei steht \\(\\theta\\) f\u00fcr die Parameter der Policy, \\(E_t\\) repr\u00e4sentiert die erwartete Wertung \u00fcber den Zeitpunkt \\(t\\), \\(r_t(\\theta)\\) ist das Verh\u00e4ltnis zwischen den Wahrscheinlichkeiten der aktuellen Policy und der alten Policy f\u00fcr eine bestimmte Aktion im Zustand \\(s_t\\), \\(A_t\\) ist der Vorteil (Advantage) der Aktion und clip ist eine Funktion, die den Wert \\(r_t(\\theta)\\) innerhalb eines bestimmten Intervalls einschr\u00e4nkt.</p> <p>Die Verlustfunktion \\(L_clip(\\theta)\\) vergleicht die Aktionen der aktuellen Policy mit denen der alten Policy und ber\u00fccksichtigt dabei den Vorteil der Aktionen. Sie sorgt daf\u00fcr, dass die \u00c4nderungen der Policy begrenzt bleiben und die Divergenz zwischen der alten und der neuen Policy kontrolliert wird. Die Klippfunktion im Ausdruck \\(\\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon)\\) stellt sicher, dass das Verh\u00e4ltnis \\(r_t(\\theta)\\) innerhalb des Intervalls \\([1 - \\epsilon, 1 + \\epsilon]\\) bleibt.</p> <p>Durch die Verwendung dieses surrogate objectives in Kombination mit der trust region wird sichergestellt, dass die Policy allm\u00e4hlich verbessert wird, ohne dass sie zu stark von der alten Policy abweicht. Dies tr\u00e4gt zur Stabilit\u00e4t des Lernprozesses bei und erm\u00f6glicht eine kontrollierte Anpassung der Policy, um bessere Ergebnisse zu erzielen.</p> <p>Bei der PPO-Funktion von GPT wird die Verlustfunktion \\(L^{CLIP}(\\theta)\\) mit einem Faktor \\(\\lambda\\) gewichtet, um die Ver\u00e4nderung der Policy zu begrenzen. Die PPO - Funktion lautet:</p> \\[ \\text{objective}(\\phi) = E_{(x, y) \\sim D_{\\phi}^{RL}} [r_{\\theta}(x, y) - \\beta \\log(\\pi_{\\phi}^{RL}(y | x) / \\pi^{SFT}(y | x))] + \\gamma * E_{x \\sim D_{pretrain}} [ log(\\pi_{\\phi}^{RL}(x))] \\] <ul> <li> <p>\\(E\\): Erwartungswert</p> </li> <li> <p>\\(x, y\\): Eingabe- und Ausgabedatenpaare</p> </li> <li> <p>\\(D_{\\phi}^{RL}\\): Datenverteilung, die durch die RL-Policy \\(\\pi^{RL}\\) bestimmt wird</p> </li> <li> <p>\\(r_{\\theta}(x, y)\\): R\u00fcckgabewert (Reward) f\u00fcr das Datenpaar \\((x, y)\\) unter den aktuellen Parametern \\(\\theta\\)</p> </li> <li> <p>\\(\\beta\\): Ein Gewichtungsfaktor f\u00fcr die Regularisierung des Rewards</p> </li> <li> <p>\\(\\pi_{\\phi}^{RL}(y | x)\\): Wahrscheinlichkeit der Ausgabe y gegeben der Eingabe x unter der RL-Policy \\(\\pi^{RL}\\) mit den Parametern \\(\\phi\\)</p> </li> <li> <p>\\(\\pi^{SFT}(y | x)\\): Wahrscheinlichkeit der Ausgabe y gegeben der Eingabe x von dem fine-tuning-Modell</p> </li> <li> <p>\\(\\gamma\\): Ein Gewichtungsfaktor f\u00fcr den Pretraining-Term (normal \\(\\gamma=0\\) f\u00fcr PPO)</p> </li> <li> <p>\\(D{pretrain}\\): Datenverteilung f\u00fcr das Pretraining</p> </li> <li> <p>\\(\\pi_{\\phi}^{RL}(x)\\): Wahrscheinlichkeit der Eingabe x unter der RL-Policy \\(\\pi^{RL}\\) mit den Parametern \\(\\phi\\)</p> </li> </ul> <p> </p> Fig. GPT full learning cycle"},{"location":"Themen/LLMs/#26-meta-learning","title":"2.6 Meta-Learning","text":"<p>Meta-Learning ist ein Ansatz im maschinellen Lernen, bei dem Modelle trainiert werden, um aus Erfahrungen zu lernen und dieses Wissen auf neue, \u00e4hnliche Aufgaben anzuwenden. Es erm\u00f6glicht schnelles und effizientes Lernen mit begrenzten Trainingsdaten und Anpassung an neue Situationen. Es gibt viel versschiedene Arten von Meta-Learning, wie z.B. Zero-Shot Learning, One-Shot Learning und Few-Shot Learning. Andere Ans\u00e4tze beinhalten einen Meta Learner, der schw\u00e4chere Modelle rauswirft und die Anpassungsf\u00e4higkeiten der restlichen Modelle zu verbessern. Im folgenden wird auf die shot-learning Ans\u00e4tze eingegangen.</p> <p>Ziel der Shot-Learning Methoden ist es das Modell robuster gegen Unbekanntes machen, indem neue Klassen, Beziehungen oder Embeddings hinzugef\u00fcgt werden. Das Modell soll in der Lage sein, neue Klassen zu erkennen, die es noch nie gesehen hat und mit ihnen umgehenzu k\u00f6nnen. Es werden keine Parameterupdates durchgef\u00fchrt w\u00e4hrend der Inferenzzeit (Vorhersagen).</p>"},{"location":"Themen/LLMs/#261-zero-shot-learning","title":"2.6.1 Zero-Shot Learning","text":"Fig. Zero-Shot Learning  <p>Das Zero-Shot Learning wird als unfair angesehen, da der Transformer nur durch die Aufgabenbeschreibung f\u00fcr einen Input die richtige Antwort vorhersagen muss. Folglich muss das Modell die Aufgabenbeschreibung verstehen und die richtige Antwort vorhersagen.</p>"},{"location":"Themen/LLMs/#262-one-shot-learning","title":"2.6.2 One-Shot Learning","text":"Fig. One-Shot Learning  <p>Beim One-Shot Learning wird das Modell mit einem zus\u00e4tzlichen Beispiel zu der Zero-Shot Methode trainiert und muss dann die richtige Antwort vorhersagen. Das Modell soll in der Lage sein, die Beziehung zwischen Beispiel, Aufgabe und der Antwort zu verstehen.</p>"},{"location":"Themen/LLMs/#263-few-shot-learning","title":"2.6.3 Few-Shot Learning","text":"Fig. Few-Shot Learning  <p>Das Few-Shot Learning ist eine Erweiterung des One-Shot Learning, indem mehrere Beispiele zu der Aufgabe hinzugef\u00fcgt werden. Es wird vorgeschlage zwischen 10 und 100 Beispiele zu verwenden.</p>"},{"location":"Themen/LLMs/#27-benchmarking","title":"2.7 Benchmarking","text":"Fig. Benchmarks"},{"location":"Themen/LLMs/#271-glue","title":"2.7.1 GLUE","text":"<p>GLUE steht f\u00fcr General Language Understanding Evaluation und ist ein Benchmark-Datensatz f\u00fcr die Evaluation von Modellen zur Sprachverarbeitung und zum Textverst\u00e4ndnis. Der Datensatz besteht aus 9 verschiedenen Aufgaben, die jeweils eine andere Art von Sprachverst\u00e4ndnis erfordern. Die Aufgaben sind wie folgt:</p> <ol> <li> <p>CoLA - Corpus of Linguistic Acceptability: Aufgabe zur bin\u00e4ren Klassifikation, bei der die Akzeptanz grammatischer S\u00e4tze bestimmt werden soll.</p> </li> <li> <p>SST-2 - Stanford Sentiment Treebank: Aufgabe zur bin\u00e4ren Klassifikation, bei der die Sentiment-Klassifizierung von S\u00e4tzen gefordert wird.</p> </li> <li> <p>MRPC - Microsoft Research Paraphrase Corpus: Aufgabe zur Bestimmung der semantischen \u00c4hnlichkeit zwischen S\u00e4tzen durch die Erkennung von Paraphrasen.</p> </li> <li> <p>QQP - Quora Question Pairs: Aufgabe zur Erkennung von Paraphrasen, bei der die \u00c4hnlichkeit von Fragepaaren beurteilt werden soll.</p> </li> <li> <p>STS-B - Semantic Textual Similarity Benchmark: Aufgabe zur Berechnung der semantischen \u00c4hnlichkeit zwischen S\u00e4tzen anhand von kontinuierlichen Wertungen.</p> </li> <li> <p>MNLI - Multi-Genre Natural Language Inference: Aufgabe zur \u00dcberpr\u00fcfung der Textual Entailment-F\u00e4higkeiten, bei der die logische Beziehung zwischen Satzpaaren bestimmt werden soll.</p> </li> <li> <p>QNLI - Question Natural Language Inference: Aufgabe zur Textual Entailment-Bestimmung basierend auf Fragen und Antworttexten.</p> </li> <li> <p>RTE - Recognizing Textual Entailment: Aufgabe zur Bestimmung der Textual Entailment-Beziehung zwischen S\u00e4tzen.</p> </li> <li> <p>WNLI - Winograd Schema Challenge: Aufgabe zur maschinellen Resolution von referenziellen Ausdr\u00fccken in einer Frage-Antwort-Form.</p> </li> </ol>"},{"location":"Themen/LLMs/#272-superglue","title":"2.7.2 SuperGLUE","text":"<p>SuperGLUE steht f\u00fcr \"Super General Language Understanding Evaluation\" und ist eine Weiterentwicklung des GLUE-Benchmarks. W\u00e4hrend GLUE neun Aufgaben zur Bewertung von Modellen zur Sprachverarbeitung umfasst, erweitert SuperGLUE den Benchmark um zus\u00e4tzliche und anspruchsvollere Aufgaben, um die Leistung von Modellen f\u00fcr Sprachverst\u00e4ndnis und Textverarbeitung weiter zu testen. SuperGLUE umfasst insgesamt acht Aufgaben, die komplexere Sprachverst\u00e4ndnisf\u00e4higkeiten erfordern. Die Aufgaben sind:</p> <ol> <li> <p>BoolQ - Boolean Questions: Aufgabe zur Beantwortung von Ja/Nein-Fragen auf der Grundlage eines gegebenen Kontexts.</p> </li> <li> <p>CB - CommitmentBank: Aufgabe zur Bestimmung der semantischen Implikation zwischen zwei S\u00e4tzen.</p> </li> <li> <p>COPA - Choice of Plausible Alternatives: Aufgabe zur Kausalit\u00e4tsbeurteilung, bei der aus zwei m\u00f6glichen Ursachen die plausiblere ausgew\u00e4hlt werden soll.</p> </li> <li> <p>MultiRC - Multi-Sentence Reading Comprehension: Aufgabe zum Leseverst\u00e4ndnis mehrerer S\u00e4tze, bei der mehrere Fragen zu einem gegebenen Text beantwortet werden m\u00fcssen.</p> </li> <li> <p>ReCoRD - Reading Comprehension with Commonsense Reasoning Dataset: Aufgabe zum Leseverst\u00e4ndnis mit \"Commonsense\" -Reasoning, bei der fehlende Informationen in einem gegebenen Text erg\u00e4nzt werden m\u00fcssen.</p> </li> <li> <p>RTE - Recognizing Textual Entailment: \u00c4hnlich wie im GLUE-Benchmark ist diese Aufgabe darauf ausgerichtet, die Textual Entailment-Beziehung zwischen S\u00e4tzen zu bestimmen.</p> </li> <li> <p>WiC - Word-in-Context: Aufgabe zur Beurteilung der Bedeutung eines Wortes in verschiedenen Kontexten.</p> </li> <li> <p>WSC - Winograd Schema Challenge: \u00c4hnlich wie im GLUE-Benchmark ist dies eine Aufgabe zur maschinellen Resolution von referenziellen Ausdr\u00fccken in einer Frage-Antwort-Form.</p> </li> </ol>"},{"location":"Themen/LLMs/#273-squad","title":"2.7.3 SQuAD","text":"<p>SQuAD steht f\u00fcr \"Stanford Question Answering Dataset\" und ist ein bekannter Benchmark-Datensatz f\u00fcr maschinelles Lesen und Frage-Antwort-Aufgaben. Der SQuAD-Datensatz besteht aus einer umfangreichen Sammlung von Frage-Antwort-Paaren, die auf Textpassagen aus Wikipedia-Artikeln basieren. Die Besonderheit von SQuAD besteht darin, dass die Fragen im Kontext des Textes gestellt werden, d.h., die Antworten auf die Fragen k\u00f6nnen direkt aus dem gegebenen Text extrahiert werden, ohne externe Quellen zu konsultieren. Dies macht SQuAD zu einer herausfordernden Aufgabe f\u00fcr maschinelles Lesen und Textverst\u00e4ndnis.</p>"},{"location":"Themen/LLMs/#274-modelle","title":"2.7.4 Modelle","text":"Fig. Modelle"},{"location":"Themen/LLMs/#3-anwendungen","title":"3 Anwendungen","text":""},{"location":"Themen/LLMs/#31-limitationen-heutiger-large-language-models","title":"3.1 Limitationen heutiger Large Language Models","text":"<p>Mit dem Aufstieg von Large Language Model wie OpenAI's ChatGPT erleben wir eine Revolution im Bereich der automatischen Generierung von Inhalten. Aber wie mit jeder Technologie haben auch solch m\u00e4chtige Modelle Probleme und Limitationen. In diesem Gebiet k\u00f6nnen wir 2 Hauptprobleme ausmachen.</p> <p>Das erste bezieht sich auf das sogenannte \"Knowledge Cutoff\". Wer kennt diese Situation nicht? Man stellt ChatGPT eine Frage und als Antwort bekommt man dann im ersten Abschnitt (auf Deutsch \u00fcbersetzt): \"Nach meinem derzeitigen Wissenstand bis September 2021...\". Dieses Problem macht es uns extrem schwer neue Ereignisse zu erfassen, auszuwerten und als Transferwissen in unsere erstellten Inhalte einzubinden.</p> <p>Im zweiten Problem sehen wir, dass sich dies allerdings nicht nur auf Inhalte, die nach dem letzten Trainingszyklus erstellt wurden, bezieht, sondern auch auf Daten die gar nicht erst in das Trainingsset mit eingeflossen sind. Gehen wir davon aus, dass man zum Beispiel Fragen zu privaten Dokumenten stellen m\u00f6chte, um entweder einen \u00dcberblick zu bekommen oder deren Richtigkeit zu pr\u00fcfen. Vielleicht sind Sie Teil einer Firma die mit ihrem neusten Produkt eine Werbekampagne starten m\u00f6chte und ChatGPT f\u00fcr die beste Ausf\u00fchrung das Fachwissen zu diesem ben\u00f6tigt. Oder Sie m\u00f6chten auf bestimmte E-Mails automatisch antworten und ausgemachte Termine in Ihren Kalender \u00fcbertragen.</p> <p>Die denkbaren M\u00f6glichkeiten auf diesem Gebiet sind fast unbegrenzt, dennoch ist dies nicht mit der Implementation des Standardprodukts ChatGPT machbar, oder vielleicht doch?...</p>"},{"location":"Themen/LLMs/#32-die-losung-langchain","title":"3.2 Die L\u00f6sung: LangChain","text":"<p>Die beschriebenen Limitationen scheinen ein sehr komplexes Problem aufzuwerfen, aber wie so oft wird dadurch nur die T\u00fcr f\u00fcr innovative L\u00f6sungen aufgemacht. Das ist der Punkt an dem LangChain ins Spiel kommt.</p> <p>Man kann es sich wie ein Schweizer Taschenmesser vorstellen, dass nur darauf ausgelegt ist, das bauen von Applikationen mit Large Language Models zu vereinfachen. Dabei agiert es wie eine Art Mittelschicht zwischen LLM und anderen Tools.</p> <p>Mit LangChain ist es uns m\u00f6glich aufgrund von Nutzeranfragen aktuelle Daten zu googeln, private Dateien auszuwerten, mathematische Formeln zu berechnen, Code zu erstellen und auszuf\u00fchren und noch vieles mehr. Diese genannten Punkte sind schon sehr beindruckend, aber das mit Abstand m\u00e4chtigste Feature, und auch das simpelste, ist dann man die Ausgabe von einer Antwort als Eingabe f\u00fcr den n\u00e4chsten benutzen kann. Das bedeutet, dass ChatGPT sich selbst Feedback zu den erledigten Aufgaben geben kann und seine aktuelle L\u00f6sung darauf basierend verbessern kann. Au\u00dferdem kann man selbst festlegen, welche Teile der vorherigen Aufgaben in die neue Eingabe mit eingehen sollen. Eine solch simple Technik is wirklich faszinierend und ihre M\u00f6glichkeiten k\u00f6nnen wir am Projekt AutoGPT bewundern.</p> <p>Mit diesem Gedanken wollen wir jetzt ein Szenario definieren, welches eine Einf\u00fchrung in die erw\u00e4hnten M\u00f6glichkeiten gibt. Ein gutes Beispiel ist die Erstellung eines Podcast Skripts welche die neusten Entwicklungen im Bereich der Large Language Models beleuchten soll. Au\u00dferdem wollen wir die Anforderungen, welche in den Vorlesungsskript f\u00fcr diese Aufgabe gegeben worden sind miteinbeziehen.</p> <p>Diese Aufgabe k\u00f6nnen wir somit in 3 Schritte unterteilen:</p> <ol> <li>Die Anforderungen f\u00fcr den Podcast auf dem Vorlesungsskript extrahieren</li> <li>Nachforschungen zu den aktuellsten Themen in diesem Gebiet recherchieren</li> <li>Mithilfe der Anforderungen und Recherchen unser Podcast Skript generieren</li> </ol>"},{"location":"Themen/LLMs/#33-code-demo-der-allwissende-podcast-schreiber","title":"3.3 Code Demo: Der allwissende Podcast Schreiber","text":"<p>Um ein m\u00f6glichst gutes Ergebnis zu erzielen benutzen wir zu erstellen des Skripts ChatGPT, Model GPT3.5-turbo. Um Zugriff auf Antworten des Modells in LangChain zu erreichen m\u00fcssen wir einen sogenannten API Token erstellen und angeben.</p> <p>APIs sind h\u00e4ufig zahlungspflichtig. Die genauen Kosten sollte man auf der OpenAI Website recherchieren.</p> <pre><code>llm = OpenAI(openai_api_key=openai_api_key, temperature=0.9)\ntopic = 'Large Language Models'\n</code></pre> <p>Damit k\u00f6nnen wir unser <code>llm</code> initialisieren und geben mit dem Temperaturwert an, dass wir kreative Antworten erhalten wollen. Au\u00dferdem geben wir an, zu welchem Thema ein Podcast erstellt werden soll.</p> <pre><code># document loaders\nloader = TextLoader(file_path='../documents/VL1-1-processed-eng.txt', encoding='utf-8')\ndocument_content = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=600, chunk_overlap=0)\nsplit_content = text_splitter.split_documents(document_content)\n\n# create embeddings\nembeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\nembeddings_search = Chroma.from_documents(split_content, embeddings)\nembeddings_search\n</code></pre> <p>Der n\u00e4chste Schritt l\u00e4dt das Vorlesungsskript und die Embeddings. Diese werden dann auf das Dokument angewendet um eine Suche mit ChatGPT m\u00f6glich zu machen.</p> <pre><code># create prompt template to get usable results\nprompt_template_text_document = \"\"\"\nInstruction:\n- Use the following pieces of context to answer the question at the end.\n- If you don't know the answer output: NULL\n- Just answer the question without providing any additional information\n\nContext:\n{context}\n\nQuestion:\n{question}\n\nAnswer:\n\"\"\"\n\nprompt_template_documents = PromptTemplate(template=prompt_template_text_document, input_variables=['context', 'question'])\nchain_type_kwargs = {'prompt': prompt_template_documents}\n\nqa = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=embeddings_search.as_retriever(), chain_type_kwargs=chain_type_kwargs)\n\nquery_topics = f'Which topics should be covered in the podcast about {topic}?'\nres_topics = qa.run(query_topics)\n</code></pre> <p>Um eine effiziente und aussagekr\u00e4ftige Suche zu erstellen ist es wichtig eine gute <code>Prompt Template</code> zu benutzen. Diese Struktur orientiert sich an AutoGPT. Wir definieren unsere Bedinungen, geben den Kontext an, stellen anschlie\u00dfend unsere Frage und speichern die Antwort. Nachdem wir jetzt unsere Teil-Themen herausgefunden haben, wollen wir zu ihnen und dem Haupttehema eine Internetrecherche durchf\u00fchren.</p> <pre><code>tools = load_tools(['serpapi'], llm=llm)\nagent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n\n# do a web research for all topics\nall_topics = [topic, *formatted_topics]\nweb_research = []\n\nfor topic in all_topics:\n    agent_res = agent.run(f\"Task: Do a thorough web research about {topic}. Provide at least 3 sentences of information.\")\n    web_research.append(agent_res)\n</code></pre> <p>F\u00fcr dieses Szenario bietet uns LangChain sehr m\u00e4chtige Konzepte an. Und zwar Tools und Agents. Mit deren Hilfe kann ChatGPT selbst definieren welche Informationen es f\u00fcr die Recherche ben\u00f6tigt und gegebenenfalls wiederholte Suchanfragen f\u00fcr verwandte und Unterthemen stellen um das best m\u00f6gliche Ergebnis zu erzielen.</p> <p>F\u00fcr die Nutzung des 'serpapi' tools wird ein weiterer API Key ben\u00f6tigt. Siehe SerpApi</p> <pre><code>prompt_template_text_script = \"\"\"\nSub Topics:\n{sub_topics}\n\nContext:\n\\\"{context}\\\"\n\nPodcast Participants:\n  - Host\n  - Expert\n\nPrevious Section of the Podcast Script:\n\\\"{previous_section}\\\"\n\nTask:\n  - Your task is to write a podcast script about \\\"{topic}\\\".\n  - The Sub Topics refine the main topic and need to be addressed!\n  - Use your own knowledge and the one provided in Context if you think it fit the topic.\n  - Continue from the previous section and output the new content.\n  - If you think you are done output [END]\n\n\"\"\"\nprompt_template_podcast = PromptTemplate(template=prompt_template_text_script, input_variables=['sub_topics', 'context', 'topic', 'previous_section'])\n\n# generate the podcast script like before just now with the serpapi agent web research\npodcast_chain = LLMChain(llm=llm, prompt=prompt_template_podcast)\n\npodcast_script_web = run_repeated_chain(podcast_chain, sub_topics=sub_topics, context=formatted_web_research, topic=topic, previous_section='')\npodcast_script_web\n</code></pre> <p>Mit den gesammelten Informationen ist es uns jetzt m\u00f6glich den Podcast zu erstellen. Daf\u00fcr definieren wir uns num wieder einen passenden Prompt. Das besondere an diesem ist allerdings, dass er auf der Grundlage des vorherigen Abschnitts des Podcast Skripts den n\u00e4chsten erstellt. Jetzt ist die logische Frage: Warum ist das notwendig? K\u00f6nnen wir uns nicht einfach das ganze Skript auf einmal erstellen lassen? Die traurige Antwort auf diese Frage ist leider nein. Das liegt daran, dass GPT3.5 ein maximales Eingabe- &amp; Ausgabelimit hat. Dieses liegt bei 4096. Wenn wir also einen l\u00e4ngern Podcast erstellen wollen m\u00fcssen wir in einem iterativen Prozess vorgehen. Das macht die selbst definierte Funktion <code>run_repeated_chain</code>. Sie wird so oft ausgef\u00fchrt bis das Stop Wort <code>[END]</code> erreicht wird, oder ein Limit f\u00fcr die maximalen Iterationen (Standardwert: <code>8</code>). Am Ende bekommen wir ein hochwertiges Podcastskript, dessen Erstellung mit LangChain auf triviale Weise m\u00f6glich war. Selbst an diesem einfachen Beispiel k\u00f6nnen wir die Macht eines Toolsets sehen, dass LLMs mit anderen Diensten und Pipelines integriert.</p>"},{"location":"Themen/LLMs/#34-eine-neue-art-des-programmierens-prompt-engineering","title":"3.4 Eine Neue Art des Programmierens: Prompt Engineering","text":"<p>Wie wir an dem Code der Demo sehen konnten ist ein integraler Teil immer wieder aufgetaucht: Die spezifische Syntax der <code>PromptTemplates</code>. Warum ist dies nun so wichtig? Na weil LLMs einen riesigen Wissensschatz in sich bergen, aber man kann diesen nicht auf optimale Art und Weise nutzen, wenn man nicht dazu in der Lage ist auf die richtige Art und Weise Fragen und Anweisungen zu stellen. Um dieses Problem zu l\u00f6sen is das neue Gebiet des Prompt Engineerings entstanden.</p> <p>Im Kontext von LangChain wird diese Technik nochmal wichtiger. Wie wir schon gesehen haben k\u00f6nnen wir uns damit Daten von privaten Dateien und aktuelle Berichte aus dem Internet abholen. Diese m\u00fcssen nun aber in einer definierten und optimierten Weise von ChatGPT ausgewertet werden. Auf Grundlage dessen wird nun eine Aufgabe definiert und eine Richtung vorgegeben. Dies w\u00e4re unm\u00f6glich ohne die richtigen Prompts zu l\u00f6sen und w\u00fcrde verschiedenes, sowie unauswertbares Verhalten hervorrufen. Eine Definition eines Ablaufs w\u00e4re somit nicht tragbar.</p> <p>Wenn wir auf unser Beispiel mit dem Podcast Skript zur\u00fcckgehen sehen wir, dass die Prompts ChatGPT dazu auffordern mehrere Bedingungen zu erf\u00fcllen und definieren, wie sie zu erf\u00fcllen sind. Wir sehen hier immer wieder die klare Struktur, welche mehrere Anforderungen stellt, denn Kontext angibt und eine Hauptaufgabe enth\u00e4lt.</p> <p>Nat\u00fcrlich kann sich je nach Themengebiet und Aufgabenanforderung die Form des Prompt ver\u00e4ndern. Das Gebiet des Prompt Engineering ist noch sehr jung und vieles ist unbekannt. Man kann immer bessere Eingaben mit viel Testen und innovativen Ideen erreichen. Zum Beispiel wird vermutet, dass das Aussehen der Prompts mit der Struktur der Trainingsdaten korreliert. Au\u00dferdem sehen wir, dass die je nach verwendetem LLM die Syntax der Prompts ver\u00e4ndert werden muss um bessere Ergebnisse zu erzielen. Allerdings ist im Moment noch nichts mit Sicherheit zu sagen und dies bleibt ein Gebiet aktiver Forschung.</p> <p>Eines k\u00f6nnen wir jedoch feststellen, und zwar das dieses Gebiet nicht mehr ein bel\u00e4chelt werden sollte. Um das volle Potential von LLMs auszusch\u00f6pfen m\u00fcssen wir wie mit Google Suchanfragen lernen, wie wir sie am besten stellen k\u00f6nnen.</p> <p>Trotzdem m\u00f6chte hier nochmal gesagt sein, dass die Technik des Prompt Engineering nicht die allgemeinen Limitationen von LLMs \u00fcberkommen kann.</p>"},{"location":"Themen/LLMs/#35-open-source-alternativen","title":"3.5 Open Source Alternativen","text":"<p>Wenn man sich die jetzige Lage von LLM Anbietern anschaut zeichnet sich ein klarer Trend ab. Propriet\u00e4re, gigantische Tech-Konzerne wie OpenAI, Microsoft, Google, etc. dominieren den Markt. Vor allem ChatGPT mit den zugrunde liegenden Modellen GPT3.5-turbo und GPT4 sind ihrer Konkurrenz meilenweit voraus.</p> <p>Allerdings sollte man niemals die Macht der Open Source Community untersch\u00e4tzen. Nach dem Leak des Models LLaMA von Meta ist eine neue Welle von Open Source LLMs entstanden. Diese sind zwar nicht so leistungsstark wie ihre propriet\u00e4ren Pendants, aber sie stellen den Anfang einer Gegenbewegung dar.</p> <p>Vor allem sticht das Vicuna-13B Model heraus, welches, obwohl es \"nur\" 13 Milliarden Parameter hat auf 92% der Leistung von GPT3.5-turbo kommt. Dies ist nochmal beindruckender, wenn man bedenkt dass das OpenAI Model 175 Milliarden Parameter besitzt.</p> <p> </p> ChatGPT, Bard, Vicuna13B im Vergleich <p>Allerdings haben die Open Source Varianten gro\u00dfe Probleme zu l\u00f6sen:</p> <ol> <li> <p>Trainings Ressourcen    Wie zu erwarten stellt die Notwendigkeit von exorbitanter Rechenleistung zum Training dieser Modelle ein unglaublich gro\u00dfes Problem f\u00fcr den normalen Programmieren / KI Experten da. Einzelne Personen haben einfach nicht die M\u00f6glichkeiten Modelle mit Milliarden \u00fcber Milliarden Parameter zu trainieren und sind abh\u00e4ngig von guten Willen von Organisationen, was das angeht. Um diese Probleme zu adressieren ist ein sehr interessantes Projekt zum dezentralisierten Training, also jeder Nutzer im Netzwerk tr\u00e4gt einen kleinen Teil dazu bei, aufgetaucht. Wenn Sie mehr wissen wollen geht es hier zum Petals Projekt.</p> </li> <li> <p>Datens\u00e4tze    Bei den Datens\u00e4tzen haben wir ein \u00e4hnliches Problem wie bei den Rechenressourcen. Selbst um die \"kleineren\" Open Source Modell zu trainieren sind schon Unmengen an Daten notwendig. Nat\u00fcrlich sind auch gro\u00dfe \u00f6ffentlich zug\u00e4ngliche Datens\u00e4tze vorhanden, diese sind allerdings nicht in Gr\u00f6\u00dfe und Qualit\u00e4t mit denen der gro\u00dfen Tech-Konzerne zu vergleichen. Findig wie eh und je hat die Community einen cleveren \"Workaround\" gefunden. Und zwar benutzen die Entwickler ChatGPT zur Erstellung von Trainingsdaten. Diese hat aber das Problem, dass das zu trainierend Modell somit nicht besser als sein Trainer werden kann.</p> </li> <li> <p>Imitieren vs. Verstehen    Beim Trainieren von Open Source Modellen mit ChatGPT ist den Entwicklern eine Sache immer wieder aufgefallen. Und zwar dass wir exzellente Antworten auf dagewesene Probleme bekommen, aber sobald Probleme, die eine Art von Transferleistung erfordern gel\u00f6st werden sollen, die Open Source Modelle kl\u00e4glich scheitern. Das wurde dann als Schw\u00e4che im Trainingsprozess festgemacht und neuere Ans\u00e4tze fahren nun eine verbesserte Strategie. Sie lassen sich von ChatGPT den logischen Denkverlauf darlegen und lernen diesen in einem mehrschrittigen Prozess. Es hat sich gezeigt, dass selbst bei kleineren Modellen ein signifikanter Leistungsschub erreicht werden kann.</p> </li> </ol> <p>Zusammenfassend ist es wichtig eine wachsende Open Source Community, sowie performante offene Modelle zu haben. Dies f\u00f6rdert Wettbewerb, Privatsph\u00e4re und das Aufkommen von kreativen Ideen. Diversit\u00e4t f\u00f6rdert Fortschritt und Fortschritt ist der Inbegriff dieses faszinierenden Gebietes.</p>"},{"location":"Themen/LLMs/#4-fazit","title":"4 Fazit","text":"<p>Leistungsstarke Sprachverarbeitung: LLMs wie GPT-4 oder Bard sind \u00e4u\u00dferst leistungsf\u00e4hige Modelle, die eine breite Palette von sprachlichen Aufgaben bew\u00e4ltigen k\u00f6nnen. Sie k\u00f6nnen Texte generieren, Fragen beantworten, \u00dcbersetzungen liefern, Dialoge f\u00fchren und vieles mehr. Ihre F\u00e4higkeiten haben sich in den letzten Jahren erheblich verbessert und sie k\u00f6nnen mittlerweile oft erstaunlich menschen\u00e4hnliche Antworten liefern. Das hat sie zu einem der vielversprechendsten Bereiche der KI-Forschung gemacht und deren Einsatzm\u00f6glichkeiten sind noch lange nicht ausgesch\u00f6pft.</p> <p>Kreative Textgenerierung: LLMs haben gezeigt, dass sie in der Lage sind, kreativen Text zu generieren, wie Gedichte, Geschichten oder Songtexte. Sie k\u00f6nnen auf der Grundlage des ihnen gegebenen Kontextes innovative und unterhaltsame Inhalte erstellen. Dies hat das Interesse von K\u00fcnstlern, Schriftstellern und Kreativen geweckt, die LLMs als Werkzeug zur Inspiration und Unterst\u00fctzung nutzen k\u00f6nnen. Auch Google ist auf diesen Zug aufgesprungen und verfolgt das Ziel, dass Menschen sich zuk\u00fcnftig nur noch f\u00fcr die wichtigsten Aufgaben konzentrieren m\u00fcssen, w\u00e4hrend die Maschinen den Rest erledigen.</p> <p>Herausforderungen der Ethik und Verantwortung: Mit der Macht der LLMs kommen auch ethische und verantwortungsvolle Bedenken. LLMs lernen aus gro\u00dfen Mengen an Textdaten und spiegeln daher die Vorurteile und Verzerrungen wider, die in diesen Daten enthalten sind. Dies kann zu ungewollter Diskriminierung, Fehlinformationen oder der Verbreitung von Hassrede f\u00fchren. Die Forschung und Entwicklung von LLMs erfordert daher eine bewusste Auseinandersetzung mit diesen Fragen und die Implementierung von Mechanismen zur Vermeidung von Missbrauch.</p> <p>Datenschutz und Sicherheit: Der Betrieb von LLMs erfordert gro\u00dfe Mengen an Daten, um gute Leistungen zu erzielen. Dies stellt eine Herausforderung f\u00fcr den Datenschutz dar, insbesondere wenn sensible Informationen in die Modelle eingegeben werden. Der Schutz der Privatsph\u00e4re und die Gew\u00e4hrleistung der Datensicherheit m\u00fcssen bei der Verwendung von LLMs ber\u00fccksichtigt werden, um Missbrauch oder Datenlecks zu vermeiden.</p> <p>Chancen und Potenziale: Trotz der Herausforderungen bieten LLMs auch viele Chancen und Potenziale. Sie k\u00f6nnen beispielsweise in Bildungssystemen eingesetzt werden, um personalisierte Lerninhalte bereitzustellen oder bei der Automatisierung von Kundeninteraktionen in Unternehmen helfen. LLMs k\u00f6nnten auch bei der Bew\u00e4ltigung komplexer Probleme wie der medizinischen Diagnose oder der Entwicklung neuer Materialien unterst\u00fctzen. Dar\u00fcber hinaus k\u00f6nnen LLMs durch ihre beeindruckende F\u00e4higkeit zur Textgenerierung auch kreative Prozesse in der Softwareentwicklung unterst\u00fctzen. Sie k\u00f6nnen Code generieren, der von professionellen Entwicklern weiterverarbeitet werden kann, wodurch diese sich auf den Kern der Aufgabe konzentrieren k\u00f6nnen. Dies stellt nur ein konkretes Beispiel von vielen dar, wie LLMs die Arbeit von Menschen unterst\u00fctzen k\u00f6nnen, ohne sie zu ersetzen.</p> <p>Fazit</p> <p>LLMs sind eine der spannendsten und vielversprechendsten Technologien der letzten Jahre. Sie haben das Potential die Art und Weise wie wir mit Computern interagieren zu revolutionieren. Allerdings sind sie noch nicht perfekt und haben ihre Limitationen. Diese sind aber nicht un\u00fcberwindbar und mit der Zeit werden sie immer besser werden. Es ist wichtig sich mit dieser Technologie auseinanderzusetzen und sie zu verstehen, denn sie wird in Zukunft immer bedeutender werden.</p>"},{"location":"Themen/LLMs/#5-weiterfuhrendes-material","title":"5 Weiterf\u00fchrendes Material","text":""},{"location":"Themen/LLMs/#51-podcast","title":"5.1 Podcast","text":"<p>Hier einfach den Podcast embedden. Der Campus Talk \u2013 Silicon Forest \u2013 Folge 3</p>"},{"location":"Themen/LLMs/#52-talk","title":"5.2 Talk","text":"<p>Hier einfach Youtube oder THD System embedden.</p>"},{"location":"Themen/LLMs/#53-demo","title":"5.3 Demo","text":"<p>Link zum Repository: demo-ki-seminar</p>"},{"location":"Themen/LLMs/#6-literaturliste","title":"6 Literaturliste","text":"<p>Acheampong, Francisca Adoma, Henry Nunoo-Mensah, und Wenyu Chen. \u201eTransformer Models for Text-Based Emotion Detection: A Review of BERT-Based Approaches\u201c. Artificial Intelligence Review 54, Nr. 8 (1. Dezember 2021): 5789\u20135829. https://doi.org/10.1007/s10462-021-09958-2.</p> <p>Alammar, Jay. \u201eThe Illustrated Transformer\u201c. Zugegriffen 18. Juni 2023. http://jalammar.github.io/illustrated-transformer/.</p> <p>Amirhossein Kazemnejad. \u201eTransformer Architecture: The Positional Encoding - Amirhossein Kazemnejad\u2019s Blog\u201c. Zugegriffen 18. Juni 2023. https://kazemnejad.com/blog/transformer_architecture_positional_encoding/.</p> <p>Bojanowski, Piotr, Edouard Grave, Armand Joulin, und Tomas Mikolov. \u201eEnriching Word Vectors with Subword Information\u201c. arXiv, 19. Juni 2017. http://arxiv.org/abs/1607.04606.</p> <p>Brants, Thorsten, Ashok C Popat, Peng Xu, Franz J Och, und Jeffrey Dean. \u201eLarge Language Models in Machine Translation\u201c, o.\u00a0J.</p> <p>Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, u.\u00a0a. \u201eLanguage Models are Few-Shot Learners\u201c. arXiv, 22. Juli 2020. http://arxiv.org/abs/2005.14165.</p> <p>Carlini, Nicholas, Florian Tram\u00e8r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, u.\u00a0a. \u201eExtracting Training Data from Large Language Models\u201c, o.\u00a0J. https://www.usenix.org/system/files/sec21-carlini-extracting.pdf.</p> <p>DeepMind x UCL | Deep Learning Lectures | 6/12 | Sequences and Recurrent Networks, 2020. https://www.youtube.com/watch?v=87kLfzmYBy8.</p> <p>DeepMind x UCL | Deep Learning Lectures | 7/12 | Deep Learning for Natural Language Processing, 2020. https://www.youtube.com/watch?v=8zAP2qWAsKg.</p> <p>Devlin, Jacob, Ming-Wei Chang, Kenton Lee, und Kristina Toutanova. \u201eBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u201c. arXiv, 24. Mai 2019. http://arxiv.org/abs/1810.04805.</p> <p>Domingos, Pedro. \u201eA Few Useful Things to Know about Machine Learning\u201c. Communications of the ACM 55, Nr. 10 (Oktober 2012): 78\u201387. https://doi.org/10.1145/2347736.2347755.</p> <p>Floridi, Luciano. \u201eAI as Agency Without Intelligence: On ChatGPT, Large Language Models, and Other Generative Models\u201c. Philosophy &amp; Technology 36, Nr. 1 (10. M\u00e4rz 2023): 15. https://doi.org/10.1007/s13347-023-00621-y.</p> <p>google. \u201eClassify Text with BERT | Text\u201c. TensorFlow. Zugegriffen 1. April 2023. https://www.tensorflow.org/text/tutorials/classify_text_with_bert.</p> <p>Hassan, Abdalraouf, und Ausif Mahmood. \u201eEfficient Deep Learning Model for Text Classification Based on Recurrent and Convolutional Layers\u201c. In 2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA), 1108\u201313, 2017. https://doi.org/10.1109/ICMLA.2017.00009.</p> <p>Holtzman, Ari, Jan Buys, Li Du, Maxwell Forbes, und Yejin Choi. \u201eThe Curious Case of Neural Text Degeneration\u201c. arXiv, 14. Februar 2020. http://arxiv.org/abs/1904.09751.</p> <p>Kaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, und Dario Amodei. \u201eScaling Laws for Neural Language Models\u201c. arXiv, 22. Januar 2020. http://arxiv.org/abs/2001.08361.</p> <p>Kim, Sang-Bum, Kyoung-Soo Han, Hae-Chang Rim, und Sung-Hyon Myaeng. \u201eSome Effective Techniques for Naive Bayes Text Classification\u201c. Knowledge and Data Engineering, IEEE Transactions on 18 (1. Dezember 2006): 1457\u201366. https://doi.org/10.1109/TKDE.2006.180.</p> <p>lbayad, Maha, Laurent Besacier, und Jakob Verbeek. \u201ePervasive Attention: 2D Convolutional Neural Networks for Sequence-to-Sequence Prediction\u201c. arXiv, 1. November 2018. https://doi.org/10.48550/arXiv.1808.03867.</p> <p>Luitse, Dieuwertje, und Wiebke Denkena. \u201eThe great Transformer: Examining the role of large language models in the political economy of AI\u201c. Big Data &amp; Society 8, Nr. 2 (1. Juli 2021): 20539517211047736. https://doi.org/10.1177/20539517211047734.</p> <p>Liu, Pengfei, Xipeng Qiu, und Xuanjing Huang. \u201eRecurrent Neural Network for Text Classification with Multi-Task Learning\u201c. arXiv, 17. Mai 2016. https://doi.org/10.48550/arXiv.1605.05101.</p> <p>Liu, Shengzhong, Franck Le, Supriyo Chakraborty, und Tarek Abdelzaher. \u201eOn Exploring Attention-based Explanation for Transformer Models in Text Classification\u201c. In 2021 IEEE International Conference on Big Data (Big Data), 1193\u20131203, 2021. https://doi.org/10.1109/BigData52589.2021.9671639.</p> <p>Manyika, James. \u201eAn Overview of Bard: An Early Experiment with Generative AI\u201c, o.\u00a0J. https://ai.google/static/documents/google-about-bard.pdf.</p> <p>McCallum, Andrew, und Kamal Nigam. \u201eA Comparison of Event Models for Naive Bayes Text Classification\u201c, o.\u00a0J. http://www.cs.cmu.edu/~dgovinda/pdf/multinomial-aaaiws98.pdf.</p> <p>McCoy, R. Thomas, Ellie Pavlick, und Tal Linzen. \u201eRight for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference\u201c. arXiv, 24. Juni 2019. http://arxiv.org/abs/1902.01007.</p> <p>Merity, Stephen, Nitish Shirish Keskar, und Richard Socher. \u201eRegularizing and Optimizing LSTM Language Models\u201c. arXiv, 7. August 2017. https://doi.org/10.48550/arXiv.1708.02182.</p> <p>Mikolov, Tom\u00e1\u0161, Anoop Deoras, Daniel Povey, Luk\u00e1\u0161 Burget, und Jan \u010cernock\u00fd. \u201eStrategies for training large scale neural network language models\u201c. In 2011 IEEE Workshop on Automatic Speech Recognition &amp; Understanding, 196\u2013201, 2011. https://doi.org/10.1109/ASRU.2011.6163930.</p> <p>Min, Bonan, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heinz, und Dan Roth. \u201eRecent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey\u201c. arXiv, 1. November 2021. http://arxiv.org/abs/2111.01243.</p> <p>OpenAI. \u201eGPT-4 Technical Report\u201c. arXiv, 27. M\u00e4rz 2023. https://doi.org/10.48550/arXiv.2303.08774.</p> <p>Ouyang, Long, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, u.\u00a0a. \u201eTraining language models to follow instructions with human feedback\u201c. arXiv, 4. M\u00e4rz 2022. http://arxiv.org/abs/2203.02155.</p> <p>Peters, Matthew E., Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, und Luke Zettlemoyer. \u201eDeep contextualized word representations\u201c. arXiv, 22. M\u00e4rz 2018. http://arxiv.org/abs/1802.05365.</p> <p>Radford, Alec, Karthik Narasimhan, Tim Salimans, und Ilya Sutskever. \u201eImproving Language Understanding by Generative Pre-Training\u201c, o.\u00a0J. https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf.</p> <p>Rei, Marek. \u201eSemi-supervised Multitask Learning for Sequence Labeling\u201c. arXiv, 24. April 2017. http://arxiv.org/abs/1704.07156.</p> <p>Shin, Andrew, Masato Ishii, und Takuya Narihira. \u201ePerspectives and Prospects on Transformer Architecture for Cross-Modal Tasks with Language and Vision\u201c. International Journal of Computer Vision 130, Nr. 2 (1. Februar 2022): 435\u201354. https://doi.org/10.1007/s11263-021-01547-8.</p> <p>Sidorov, Grigori, Francisco Castillo, Efstathios Stamatatos, Alexander Gelbukh, und Liliana Chanona-Hern\u00e1ndez. \u201eSyntactic N-grams as machine learning features for natural language processing\u201c. Expert Systems with Applications: An International Journal 41 (1. Februar 2014): 853\u201360. https://doi.org/10.1016/j.eswa.2013.08.015.</p> <p>Sundermeyer, Martin, Ralf Schl\u00fcter, und Hermann Ney. \u201eLSTM Neural Networks for Language Modeling\u201c. In Interspeech 2012, 194\u201397. ISCA, 2012. https://doi.org/10.21437/Interspeech.2012-65.</p> <p>TensorFlow. \u201eNeural Machine Translation with a Transformer and Keras | Text\u201c. TensorFlow. Zugegriffen 1. April 2023. https://www.tensorflow.org/text/tutorials/transformer.</p> <p>TensorFlow. \u201eText Classification with an RNN | TensorFlow\u201c. Zugegriffen 1. April 2023. https://www.tensorflow.org/text/tutorials/text_classification_rnn.</p> <p>Thoppilan, Romal, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, u.\u00a0a. \u201eLaMDA: Language Models for Dialog Applications\u201c. arXiv, 10. Februar 2022. http://arxiv.org/abs/2201.08239.</p> <p>Topal, M Onat, Anil Bas, und Imke van Heerden. \u201eExploring Transformers in Natural Language Generation: GPT, BERT, and XLNet\u201c, o.\u00a0J. https://arxiv.org/abs/2102.08036.</p> <p>Tunstall, Lewis, Leandro von Werra, und Thomas Wolf. Natural Language Processing with Transformers. O\u2019Reilly Media, Inc., 2022.</p> <p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, akob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, und Illia Polosukhin. \u201eTransformer: A Novel Neural Network Architecture for Language Understanding\u201c, 31. August 2017. https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html.</p> <p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, und Illia Polosukhin. \u201eAttention is All you Need\u201c. In Advances in Neural Information Processing Systems, Bd. 30. Curran Associates, Inc., 2017. https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.</p> <p>Yan, Xueming, Zhihang Fang, und Yaochu Jin. \u201eAugmented Transformers with Adaptive n-grams Embedding for Multilingual Scene Text Recognition\u201c. arXiv, 27. Februar 2023. http://arxiv.org/abs/2302.14261.</p> <p>Zhou, Chunting, Chonglin Sun, Zhiyuan Liu, und Francis C. M. Lau. \u201eA C-LSTM Neural Network for Text Classification\u201c. arXiv, 30. November 2015. http://arxiv.org/abs/1511.08630.</p> <p>Zhu, Q., und J. Luo. \u201eGenerative Pre-Trained Transformer for Design Concept Generation: An Exploration\u201c. Proceedings of the Design Society 2 (Mai 2022): 1825\u201334. https://doi.org/10.1017/pds.2022.185.</p>"},{"location":"Themen/Spracherkennung/","title":"Spracherkennung","text":"<p>von Amir Amuri, Stefan Hagemann und Florian Hagemann</p>"},{"location":"Themen/Spracherkennung/#abstract","title":"Abstract","text":"<p>Sprachverarbeitung ist heutzutage allgegenw\u00e4rtig und die meisten Menschen sind bewusst oder unbewusst schon damit in Ber\u00fchrung gekommen. Sei es als Teil eines Sprachassistenten in einem Smartphone, in einem Auto oder in einem Smart Home, oder in einer Diktier App. Aufgrund der Pr\u00e4senz des Themas in der Gesellschaft wurde in dieser Arbeit ein Podcast aufgenommen, ein Fachvortrag und eine Code-Demonstration zum Thema Spracherkennung im Rahmen der Vorlesung Seminar aktuelle Themen der KI bei Prof. Dr. Wahl ausgearbeitet und gehalten. Der Folgende Text ist eine schriftliche Ausarbeitung davon.</p> <p>Der Podcast behandelt das Thema Spracherkennung und pr\u00e4sentiert Erkenntnisse eines Experten. Es werden Fragen zur Funktionsweise der Umwandlung von gesprochener Sprache in Computersprache, der Zuh\u00f6rf\u00e4higkeit von Sprachassistenten, der Verantwortlichkeit bei fehlerhaften Bestellungen, dem Lernpotenzial von K\u00fcnstlicher Intelligenz, der Entwicklung der Spracherkennung und zuk\u00fcnftigen Ver\u00e4nderungen diskutiert. Die M\u00f6glichkeit der Emotionserkennung in der Sprache und potenzielle Anwendungen werden ebenfalls beleuchtet. Datenschutz und ethische Aspekte werden betont. Der Podcast endet mit einem Ausblick auf kommende Entwicklungen und Vorfreude auf weitere spannende Themen.</p> <p>Der Fachvortrag besch\u00e4ftigt sich haupts\u00e4chlich mit der Entwicklung von Spracherkennung \u00fcber die Zeit und welche Rolle Deep Learning dabei gespielt hat. Dabei werden verschiedene Ans\u00e4tze grob erkl\u00e4rt oder deren idee beschrieben. Zudem wird eine State of the Art Architektur, der Speech Transformer, genauer vorgestellt und erkl\u00e4rt.</p> <p>In der Code-Demonstration ist viel \u00fcber das Preprocessing von Daten und deren Aufbereitung zu sehen und wie zwei verschiedene Ans\u00e4tze trainiert werden k\u00f6nnen. Dabei ist einer davon klassisch und der andere ein Deep Learning Ansatz.</p>"},{"location":"Themen/Spracherkennung/#1-einleitung-motivation","title":"1 Einleitung / Motivation","text":"<p>Es gibt keine offizielle allgemein anerkannte Definition von Spracherkennung. Eine Definition, die die wesentlichen Punkte zur Beschreibung von Spracherkennung gut zusammenf\u00e4sst ist von ibm:</p> <p>Definition</p> <p>Speech recognition, also known as automatic speech recognition (ASR), computer speech recognition, or speech-to-text, is a capability which enables a program to process human speech into a written format.</p> <p>Wie sich aus der Definition erkennen l\u00e4sst hat Spracherkennung viele Anwendungen. \u00dcberall dort, wo ein Computer oder eine Maschiene Die Sprache eines Menschen als akustisches Signal verstehen und verarbeiten muss kommt Spracherkennung zum Einsatz. In jedem Sprachassistenten, ob dieser in einem Handy, in einem Computer, in einem Auto oder in einem Smart Home ist, findet sich Spracherkennung als teil der Software wieder. Aber auch in Apps die Diktiertes in Text umwandeln sollen kommt es zum Einsatz. Da die Anwendungsgebiete von Spracherkennung so weitl\u00e4ufig sind, ist die Bedeutung des Themas und das Interesse daran klar.</p> <p>Im Laufe der Zeit wurden verschieden Methoden angewandt um Spracherkennung zu betreiben. Dabei gibt es sowhol klassische Methoden, welche mit mathematischen oder statistischen Modellen arbeiten, und moderne Ans\u00e4tze mit Deep Learning Methoden, welche mit neuronalen Netzen, die verschiedenen Architekturen haben, arbeiten.</p> <p>Im n\u00e4chsten Abschnitt wird der Schwerpunkt des Fachvortrages behandelt. Ziel ist es, einen \u00dcberblick \u00fcber die Entwicklung von Spracherkennung im Laufe der Zeit zu geben, indem wichtige und interessante Ans\u00e4tze, Produkte oder Vorschl\u00e4ge aufgez\u00e4hlt oder erkl\u00e4rt werden.</p>"},{"location":"Themen/Spracherkennung/#2-geschichte-der-spracherkennung","title":"2 Geschichte der Spracherkennung","text":""},{"location":"Themen/Spracherkennung/#1950er-und-1960er","title":"1950er und 1960er","text":"<p>In den 1950er Jahren wurden die ersten Spracherkennungssysteme erfunden. Damals waren die Systeme allerdings wesentlich schlechter als die, welche wir heutzutage aus bekannten Sprachassisten wie Siri oder Alexa kennen. Die ersten Systeme waren darauf fokussiert Zahlen zu erkennen.</p> <p>1952 wurde von Bell Laboratories das \"Audrey\" System erfunden. Das \"Audrey\" System konnte nur einzelne Zahlen erkennen, die von einer Stimme gesprochen worden sind. Der Grund daf\u00fcr, warum nur die gesprochenen Zahlen von einer einzigen Stimme erkannt werden konnten war, dass daf\u00fcr template matching verwendet wurde.</p> <p>Definition</p> <p>Template matching is the process of moving the template over the entire image and calculating the similarity between the template and the covered window on the image.</p> <p>Im Fall der Spracherkennung ist beim template matching das Bild zum Beispiel ein Spektogramm.</p> <p>1962 wurde von IBM \"Shoebox\" vorgestellt. Das System konnte 16 englische W\u00f6rter verstehen und darauf antworten. Die Spracherkennung hat funktioniert, indem Merkmale aus dem Input extrahiert und mit abgespeicherten Verglichen wurden. So wurde bestimmt welches Wort gesagt wurde.</p> <p> </p> Fig. 1 Shoebox <p>1966 wurde Linear Predictive Coding (LPC) vorgeschlagen. Linear Predictive Coding ist ein Verfahren zur Reduktion der Datenmenge. Es basiert auf der Annahme, dass Sprachsignale viele wiederholende Muster enthalten, die durch eine Vorhersagemethode erfasst werden k\u00f6nnen. Der Vorteil von diesem Ansatz ist, dass durch die Reduktion der Datenmenge eine effizientere Verarbeitung und \u00dcbertragung von Sprache m\u00f6glich ist.</p> <p>1968 wurde Dynamic Time Warping (DTW) erfunden. Mit Dynamic Time Warping kann man die \u00c4hnlichkeit zwischen zwei zeitlichen Signalen, die sich in Geschwindigkeit und / oder Zeitachse unterscheiden messen. Dadurch k\u00f6nnen zum Beispiel auch langsamer oder schneller ausgesprochene W\u00f6rter als in der abgespeicherten Vorlage erkannt werden. </p> <p>Au\u00dferdem wurde zu dieser Zeit die Mathematik des Hidden Markov Models formuliert, welche aber erst sp\u00e4ter in der Spracherkennung zum Einsatz kam.</p>"},{"location":"Themen/Spracherkennung/#1970er","title":"1970er","text":"<p>In den 1970er Jahren wurden Fortschritte in der Spracherkennung vor allem aufgrund des Speech Understanding Research (SUR) Programs von der Defense Advanced Research Project Agency (DARPA) von 1971 bis 1976 gemacht. Ziel des Programs war es, Fortschritte in der automatischen Spracherkennung und Sprachverst\u00e4ndnisforschung zu erzielen.</p> <p>Durch das Program ist unter anderem das \"Harpy\" System von der Carnegie-Mellon Universit\u00e4t entstanden. Das System konnte \u00fcber 1000 W\u00f6rter verstehen, was ungef\u00e4hr dem Vokabular eines drei J\u00e4hrigem gleicht. \"Harpy\" war der Versuch die besten Merkmale des Hearsy-I Systems, welches ein regelbasiertes System war, und des DRAGON Systems, welches eine Markov Kette verwendete zu kombinieren. Es hatte ein State Transition Network mit 15000 states. In dem State Transition Network sind alle m\u00f6glichen Pfade durch die endliche Anzahl an states dargestellt, wie zum Beispiel alle m\u00f6glichen W\u00f6rter oder S\u00e4tze. Zudem waren Regeln zu Wortgrenzen und Grammatik implementiert. Durch diesen Ansatz konnte \"Harpy\" die f\u00fcr das SUR vorgesehenen Ziele, n\u00e4mlich \u00fcber 90 % eines Satzes aus einem Lexikon mit 1000 W\u00f6rtern verstehen zu k\u00f6nnen, erf\u00fcllen.</p> <p> </p> Fig. 2 Harpy"},{"location":"Themen/Spracherkennung/#1980er","title":"1980er","text":"<p>Der bedeutenste Fortschritt in den 1980er Jahren war die Verwendung von Hidden Markov Models f\u00fcr Spracherkennung. Zwar waren diese schon vorher bekannt, fanden aber erst jetzt Anwendung.</p> <p>1986 wurde das SPHINX System von der Carnegie-Melln Universit\u00e4t entwickelt. Es gibt noch weitere Versionen und Verbesserungen davon, aber die erste Version verwendete ein Hidden Markov Model.</p> <p>Ein Hidden Markov Model ist ein Modell, bei dem der zugrunde liegende Prozess eine Markov-Kette ist, aber die Zust\u00e4nde nicht direkt beobachtbar sind. Stattdessen werden beobachtbare Ereignisse oder Symbole (z. B. W\u00f6rter, Phoneme oder akustische Merkmale) erzeugt, die mit bestimmten Wahrscheinlichkeiten zu den zugrunde liegenden versteckten Zust\u00e4nden geh\u00f6ren. Die versteckten Zust\u00e4nde sind nicht direkt sichtbar und werden als \"versteckt\" bezeichnet.</p> <p>Eine Markov-Kette ist ein Modell, bei dem ein System eine Reihe von diskreten Zust\u00e4nden durchl\u00e4uft und sich von einem Zustand zum n\u00e4chsten gem\u00e4\u00df einer bestimmten \u00dcbergangswahrscheinlichkeit bewegt. Jeder Zustand ist direkt beobachtbar, und die Wahrscheinlichkeiten f\u00fcr die Zustands\u00fcberg\u00e4nge sind bekannt.</p> <p>Der Unterschied zwischen einer Markov Kette und einem Hidden Markov Model ist, dass eine Markov-Kette die Wahrscheinlichkeiten der Zustands\u00fcberg\u00e4nge direkt modelliert, w\u00e4hrend ein Hidden Markov Model zus\u00e4tzlich die Wahrscheinlichkeiten der beobachteten Ereignisse in Bezug auf die versteckten Zust\u00e4nde modelliert.</p>"},{"location":"Themen/Spracherkennung/#1990er","title":"1990er","text":"<p>In den 1990er Jahren wurde die Spracherkennung vor allem aufgrund von Verbesserungen am pers\u00f6nlichen Computer vorangetrieben, da dadurch die Kommerzialisierung von Spracherkennungsssoftware m\u00f6glich wurde. Durch die Erfindung von schnelleren Prozessoren konnten jetzt auch privat Personen Spracherkennungsssoftware nutzen. Neben der Kommerzialisierung, besch\u00e4ftigte man sich zu dieser Zeit auch mit der Robustheit von Modellen.</p> <p>1990 wurde zum Beispiel eine Diktiersoftware namens Dragon Dictate herausgebracht.</p> <p>1993 hat Apple \"Speakable Items\", eine Sprachsteuerung f\u00fcr Apple Ger\u00e4te herausgebracht. Hier wurden nur bestimmte bekannte Befehle durch Wortlisten und Mustererkennung erkannt.</p> <p>1999 wurde das BellSouth Voice Portal herausgebracht. Das BellSouth Voice Portal hat den Benutzern erm\u00f6glicht sprachgesteuerte Dienste und Informationen, wie zum Beispiel die Wettervorhersage, \u00fcber das Telefon zu bekommen.</p>"},{"location":"Themen/Spracherkennung/#2000er","title":"2000er","text":"<p>Von 2002 bis 2004 gab es ein weiteres Programm der Defense Advanced Research Project Agency (DARPA) namens Effective Affordable Reusable Speech-to-Text (EARS). Durch dieses Programm wurde die Sammlung des Switchboard-Telefonkorpus, welcher 260 Stunden aufgezeichneter Gespr\u00e4che von \u00fcber 500 Sprechern enth\u00e4lt, finanziert.</p> <p>2007 wurde GOOG-411, ein telefonischer Informationsdienst herausgebracht. Die Aufnahmen von GOOG-411 haben wertvolle Daten geliefert, die Google geholfen haben ihre Spracherkennungssysteme zu verbessern.</p> <p>2007 wurde erstmals ein Long Short Term Memory (LSTM) mit Connectionist Temporal Classification (CTC) trainiert das angefangen hat klassische Ans\u00e4tze in manchen Aufgaben zu \u00fcbertreffen.</p> <p> </p> Fig. 3 Connectionist Temporal Classification <p>Connectionist Temporal Classification ist ein System um ein rekurrentes Neuronales Netz zu trainieren. Dabei ist der Output des Neuronalen Netzes eine Wahrscheinlichkeitsmatrix, welche f\u00fcr jeden Zeitpunkt t die Wahrscheinlichkeit f\u00fcr jedes vorher definiertes Zeichen angibt. Anschlie\u00dfend wird durch einen Algorithmus der Pfad berechnet, der am Ende die h\u00f6chste Wahrscheinlichkeit hat. Die Ausgabesequenz, am Beispiel von Fig. 3, w\u00e4re dann \"aaa-b\". Alle Buchstaben die Mehrmals hintereinander vorkommen werden zusammengefasst zu einem Buchstaben. Dabei ist zu beachten, dass es ein Platzhalter Symbol gibt welches dazu dient auch den selben Buchstaben mehrmals hintereinander zu erkennen, in diesem Beispiel durch ein \"-\" dargestellt. Somit ist die Ausgabe dann der richtige Output \"ab\".</p> <p>2008 wurde von Google die Voice Search App f\u00fcr Smartphones von Apple herausgebracht. Da es Spracherkennung f\u00fcr Smartphones gab, konnte Google Daten von mehreren Milliarden Suchen sammeln.</p> <p>2009 wurde ein deep feedforward neural network f\u00fcr acoustic modeling hergenommen. Der Ansatz f\u00fcr Acoustic modeling besteht darin ein feedforward neural network zu verwenden, um zu bestimmen, wie gut die jeweiligen states des Hidden Markov Models zu den einzelnen Inputs passen. Das neuronale Netz nimmt mehrere Ausschnitte von Koeffizienten des akustischen Inputs, die diesen repr\u00e4sentieren und erzeugt als Ausgabe Wahrscheinlichkeiten f\u00fcr die Zust\u00e4nde des Hidden Markov Models.</p>"},{"location":"Themen/Spracherkennung/#2010er","title":"2010er","text":"<p>2014 gab es den ersten Versuch f\u00fcr ein end-to-end automatic speechrecognition (ASR) system mit einem RNN-CTC Modell von Google.</p> <p>Ein end-to-end ASR System ist ein Spracherkennungssystem, das den gesamten Prozess der Spracherkennung von der Audioeingabe bis zur Textausgabe abdeckt, ohne den Einsatz traditioneller separater Komponenten wie Sprachmerkmalsextraktion, Ausrichtung oder Sprachmodellierung.</p> <p>2015 wurde Googles Spracherkennung durch ein CTC trainiertes LSTM sehr verbessert und wurde dann auch f\u00fcr Google Voice f\u00fcr alle mit einem Smartphone verf\u00fcgbar.</p> <p>2016 wurder das erste Aufmerksamkeitsbasierte automatic speechrecognition Model vorgeschlagen. Das Modell \"Listen, Attend und Spell\" (LAS) h\u00f6rt die akustischen Signale, achtet auf verschiedene Teile des Signals und gibt das Transkript einen Buchstaben nach dem anderen aus. Es besteht aus zwei Teilen, einem Listener und einem Speller. Der Listener ist ein Encoder recurrent neural network, genauer gesagt ein pyramidal Bidirectional Long Short Term Memory (BLSTM). Der Listener extrahiert Merkmale aus den akustische Signalen. Der Output des Listners dient als Eingabe f\u00fcr den Speller. Der Speller ist Aufmerksamkeitsbasierter Zeichen Decoder, genauer gesagt ein Long Short Term Memory (LSTM) der eine Wahrschenilichkeitsmatrix als Output hat.</p> <p> </p> Fig. 4 \"Listen, Attend, Spell\" <p>Aufmerksamkeits basierte Modelle k\u00f6nnen im Gegensatz zu den Ans\u00e4tzen mit Connectionist Temporal Classification oder Hidden Markov Models alle Teile eines Spracherkennungssystems lernen. Dadurch spart man sich den Speicherplatz f\u00fcr ein gro\u00dfes language model, wodurch dieser Ansatz gut f\u00fcr Anwendungen mit limitiertem Speicherplatz geeignet ist. Ende 2016 haben diese Modelle die Ans\u00e4tze mit Connectionist Temproal Classification schon \u00fcbertroffen.</p> <p>2017 errreichten Forscher von Microsoft einen historischen Meilenstein im Bezug auf menschliche Parit\u00e4t bei der Transkription von Telefongespr\u00e4chen. Das Modell war besser als vier proffessionelle Transkriptoren (Menschen).</p> <p>Zudem wurde 2017 im Paper \"Attention is all you need\" die erste Transformer Architektur vorgestellt. Diese war aber noch nicht f\u00fcr Spracherkennung gedacht, sondern f\u00fcr machine translation.</p> <p>2018 wurde dann die Speech Transformer Architektur im Paper Speech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition vorgestellt. Diese Archtektur ist eine Erweiterung der urspr\u00fcnglichen transformer Architektur f\u00fcr Spracherkennung.</p> <p>In 2019 bis Anfang der 2020er erreichten Transformer dann state of the Art Ergebisse.</p>"},{"location":"Themen/Spracherkennung/#3-deep-learning-und-spracherkennung","title":"3 Deep Learning und Spracherkennung","text":"<p>Es gibt viele verschiedene Definitionen f\u00fcr Deep Learning, aber keine allgemein offiziell anerkannte Definition. Zwei Definitionen, welche die wesentlichen Punkte gut zusammenfassen sind Folgende:</p> <p>Definition von ibm</p> <p>Deep Learning ist ein Teil des  maschinellen Lernens, bei dem es sich im Wesentlichen um ein neuronales Netz mit drei oder mehr Schichten handelt.</p> <p>Definition von oxford languages</p> <p>A type of machine learning based on artificial neural networks in which multiple layers of processing are used to extract progressively higher level features from data.</p> <p>Zusammengefasst ist Deep Learning ein Teil des maschinellen Lernens, bei dem man ein neuronales Netz mit mindestens 3 Schichten hat.</p> <p>Deep Learning wurde in der Spracherkennung erst 2009 wirklich relevant, wurde aber schon fr\u00fcher f\u00fcr die Spracherkennung erforscht. Sowohl normale als auch deep Neuronal networks konnten nicht mit einem Hidden Markov Model mithalten, da es einige Probleme gab.</p> <p>Ein Problem, war der diminishing gradient. Je weiter im Netz bei der Backpropagation zur\u00fcck gerechnet wird,  um so kleiner wird der Gradient und \"verschwindet\" irgendwann. Das Problem hierbei ist, dass somit in den Schichten weiter hinten im Netz wenig bis keine Anpassungen mehr gemacht werden k\u00f6nnen und somit gewisse Datenzusammenh\u00e4nge nicht erfasst werden k\u00f6nnen. Das tritt vor allem bei Aktivierungsfunktionen auf, die den Gradienten eher d\u00e4mpfen. Ein gutes Beispiel daf\u00fcr ist die Sigmoid Funktion. Ein weiteres Problem war die schwache zeitliche Korrelationsstruktur in neuronalen Netzen. Zudem fehlten damals noch gro\u00dfe Trainingsdatens\u00e4tze und Rechenleistung.</p> <p>Als die Probleme nach und nach gel\u00f6st worden sind konnte durch Deep Learng gro\u00dfe Fortschritte in der Spracherkennung erreicht werden. Die Ans\u00e4tze sind mittlerweile so gut, dass diese mit mehreren menschlichen Transkriptoren mithalten k\u00f6nnen und sind heute State of the Art.</p>"},{"location":"Themen/Spracherkennung/#4-speech-transformer-architektur","title":"4 Speech Transformer Architektur","text":"<p>In diesem Abschnitt wird die Speech Transformer Architektur aus dem Paper Speech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition erkl\u00e4rt. Der Grund, warum genau diese Architektur genauer erkl\u00e4rt wird ist, dass es viele Ab\u00e4nderungen und Verbesserungen gibt die auf dieser Architektur aufbauen. Ziel ist es zu verstehen wie der Speech Transformer aufgebaut ist und funktioniert, um dann Ab\u00e4nderungen die darauf aufbauen leichter und schneller zu verstehen.</p> <p> </p> Fig. 5 Speechtransformer <p>Die Speechtransformer Architektur besteht aus zwei Teilen, einem Encoder und einem Decoder.</p> <p>Der Encoder hat als Input ein Spektogramm. Am Anfang des Encoders sind drei Convolutional Layer mit einem 3x3 Kernel und Stride 2. Anschlie\u00dfend ist hier ein Block frei in dem optional zus\u00e4tzliche Module eingef\u00fcgt werden k\u00f6nnen. In dem Paper werden drei Beispiel genannt, wobei das interessanteste ein 2D Attention Mechanismus ist.</p> <p> </p> Fig. 6 2D Attention Mechanismus <p>Bei dem vorgeschlagenen Mechanismus werden zun\u00e4chst drei convolutional networks auf den Input angewendet um daraus die Repr\u00e4sentationen f\u00fcr Querries, Keys, und Values zu bekommen. Danach gibt es zwei Arten von Attention. Beides sind Scaled Dot-Product Attentions, jedoch ist die Erste f\u00fcr den zeitlichen Aspekt und die Zweite f\u00fcr den frequency Aspekt zust\u00e4ndig. Am Ende werden die ouputs zusammengef\u00fcgt und in ein weiteres convolutional network gegeben.</p> <p>Nach dem zus\u00e4tzlichen optionalen Block wird eine linear Transformationen auf den flachgemachten feature map durchgef\u00fchrt und mit dem positional encoding addiert. Positional Encoding erm\u00f6glicht dem Modell auf relative Positionen zu achten.</p> <p> </p> Fig. 7 Formel f\u00fcr das Berechnen des positional encoding <p>Das Ergebnis der Summe durchl\u00e4uft anschlie\u00dfend N Encoder Bl\u00f6cke. Ein Encoder Block besteht aus einem Multi-Head-Attention Layer gefolgt von einem feedforward neuronal network. Zudem sind nach jedem Layer Layer Normalisationen eingebaut und es gibt residual connections.</p> <p>Multi-Head-Attention besteht aus mehreren Scaled Dot-Products.</p> <p> </p> Fig. 8 Multi-Head-Attention <p>Ein Scaled Dot-Product wird wie folgt berechnet:</p> <p> </p> Fig. 8 Scaled Dot-Product <p>Der Decoder hat die bisher generierte Ziel Sequenz als input. Als Erstes wird ein Zeichen Embedding angewandt um W\u00f6rter, Buchstaben oder Zeichen im Vektorraum darzustellen. Das Ergebnis wird zu einem positional encoding addiert, das genau wie bei dem Encoder berechnet wird. Das Ergebnis der Summe durl\u00e4uft dann N Decoder Bl\u00f6cke.</p> <p>Ein Decoder Block besteht aus drei Teilen, einem masked Multi-Head-Attention Layer, einem Multi-Head-Attention Layer und einem feedforward neural network. nach jedem Layer gibt es wie beim Encoder auch Layer Normalsations und residual connections. Eine masked Multi-Head-Attention stellt sicher, dass die Vorhersagen f\u00fcr position j nur auf den outputs basiert die bis position j-1 gehen. Au\u00dferdem nimmt das Multi-Head-Attention Layer die Values und Keys von dem Output des Encoders, und die Querries aus dem eigenen Block.</p> <p>Nach den Decoder Bl\u00f6cken werden die outputs durch eine lineare Projektion und die anschlie\u00dfende Softmax Funktion in die Wahrscheinlichkeiten der Ausgangsklassen umgewandelt.</p>"},{"location":"Themen/Spracherkennung/#5-anwendung","title":"5 Anwendung","text":""},{"location":"Themen/Spracherkennung/#code-demo","title":"Code-Demo","text":""},{"location":"Themen/Spracherkennung/#dataset","title":"Dataset","text":""},{"location":"Themen/Spracherkennung/#data-ingestion","title":"Data Ingestion","text":"<p>In dieser Code-Demo werde ich \u00fcber das Sammeln von Daten f\u00fcr Spracherkennung, Preprocessing und die Anwendung von zwei verschiedenen Models reden. Der hier erw\u00e4hnte Datensatz wird sp\u00e4ter nicht zum Training genutzt, sondern dient nur als Beispiel, da das Training zu \"computationally expensive\" war.</p> <p>Am Anfang eines jeden Machine Learning Projekts steht das Sammeln von Daten. Auch in dieser Code-Demo wird mit dem Sammeln von Daten begonnen.  Die erste Phase hierf\u00fcr ist die sogenannte Data Integration. Diese Phase umfasst den Prozess des Sammelns und Importierens von Daten aus verschiedenen Quellen in ein System zur weiteren Verarbeitung oder Analyse. In diesem Fall wurden mehrere verschiedene Datens\u00e4tze aus dem Internet heruntergeladen. Eine alternative w\u00e4re die Daten aus der realen Welt selbst zu machen, was allerdings f\u00fcr eine Privatperson sehr schwierig ist.</p> <p>Hier werden zwei Beispiele der verwendeten Datens\u00e4tze gezeigt:</p> <p>Einerseits der Datensatz von Thorsten M\u00fcller, ein Datensatz der von einer Privatperson der \u00d6ffentlichkeit zur Verf\u00fcgung gestellt wurde:</p> <p></p> <p>Und andererseits der sehr bekannte Common Voice Datensatz von Mozilla: </p> <p></p>"},{"location":"Themen/Spracherkennung/#data-transformation","title":"Data Transformation","text":"<p>Die n\u00e4chste Phase ist die der Data Transformation. Diese ist der Prozess der Umwandlung von Daten von einem Format oder einer Struktur in ein anderes, um die Datenanalyse zu erleichtern.</p> <p>Die zuvor heruntergeladenen waren urspr\u00fcnglich in vielen verschiedenen Formaten/Strukturen vorhanden: * Ordnerstruktur + .csv * Json * .tsv / .csv</p> <p>Diese verschiedenen Strukturen werden in diesem Schritt in ein einheitliches Format, in diesem Fall ein .csv-file pro Datensatz mit gleicher Struktur, gebracht</p> <p>Nicht nur die Struktur, sondern auch die enthaltenen Informationen der Datens\u00e4tze unterscheiden sich. Die Hauptinformationen, die wirklich f\u00fcr das Training ben\u00f6tigt werden, sind jedoch immer vorhanden. In diesem Fall sind das:   * \"sentence\": Der Satz, der in der Audio-Datei gesprochen wird.   * \"path\": Der Pfad zur Audio-Datei</p> <p>Allerdings sind in diesen Datens\u00e4tzen auch andere Informationen, die nicht direkt f\u00fcr das Training erforderlich sind enthalten.   * Geschlecht   * Akzent   * Alter   * Sprecher ID   * source (original-Dataset)</p> <p>Obwohl diese Daten weder f\u00fcr das Training notwendig, noch in allen Datens\u00e4tzen vorhanden sind, sollten diese nicht \"weggeworfen\" werden, da diese f\u00fcr die Evaluation des Models, Identifikation von Fehlern, oder Analysierung des Aufbaus des Aufbaus des finalen Datensatzes sehr n\u00fctzlich sein k\u00f6nnen.</p> <p>Ausserdem wurde die Gr\u00f6\u00dfe der Audio-Datei als Spalte hinzugef\u00fcgt, um die Erstellung von Sub-datasets mit bestimmter Gr\u00f6\u00dfe zu erleichtern.</p>"},{"location":"Themen/Spracherkennung/#beispiel-der-umformung-eines-datensets","title":"Beispiel der Umformung eines Datensets","text":"<p>Hier wird als Beispiel der Code zur Umformung eines Datensatzes vorgef\u00fchrt.</p> <p>Die Import-Statements sind: * pandas: Arbeit mit Datens\u00e4tzen * numpy: verschiedene optimierte numerische Operationen * os/shutil: verschiedene Operationen mit Dateien * tqdm: f\u00fcr Progressbars</p> <pre><code>import pandas as pd\nimport numpy as np\nimport os\nimport shutil\nfrom tqdm.notebook import tqdm\n</code></pre> <p>Hier werden die Ordner definiert, in welche die transformierten Daten kommen:</p> <pre><code>clips_dest_dir = \"../presentation_folders/new_dataset/clips/caito\"\ndf_dest_dir = \"../presentation_folders/new_dataset/dfs/caito\"\nif not os.path.exists(clips_dest_dir):\n    os.mkdir(clips_dest_dir)\nif not os.path.exists(df_dest_dir):\n    os.mkdir(df_dest_dir)\n</code></pre> <p>Dies ist ein Screenshot, wie die Struktur dieses Datensatzes aussieht:</p> <p></p> <p>In diesem Code wird durch die Ordnerstruktur iteriert um die dort enthaltenen Informationen in ein .csv-file zu schreiben:</p> <pre><code>df = pd.DataFrame(columns=[\"speaker_id\", \"path\", \"sentence\",\"gender\",\"age\",\"accents\"])\n\nstart_dir = \"../presentation_folders/datasets/de_DE/by_book\"\n\nfor gender in os.listdir(start_dir):\ngender_dir = os.path.join(start_dir, gender)\nif not os.path.isdir(gender_dir):\n    continue\nfor speaker in os.listdir(gender_dir):\n    speaker_dir = os.path.join(gender_dir, speaker)\n    if not os.path.isdir(speaker_dir):\n        continue\n    for recording_session in os.listdir(speaker_dir):\n        session_dir = os.path.join(speaker_dir, recording_session)\n        if not os.path.isdir(session_dir):\n            continue\n        session_df=pd.read_table(os.path.join(session_dir, \"metadata.csv\"),sep=\"|\",header=None)\n        session_df=session_df.rename(columns={0:\"path\",1:\"raw_sentence\",2:\"sentence\"})\n        session_df=session_df.drop(columns=[\"raw_sentence\"])\n        session_df.path = session_df.path.astype(str) + \".wav\"\n\n        session_df[\"speaker_id\"] = [speaker for i in range(len(session_df))]\n        session_df[\"gender\"] = [gender for i in range(len(session_df))]\n        session_df[\"age\"] = [np.NaN for i in range(len(session_df))]\n        session_df[\"accents\"] = [np.NaN for i in range(len(session_df))]\n\n        df = pd.concat([df,session_df]).reset_index(drop=True)\n\n        print(f\"gender: {gender}, speaker: {speaker}, recording session: {recording_session}\")\n\n\n        for file in tqdm(list(session_df[\"path\"][:int(len(session_df)/10)])):\n            src_path = os.path.join(session_dir, \"wavs\", file)\n            dest_path = os.path.join(clips_dest_dir, file)\n            shutil.copy(src_path, dest_path)\n\n\ndf.to_csv(os.path.join(df_dest_dir, \"caito.csv\"))\n</code></pre>"},{"location":"Themen/Spracherkennung/#data-integration","title":"Data Integration","text":"<p>Der n\u00e4chste Punkt ist die Data Integration. Hier werden die Daten in einen Datensatz zusammengefasst. In diesem Fall ist dies sehr einfach, da wir einfach nur csv-files der selben Struktur zusammenf\u00fcgen m\u00fcssen.</p> <p>Nun werden verschiedene Informationen \u00fcber den fertigen Datensatz dargestellt und analysiert: <pre><code>df=pd.read_csv(\"../complete_dataset/complete_size_df_woindex.csv\")\n</code></pre></p> <p>Hier wird die NaN-Ratio der verschiedenen Columns angegeben: <pre><code>df.sample(n=5,random_state=42)\nprint(\"NaN-Ratio\\n\")\nfor col in df.columns:\n    print(f\"{col}: {df[col].notna().sum()/len(df)*100:.1f}%\")\n</code></pre></p> <p>Hier wird die Anzahl der verschiedenen Sprecher und Akzente gezeigt: <pre><code>print(df[\"speaker_id\"].nunique())\nprint(df[\"accents\"].nunique())\n</code></pre></p> <p>In diesem Code-Snippet wird die Verteilung des Geschlechts im Datensatz gezeigt: <pre><code>df[\"gender\"].value_counts()\n</code></pre></p>"},{"location":"Themen/Spracherkennung/#preprocessing","title":"Preprocessing","text":"<p>Um die Audio-Dateien in eine Form zu bringen die f\u00fcr Machine Learning Models verarbeitbar, gibt es viele verschiedene Preprocessing-Methoden.  Hier werden zwei dieser Preprocessing-Methoden genauer angeschaut. Die erste Methode die wir uns anschauen wollen, ist das Spektogram. Dieses ist eine visuelle Darstellung des Spektrums des Signals \u00fcber die Zeit. In dieser Repr\u00e4sentation gehen fast keine Daten verloren, was einerseits dazu f\u00fchrt, dass diese Methode sehr flexibel ist, wenn es darum geht f\u00fcr welchen Anwendungszweck sie verwendet werden kann, andererseits aber auch das Problem mit sich bringt, dass diese gr\u00f6\u00dfere Menge von Daten im Vergleich zu anderen Methoden eine l\u00e4ngere Trainingszeit mit sich zieht.</p> <p>Hier kommt zum Beispiel MFCC ins Spiel. Diese Methode hat im Endeffekt eine viel kleinere Datenmenge und ist stark auf Spracherkennung spezialisiert. Ausserdem ahmt diese Methode das menschliche Geh\u00f6r nach, wodurch diese Aspekte der Datenreduktion aber auch der Spezialisierung auf Spracherkennung entstehen.</p> <p>Um diese Methoden in Python zu nutzen wird hier die library \"librosa\" verwendet. <pre><code>import matplotlib.pyplot as plt\nimport librosa\nimport librosa.display\nimport numpy as np\n</code></pre></p> <p>Hier wird die zu verarbeitenden Audiodatei geladen. <pre><code>audio_file = \"aufnahmen/english_audio.wav\"\ny, sr = librosa.load(audio_file)\n</code></pre></p> <p>In diesem Code-Snippet wird ein Spektogram der Audio-Datei geplottet. <pre><code>plt.figure(figsize=(14, 5))\nD = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\nlibrosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')\nplt.colorbar(format='%+2.0f dB')\nplt.title('Spectrogram')\nplt.show()\n</code></pre></p> <p>In diesem Code-Snippet wird das MFCC der Audio-Datei gezeigt. <pre><code>mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\nplt.figure(figsize=(14, 5))\nlibrosa.display.specshow(mfccs, sr=sr, x_axis='time')\nplt.colorbar()\nplt.title('MFCC')\nplt.show()\nmfccs\n</code></pre></p>"},{"location":"Themen/Spracherkennung/#1model-gaussian-hmm","title":"1.Model: Gaussian HMM","text":"<p>Kommen wir nun zu den Modellen. Das erste Modell \u00fcber das wir sprechen werden ist GHMM.</p> <p>Ein HMM ist ein statistisches Modell, das hilft, \"unsichtbare\" Zust\u00e4nde basierend auf beobachtbaren Daten zu sch\u00e4tzen. (z.B.: Wettervorhersage basierend auf menschlichem Verhalten, Textanalyse basierend auf Wortsequenzen etc.)</p> <p>Ausserdem nutzt das GHMM die Gausssche Normalverteilung.</p> <p>In dem folgenden Beispiel wird ein HMM auf ein Nummern-Dataset trainiert, da die urspr\u00fcngliche Idee, das deutsche Dataset in W\u00f6rter aufzuteilen, nicht umgesetzt werden konnte. <pre><code>import os\n\nimport numpy as np\nfrom hmmlearn import hmm\n\nimport librosa\nfrom python_speech_features import mfcc\nimport scipy.io.wavfile as wav\nimport random as rd\n</code></pre></p> <p>Hier wird die Klasse HMMTrainer definiert, welche f\u00fcr das trainieren und auswerten der Daten verwantwortlich ist. <pre><code>class HMMTrainer(object):\n    def __init__(self,model_name, n_components=4, cov_type='diag', n_iter=1000):\n        self.model_name = model_name\n        self.n_components = n_components\n        self.cov_type = cov_type\n        self.n_iter = n_iter\n\n        self.model = hmm.GaussianHMM(n_components=self.n_components, \n                covariance_type=self.cov_type, n_iter=self.n_iter)\n\n    def train(self, X):\n        np.seterr(all='ignore')\n        self.model.fit(X)\n\n    def get_score(self, input_data):\n        return self.model.score(input_data)\n</code></pre> In diesem Code-Snippet werden die Ordner der verschiedenen Nummern definiert: <pre><code>input_folder='../models/HMM/digit_recordings'\nword_folders = [f for f in os.listdir(input_folder) if not f.startswith(\".\")]\n</code></pre> <pre><code>word_folders\n</code></pre></p> <p>Hier werden die Audio-Dateien in MFCCs umgewandelt und f\u00fcr weitere Verarbeitung gespeichert: <pre><code>import warnings\nwarnings.filterwarnings(\"ignore\")\nlens = {}\ntraining_data = {}\nfor dirname in word_folders:\n    X = []\n    ltt=[]\n    subfolder = os.path.join(input_folder, dirname)\n    for filename in [x for x in os.listdir(subfolder) if x.endswith('.wav')][:-1]:\n        filepath = os.path.join(subfolder, filename)\n        sampling_freq, audio = librosa.load(filepath)            \n        mfcc_features = mfcc(sampling_freq, audio)[:,:13]\n        X.append(mfcc_features)\n\n    training_data[dirname] = X\n</code></pre></p> <p>So sehen die Daten aus: <pre><code>training_data[\"0\"]\n</code></pre></p> <pre><code>hmm_models = []\nfor label in training_data.keys():\n    rd.shuffle(training_data[label])\n    X = np.concatenate(training_data[label][:-5], axis=0)\n    print(X.dtype)\n    print('X.shape =', X.shape)\n    hmm_trainer = HMMTrainer(label)\n    hmm_trainer.train(X)\n    hmm_models.append(hmm_trainer)\n    hmm_trainer = None\n</code></pre> <p>Training der Modelle (ein Model pro Wort): <pre><code>results = []\nfor label in training_data.keys():\n  d_results =[]\n  for i in range(5):\n    mfcc_features=training_data[label][-i]\n\n    scores=[]\n    for model in hmm_models:\n        score = model.get_score(mfcc_features)\n        scores.append(score)\n    index=np.array(scores).argmax()\n    results.append(1 if (label==(hmm_models[index].model_name)) else 0)\n    d_results.append(1 if (label==(hmm_models[index].model_name)) else 0)\n  print(label, sum(d_results)/len(d_results))\nprint(sum(results)/len(results))\n</code></pre></p>"},{"location":"Themen/Spracherkennung/#2model-wav2vec","title":"2.Model: Wav2vec","text":"<p>Das Wav2vec ist ein End-to-End-Model, das von Facebook AI 2019 entwickelt wurde und die Transformer-Architektur, die im vorherigen Teil schon erkl\u00e4rt wurde, nutzt. Es lernt also direkt aus Daten ohne eine manuelle Extraktion von Merkmalen aus der Audio (zB. via MFCC oder Spektogram zu ben\u00f6tigen). Es lernt sogar haupts\u00e4chlich aus unmakrkierten Rohdaten, was das urspr\u00fcngliche Trainieren, wenn man von der riesigen Datenmenge, die es ben\u00f6tigt, absieht, einfacher macht. Es hat eine sehr gute Performance auf dem TIMIT-Datensatz, welcher normalerweise f\u00fcr Evaluationen von Speech-to-Text Systemen genutzt wird.</p> <p></p> <p>Zuerst m\u00fcssen die entsprechenden libraries und der Tokenizer und das Model geladen werden. <pre><code>import torch\nimport torchaudio\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n</code></pre> <pre><code>tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n</code></pre></p> <p>Hier wird eine im vorhinein aufgenommene Audiodatei geladen. <pre><code>path = \"aufnahmen/english_audio.wav\"\nspeech, rate = torchaudio.load(path)\n</code></pre></p> <p>Diese geladene Audidatei wird wird nun in das richtige Format gebracht. <pre><code>if speech.shape[0] &gt; 1:\n    speech = speech[0]\n\nif rate != 16000:\n    resampler = torchaudio.transforms.Resample(orig_freq=rate, new_freq=16000)\n    speech = resampler(speech)\n\nspeech = speech.squeeze()\n</code></pre></p> <p>Zuletzt wird der Text dieses Audifiles predictet und ausgegeben. <pre><code>input_values = tokenizer(speech, return_tensors=\"pt\").input_values\n\nwith torch.no_grad():\n    logits = model(input_values).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\ntranscription = tokenizer.batch_decode(predicted_ids)[0]\n\nprint(transcription)\n</code></pre></p>"},{"location":"Themen/Spracherkennung/#6-weiterfuhrendes-material","title":"6 Weiterf\u00fchrendes Material","text":""},{"location":"Themen/Spracherkennung/#podcast","title":"Podcast","text":"<p>Der Campus Talk \u2013 Silicon Forest \u2013 Folge XX</p>"},{"location":"Themen/Spracherkennung/#podcast-transkript","title":"Podcast Transkript","text":"<p>Moderator: Hallo zusammen und willkommen zu einer neuen Ausgabe vom THD-Podcast, sch\u00f6n, dass Sie wieder eingeschaltet haben. Jeder von uns hat doch bestimmt schon einmal Alexa, Siri oder den Google Assistant genutzt oder getestet. Doch haben Sie sich eigentlich mal Gedanken dar\u00fcber gemacht, wie uns diese Ger\u00e4te \u00fcberhaupt verstehen? Um diese und weitere Fragen zu kl\u00e4ren, haben wir heute einen Experten zu Gast.</p> <p>Herr Amuri studiert momentan K\u00fcnstliche Intelligenz im sechsten Semester. Dort hat er bereits einige Aufgaben zur Spracherkennung l\u00f6sen m\u00fcssen und hat sich k\u00fcrzlich auf diesen Themenbereich spezialisiert.</p> <p>Herr Amuri, vielen Dank, dass Sie da sind.</p> <p>Experte: Hallo, es freut mich sehr, dabei sein zu d\u00fcrfen. </p> <p>Moderator: Sagen Sie mir doch mal, wo kommt Spracherkennung \u00fcberall vor?</p> <p>Experte: Im Grunde, gibt es \u00fcberall Spracherkennung, wo aus Gesprochenem f\u00fcr eine Maschine lesbaren Text entsteht. Bedeutet im Umkehrschluss, dass Spracherkennung vom sogenannten \u201cSpeech-To-Text\u201d, also gesprochenes Wort zu Text, bis hin zu einer Alexa reicht, die das Wort \u201cAlexa\u201d selbst erkennen kann.</p> <p>Moderator:  Ah verstehe!  Also wenn Google \u00dcbersetzer mir einen Text vorliest, ist das keine Spracherkennung? Aber wenn ich was sage und es in Text umgewandelt wird, dann schon.</p> <p>Dann ist aber meine Frage, wie funktioniert denn Spracherkennung bzw. wie wird das Gesprochene in Computersprache umgewandelt?</p> <p>Experte: Ja, genau. Es verwechseln leider viele, dass die Definition von Spracherkennung an sich leider keine Gleichung ist, wie man sie aus dem Matheunterricht kennt, sondern als  Regel zu sehen ist.</p> <p>Genau unterscheiden tun wir ja, wie die Sprache zustande kommt, beziehungsweise meinen wir mit Spracherkennung auch wirklich die gesprochene Sprache.  Der Prozess selbst beginnt deshalb auch mit der Aufnahme eines Sprachsignals, beziehungsweise der gesprochenen Worte, wie diese Mikrofone hier vor uns es auch tun. Diese Aufnahmen werden dann in kleine St\u00fccke aufgeteilt und anschlie\u00dfend nach Faktoren wie die Tonh\u00f6he, Frequenz und die Dauer der jeweiligen T\u00f6ne analysiert. Diese Merkmale werden dann anschlie\u00dfend in einem Sprachmodell verwendet, um die W\u00f6rter und S\u00e4tze zu erkennen, die gesprochen wurden. Das Ganze wird dann so oft wiederholt, bis es m\u00f6glichst gut  erkannt wird. Das kann man sich als Wahrscheinlichkeit vorstellen: Wenn wir eine M\u00fcnze werfen und unser Ziel \u201cKopf\u201d ist, dann werfen wir immer wieder, bis endlich Kopf oben liegt.  Dabei gibt es  bestimmte Wurftechniken, wie wir \u201cKopf\u201d erzwingen k\u00f6nnen. Letztendlich erlernt das Sprachmodell selbstst\u00e4ndig und teilweise mit  Unterst\u00fctzung diese Wurftechniken. Nach dem jetzigen Stand wird es in keinem Fall immer das richtige Ergebnis haben, daf\u00fcr aber sehr oft.</p> <p>Moderator: Ah interessant. Das hei\u00dft also, dass Alexa selbst keine Spracherkennung ist, aber wenn Alexa mich versteht und das verarbeitet, dann schon?</p> <p>Experte: Genau.</p> <p>Moderator: Verstehe. Wir wussten ja nat\u00fcrlich, dass wir dich einladen und haben deshalb unsere Zuh\u00f6rerschaft um Fragen f\u00fcr dich gebeten. Die Erste passt sogar gut zum jetzigen Thema lorelei8977, 24 Jahre alt fragt: Sprachassistenten wie Alexa k\u00f6nnen uns ja auf unseren Wortlaut gehorchen. Nun ist meine Frage, h\u00f6ren uns Sprachassistenten immer zu?</p> <p>Experte: Danke Lorelei f\u00fcr diese wirklich sehr interessante Frage. Sprachassistenten h\u00f6ren uns generell per se nicht zu. Man kann es sich wie ein kleines Baby vorstellen: Ein Baby versteht ja auch keine wirkliche Sprache bis auf einzelne Schl\u00fcsselw\u00f6rter, die es immer wieder h\u00f6rt. Sowas wie Spielplatz, Milch oder auch seinen oder ihren Namen. So \u00e4hnlich ist es auch bei Sprachassistenten. Diese h\u00f6ren einem generell zu, jedoch verstehen sie einen nicht bzw. verstehen sie nur spezifische W\u00f6rter, wie zum Beispiel bei Alexa das jeweilige Aktivierungswort. Das bedeutet, an sich h\u00f6ren sie uns immer zu, da es anders leider nicht funktionieren w\u00fcrde. Jedoch verstehen sie uns nicht, bis zu dem Zeitpunkt, wo wir das Aktivierungswort sagen.</p> <p>Im Falle Alexa ist auch wichtig anzumerken, dass die Hardware, also das Ger\u00e4t, was m\u00f6glicherweise bei ihnen in der Wohnung steht, nur das K\u00f6nnen besitzt, sein Aktivierungswort zu erkennen. Also wenn man zum Beispiel Alexa fragt: \u201cAlexa, wie ist das Wetter\u201d, f\u00e4ngt das Ger\u00e4t an, die Anfrage aufzunehmen. Das bedeutet, dass diese Aufnahme danach  an Amazon gesendet wird. Aus diesem Grund muss es immer mit dem Internet verbunden sein. Die Aufnahme selbst gelangt an einen sicheren Ort innerhalb Amazon.  Hier ist zu ber\u00fccksichtigen, dass auch Amazon an Datenschutzrichtlinien gebunden ist und Sicherheitsvorkehrungen besitzen. Ob die Aufnahmen auch gespeichert werden d\u00fcrfen, kann jeder in den Alexa-Einstellung selbst entscheiden, standardm\u00e4\u00dfig ist das aktiviert.</p> <p>Moderator: Oh, das ist sehr interessant. Also h\u00f6ren uns die kleinen Helferlein immer zu, aber verstehen uns nicht, bevor wir Ihren Namen sagen. </p> <p>Experte: Genau, hierzu ist aber auch anzumerken, dass es ohne diese Aufnahme gar nicht so leicht w\u00e4re, alle Funktionen der Alexa in diesem kleinen Ger\u00e4t zu lagern. Die meisten Sprachassistenten funktionieren auf eine \u00e4hnliche Art und Weise und deswegen ist das Aufnehmen der Stimme momentan nicht wegdenkbar.</p> <p>Moderator: Alles klar, vielen Dank f\u00fcr die Anmerkung. Ich w\u00fcrde dann direkt mit der n\u00e4chsten Frage vom  AktienKarl79 weitermachen, 44 Jahre alt: \u201cWenn ich was per Spracherkennung bestelle, es aber etwas falsch erkennt und damit falsch bestellt, wer ist dann Schuld?\u201d</p> <p>Experte: Also erstmal vorab, ich kann und darf keine rechtlichen Ratschl\u00e4ge geben, da ich kein Experte in diesem Gebiet bin. Generell ist die Frage aber anhand Alexa beantwortbar: Wenn ich nun Alexa den Befehl gebe, mir einen Taschenrechner zu bestellen also \u201cAlexa, bestelle mir ein Taschenrechner\u201d, wird sie mir den erstbesten Taschenrechner in den Einkaufswagen legen, welcher dann meistens ein Amazon Choice Produkt ist. Anschlie\u00dfend wird sie mir den Produktnamen vorlesen, manchmal inklusive  einer Kurzbeschreibung und mich am Ende des Satzes fragen, ob sie das Produkt direkt kaufen soll. Bedeutet, dass man normalerweise immer die M\u00f6glichkeit hat, die Bestellung zu \u00fcberpr\u00fcfen, bevor sie tats\u00e4chlich abgeschlossen wird. Also wird vermutlich die Schuld immer beim Kunden liegen. Hier kann man aber beachten, dass es innerhalb der EU eine Widerrufsfrist von mindestens 14 Tage f\u00fcr Online-Eink\u00e4ufe gibt und man sich deshalb nicht viele Gedanken machen muss.</p> <p>Moderator: Ich wusste garnicht, dass Alexa auch f\u00fcr mich bestellen kann. Es ist aber sehr gut zu wissen, dass mir so etwas nicht aus Versehen passieren kann. Und hier gleich die n\u00e4chste Frage:</p> <p>KarlKlo\u00dfbr\u00fche, 16 Jahre alt fragt: \u201cLernt eine KI, die Spracherkennung benutzt, von dem was ich sage?\u201d</p> <p>Experte: Ah, das ist eine gute Frage. Ja, K\u00fcnstliche Intelligenz kann von uns lernen.  Aber ob Sie das tut, wenn wir mit ihr sprechen, h\u00e4ngt von der Art der KI ab und wie sie entwickelt wurde. Auch wie viel sie von uns tats\u00e4chlich lernt und wie schnell sie das tut, ist logischerweise abh\u00e4ngig von der genauen Art der KI.</p> <p>Wenn wir zum Beispiel mit einem Sprachassistenten wie Alexa sprechen, verwendet dieser Techniken des maschinellen Lernens, um unsere Sprache besser zu verstehen. Dabei kann die KI ihre F\u00e4higkeiten zur Spracherkennung verbessern. Hierbei ist es aber wichtig zu wissen, dass diese Art von Lernen meistens auf Daten von ganz vielen verschiedenen Personen basieren und nicht nur auf einzelne pers\u00f6nliche Daten. Und wir sprechen hier nicht von ein paar 100 oder 200 Personen, sondern meistens von mehreren Hunderttausenden.</p> <p>Auf der anderen Seite gibt es jedoch auch KI-Systeme, die darauf ausgelegt sind, sich an die Bed\u00fcrfnisse und Vorlieben des einzelnen Benutzers anzupassen.  In diesen F\u00e4llen lernt die KI tats\u00e4chlich direkt von Ihnen und kann Ihnen dadurch eine bessere Benutzererfahrung bieten, siehe YouTube und TikTok</p> <p>Kurz gesagt: KI-Systeme k\u00f6nnen von uns lernen, wenn wir sprechen. Aber wie viel sie lernen und wie das die KI beeinflusst, ist von System zu System relativ unterschiedlich.</p> <p>Moderator: Vielen Dank f\u00fcr diese Erkl\u00e4rung! Ich h\u00e4tte nicht gedacht, dass es bei Spracherkennungssystemen so viele Unterschiede gibt.</p> <p>Experte: Es ist in der Tat sehr spannend! Die M\u00f6glichkeiten, wie uns die KI helfen kann, entwickeln sich st\u00e4ndig weiter. Ich freue mich schon darauf, wie sich diese Technik im Verlauf der folgenden Jahre weiterentwickeln wird.</p> <p>Moderator: Es bleibt auf jeden Fall spannend. Dazu haben wir auch eine interessante Frage aus Twitter: curiousmind27, 28 Jahre alt, fragt: \"Wie hat sich Spracherkennung im Laufe der Zeit entwickelt und welche Ver\u00e4nderungen k\u00f6nnen wir in der Zukunft erwarten?\"</p> <p>Experte: Das ist eine schwere, aber durchaus spannende Frage. Ich werde mal mein Bestes geben, die Frage grob zu beantworten.  Sonst s\u00e4\u00dfen wir morgen noch hier. </p> <p>Zun\u00e4chst gab es die einfacheren Systeme, die nur auf bestimmte Befehle reagieren konnten und einen sehr begrenzten Wortschatz hatten. Diese Systeme waren nicht sehr flexibel und konnten oft nicht mit verschiedenen Sprech-Stilen, Akzenten oder Dialekten umgehen.</p> <p>Dank der Fortschritte in k\u00fcnstlicher Intelligenz, maschinellem Lernen und der Entwicklung neuronaler Netzwerke, sind wir nun in der Lage, Systeme zu entwickeln, die eine weit gr\u00f6\u00dfere Bandbreite an Sprachnuancen erkennen und verstehen k\u00f6nnen.</p> <p>Diese Systeme sind in der Lage, wie schon vorher erw\u00e4hnt, aus riesigen Mengen an Text- und Sprachdaten zu lernen. Was dann zu einer deutlichen Verbesserung der Genauigkeit f\u00fchrt.</p> <p>In der Zukunft k\u00f6nnen wir erwarten, dass Spracherkennungssysteme immer pr\u00e4ziser und menschen\u00e4hnlicher werden.  Sie werden besser darin sein, mit Akzenten, Dialekten und mehrsprachigen Umgebungen umzugehen. Zudem werden sie immer besser darin, den Kontext und die Absicht hinter gesprochenen W\u00f6rtern zu verstehen.  Das bedeutet, dass sie nicht nur die W\u00f6rter erkennen, sondern auch den Sinn und die Bedeutung dahinter erfassen k\u00f6nnen.</p> <p>Au\u00dferdem werden zuk\u00fcnftige Spracherkennungssysteme immer st\u00e4rker in unseren Alltag integriert sein.  Wir k\u00f6nnen erwarten, dass sie in einer Vielzahl von Anwendungen eingesetzt werden: Von pers\u00f6nlichen Assistenten, die uns bei allt\u00e4glichen Aufgaben unterst\u00fctzen, bis hin zu Systemen, die in der Arbeitswelt und im Bildungsbereich eingesetzt werden.  Eine weitere m\u00f6gliche Entwicklung ist die Verbesserung der Emotionserkennung in gesprochener Sprache, sodass Systeme besser auf unsere Stimmungen und Gef\u00fchle eingehen k\u00f6nnen.</p> <p>Moderator: Wow, da k\u00f6nnen wir uns ja auf einiges gefasst machen.</p> <p>Experte: Absolut, die Technologie entwickelt sich st\u00e4ndig weiter und wir k\u00f6nnen sicherlich noch viele spannende Entwicklungen im Bereich der Spracherkennung erwarten.</p> <p>Moderator: Ah, hier ist eine weitere Frage, die gut zu der letzten ankn\u00fcpft, von spargelg\u00fcnther, 32 Jahre alt: \"K\u00f6nnen Sprachassistenten wie Alexa in der Lage sein, Emotionen in der Stimme einer Person zu erkennen und darauf zu reagieren?\"</p> <p>Experte: Es gibt bereits Fortschritte in der Forschung und Entwicklung von Technologien zur Emotionserkennung in der Sprache.  Diese Technologien analysieren verschiedene Aspekte der menschlichen Stimme, wie Tonh\u00f6he, Lautst\u00e4rke und Sprechgeschwindigkeit, um Emotionen wie Freude, Traurigkeit, Wut oder Angst zu erkennen.</p> <p>In Zukunft k\u00f6nnten Sprachassistenten wie Alexa diese Art von Emotionserkennung nutzen, um besser auf die Bed\u00fcrfnisse und Stimmungen der Benutzer eingehen zu k\u00f6nnen.  Zum Beispiel k\u00f6nnte ein Sprachassistent, der Traurigkeit in der Stimme eines Benutzers erkennt, Empathie zeigen oder stimmungshebende Musik vorschlagen. Es ist jedoch wichtig zu beachten, dass die Technologie noch verbessert werden muss, bevor sie wirklich effektiv und allgemein verf\u00fcgbar sein kann.</p> <p>Moderator: Das klingt ja schon sehr menschlich. Wie kann das dann weiter verwendet werden? Ich meine, es muss ja auch sinnvollere Anwendungen geben, au\u00dfer das vorschlagen geeigneter Musik.</p> <p>Experte: Ja klar, zus\u00e4tzlich k\u00f6nnten zuk\u00fcnftige Sprachassistenten auch dazu f\u00e4hig sein, den Grad der Dringlichkeit bzw die Priorit\u00e4t einer Anfrage zu erkennen.  Das basiert dann auf der jeweiligen Stimmung und Betonung des Benutzers.  Dadurch k\u00f6nnten sie noch besser auf die Bed\u00fcrfnisse eingehen und je nachdem ma\u00dfgeschneiderte Antworten oder Aktionen vorschlagen.</p> <p>Ein weiterer potenzieller Anwendungsbereich ist die Integration von Emotionserkennung in Bereichen wie Kundendienst, Therapie oder Bildung.  Durch das Verstehen der Emotionen des Benutzers k\u00f6nnten solche Systeme personalisierte L\u00f6sungen anbieten und die Benutzererfahrung verbessern.</p> <p>Es ist jedoch wichtig, ethische Fragen und den Datenschutz im Hinterkopf zu behalten, wenn es um die Erkennung von Emotionen in der Sprache geht.  Es wird entscheidend sein, transparente und verantwortungsbewusste Praktiken zu entwickeln, um sicherzustellen, dass die Technologie zum Wohle der Benutzer eingesetzt und ihre Privatsph\u00e4re respektiert wird.</p> <p>Also l\u00e4sst sich zusammenfassend sagen, dass Alexa, Siri etc. das im Moment zwar nicht k\u00f6nnen, aber es vermutlich unsere Erfahrung mit Sprachassistenten  in der Zukunft ver\u00e4ndern wird.</p> <p>Moderator: Das ist faszinierend! Es ist beeindruckend, wie weit die Technologie kommen k\u00f6nnte und wie sie dazu beitragen kann, menschliche Erfahrungen besser zu verstehen und darauf zu reagieren.  Vielen Dank f\u00fcr Ihre Zeit und Ihr Wissen, Herr Amuri. </p> <p>Experte:  Sehr gerne! Danke f\u00fcr die Einladung.</p> <p>Moderator:  Wir freuen uns auf weitere spannende Themen in zuk\u00fcnftigen Folgen.</p> <p>Ich war ihr Moderator und bedanke mich f\u00fcrs Zuh\u00f6ren und schaltet doch auch beim n\u00e4chsten Mal wieder ein, wenn es hei\u00dft \"Wie macht denn Technik?\". Tsch\u00fcss. Ende</p>"},{"location":"Themen/Spracherkennung/#talk","title":"Talk","text":"<p>Hier einfach Youtube oder THD System embedden.</p>"},{"location":"Themen/Spracherkennung/#demo","title":"Demo","text":"<p>Link zur Code Demonstration:</p> <p>Link zum Repository: https://mygit.th-deg.de/sh28544/spracherkennung_codedemo</p>"},{"location":"Themen/Spracherkennung/#7-literaturliste","title":"7 Literaturliste","text":"<p>ibm: What is speech recognition</p> <p>PCWorld: Speech Recognition Through the Decades: How We Ended Up With Siri</p> <p>summalinguae: Speech Recognition Software: Past, Present, and Future</p> <p>Wiqas Ghai, Navdeep Singh (2012) Literature Review on Automatic Speech Recognition</p> <p>itbusiness: History of voice recognition: from Audrey to Siri</p> <p>sonix: A short history of speech recognition</p> <p>wikipedia: Speech recognition</p> <p>wikipedia: Timeline of speech and voice recognition</p> <p>B.H. Juang, Lawrence R. Rabiner (2004)Automatic Speech Recognition \u2013 A Brief History of the Technology Development</p> <p>ibm: Shoebox</p> <p>Robert M. Gray (2010) Linear Predictive Coding and the Internet Protocol A survey of LPC and a History of of Realtime Digital Speech on Packet Networks</p> <p>Bruce T. Lowerre (1976) The HARPY Speech Recognition System</p> <p>Dennis H. Klatt (1977) Review of the ARPA Speech Understanding Project</p> <p>Kai-Fu Lee, Hsiao-Wuen Hon, Raj Reddy (1990) An Overview of the SPHINX Speech recognition system</p> <p>Speech and Language Processing. Daniel Jurafsky &amp; James H. Martin</p> <p>Markov Chains</p> <p>M.A.Anusuya, S.K.Katti (2009) Speech Recognition by Machine: A Review</p> <p>Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury (2012) Deep Neural Networks for Acoustic Modeling in Speech Recognition</p> <p>Alex Graves, Navdeep Jaitly Towards End-to-End Speech Recognition with Recurrent Neural Networks</p> <p>William Chan, Navdeep Jaitly, Quoc V. Le, Oriol Vinyals (2015) Listen, Attend and Spell</p> <p>Xuedong Huang (2017) Microsoft researchers achieve new conversational speech recognition milestone</p> <p>wikipedia: Speakable items</p> <p>wikipedia: CMU Sphinx</p> <p>Graeme John Cole (2021) The evolution of speech recognition technology</p> <p>Jonathan Hui (2019) Speech Recognition \u2014 GMM, HMM</p> <p>ibm: Pioneering Speech Recognition</p> <p>ibm: Was ist Deep Learning?</p> <p>[Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, Brian Kingsbury (2012) Deep Neural Networks for Acoustic Modeling in Speech Recognition</p> <p>Stanley Chen, Brian Kingsbury, Lidia Mangu,Daniel Povey, George Saon, Hagen Soltau and Geoffrey ZweigIBM T. J. Watson Research Center, Yorktown Heights (2006) Advances in Speech Transcription at IBM under theDARPA EARS Program</p> <p>X. Binjie, HuJ. (2008) Template Matching</p> <p>Harald Scheidl (2018) An Intuitive Explanation of Connectionist Temporal Classification</p> <p>wikipedia: Hidden Markov model</p> <p>Linhao Dong, Shuang Xu, Bo Xu (2018) Speech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition</p> <p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin (2017) Attention is all you need</p> <p>Oliver C. Ibe (2013) Markov Processes for Stochastic Modeling</p> <p>Speech Recognition: History &amp; Fundamentals</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/","title":"T09 - Dialekte in der Spracherkennung","text":"<p>von Muhammad Daniel BIN MOHD KHIR, Sofia GUTORANSKA, und Jimmy TAN</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#abstract","title":"Abstract","text":"<p>Die automatische Spracherkennung (Automatic Speech Recognition, ASR) verspricht eine flie\u00dfende, nat\u00fcrlichsprachliche Schnittstelle f\u00fcr die Interaktion von Menschen mit ihren digitalen Gegen\u00fcbern. Trotz bemerkenswerter Fortschritte in diesem Bereich gibt es immer noch Herausforderungen, die gemeistert werden m\u00fcssen. Eine davon ist das einzigartige Problem der dialektalen Variet\u00e4ten in der gesprochenen Sprache. Dieser Bericht befasst sich mit dem speziellen Thema der Dialekte in der Spracherkennung und beleuchtet die Schwierigkeiten, mit denen ASR-Systeme konfrontiert sind, wenn sie mit dialektalen Eingaben umgehen m\u00fcssen, sowie m\u00f6gliche Verfahren zum Umgang mit diesen Problemen.</p> <p>Dieser Bericht wird von einem Podcast, einem Fachvortrag und einer Code-Demo begleitet. Der Podcast f\u00fchrt den Leser in das Thema ein und gibt einen kurzen \u00dcberblick dar\u00fcber, was Dialekte sind und warum derzeitige Spracherkennungssysteme Probleme mit ihnen haben. Er beleuchtet au\u00dferdem, warum dies ein wichtiges Problem ist, das gel\u00f6st werden muss.</p> <p>Der Fachvortrag befasst sich ausf\u00fchrlich mit den Techniken und Ans\u00e4tzen, die zur Behandlung von Dialekten verwendet werden, wie z. B. Daten-Augmentierung und Domain-Anpassung. Dar\u00fcber hinaus wird ein kurzer \u00dcberblick \u00fcber den Stand der Technik bei ASR-Modellen wie Whisper und XLSR vermittelt.</p> <p>Schlie\u00dflich wird in der Code-Demo der Vorgang zum Fine-Tuning eines Whisper-Modells f\u00fcr den schweizerdeutschen Z\u00fcrcher Dialekt veranschaulicht.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#1-einleitung","title":"1 Einleitung","text":"<p>Gespr\u00e4ch mit dem Sprachassistent Siri</p> <p>\u2014 Hej Siri, tu\u00e4 mal em Alex ahl\u00fctta!1</p> <p>\u2014 Entschuldigung, das habe ich leider nicht verstanden.</p> <p>Das ist leider h\u00e4ufig der Fall, wenn man versucht, mit modernen Sprachassistenten in einem Dialekt zu reden. Sie sind einfach nicht in der Lage, mit nicht standardisierter Sprache umzugehen. Da es sich bei Dialekten um eine Nische handelt, fehlt es an Forschung und industriereifen L\u00f6sungen zur L\u00f6sung dieses Problems. Aber was genau sind Dialekte, und warum haben unsere derzeitigen KI-Systeme Probleme mit ihnen?</p> <p>Dialekte sind Varianten einer Sprache, die in einer bestimmten Region \u00fcberwiegend gesprochen werden. In den deutschsprachigen Teilen der Welt ist Hochdeutsch die Standardsprache - sie wird als offizielle Regierungssprache und als Sprache in den Medien verwendet. Varianten wie Bayerisch und Z\u00fcrit\u00fc\u00fctsch sind Dialekte, die sich stark von den Standardsprachen unterscheiden. So sehr, dass ein Muttersprachler oft kein einziges Wort davon verstehen kann. Dialekte k\u00f6nnen sich in folgenden Aspekten von Standardsprachen unterscheiden:</p> <ul> <li>lexikalisch - der Dialekt verwendet andere W\u00f6rter f\u00fcr ein Objekt; ein abweichender Wortschatz</li> <li>akustisch - die Aussprache der W\u00f6rter ist anders</li> <li>orthografisch - dieselben W\u00f6rter werden in einem Dialekt anders geschrieben</li> </ul> <p>Auf der ganzen Welt gibt es unz\u00e4hlige Dialekte. Dieser Artikel wird sich nur auf Dialekte im Schweizerdeutschen konzentrieren, da diese Dialekte von einer bedeutenden Gruppe von Menschen in der DACH-Region gesprochen werden und auf diesem Gebiet viel geforscht wird.</p> <p>In den folgenden Abschnitten werden einige Begriffe erl\u00e4utert, die zum Verst\u00e4ndnis der weiteren Ausarbeitung notwendig sind.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#11-automatische-spracherkennung","title":"1.1 Automatische Spracherkennung","text":"<p>Die automatische Spracherkennung (eng: Automatic Speech Recognition, kurz ASR) ist der rechnerische Prozess, gesprochene Sprache in geschriebenen Text umzuwandeln. Es ist auch unter vielen anderen Namen bekannt, wie z. B. Speech to Text. ASR ist heute vor allem in Sprachassistenten wie Siri oder Alexa zu finden. Diese im Handy eingebetteten Agenten nutzen ASR, um Sprachbefehle des Benutzers zu erkennen, um bestimmte Aktionen auszuf\u00fchren. Beispielsweise l\u00e4sst sich mit einem Sprachbefehl einen Kontakt anrufen oder das Wetter abfragen. Ein weiterer Anwendungsfall f\u00fcr ASR ist die automatische Erstellung von Live-Transkriptionen f\u00fcr Videos auf Streaming-Seiten wie YouTube.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#12-schweizerdeutsch","title":"1.2 Schweizerdeutsch","text":"<p>Deutsch ist die dominierende Sprache der vier Hauptsprachen in der Schweiz, wobei 62% der Bev\u00f6lkerung im Jahr 2020 Varianten vom Deutschen sprechen2. Es ist wichtig, zwischen Schweizerdeutsch und Hochdeutsch zu unterscheiden.</p> <p>Schweizerdeutsch ist kein einziger Dialekt von Hochdeutsch, sondern eine Gruppe von Dialekten, die in der Schweiz gesprochen werden. Die schweizen Mundarten k\u00f6nnen grob nach Kantonen (Verwaltungsregionen) unterteilt werden. Beispielsweise wird B\u00e4rnd\u00fctsch in Bern gesprochen, und Z\u00fcrit\u00fc\u00fctsch in Z\u00fcrich.</p> <p>Schweizerdeutsch wird haupts\u00e4chlich gesprochen und koexistiert mit Hochdeutsch, das die Hauptform der schriftlichen Kommunikation ist. Da es vor allem gesprochen wird, hat es keine standardisierte Rechtschreibung, und jeder Dialekt unterscheidet sich in Phonetik, Grammatik und Wortschatz von anderen Dialekten. Der Berner Dialekt zum Beispiel wandelt das hochdeutsche l in ein u um. Das Wort alt wird aut ausgesprochen3.</p> <p>Da sich diese Dialekte auf die Region konzentrieren, gibt es im Vergleich zum Hochdeutsch oder Englisch nur wenige Korpora und Daten zu ihnen, was das Schweizerdeutsch zu einer Low Resource Language macht. Aus diesem Grund und wegen der unz\u00e4hligen regionalen Variationen ist die automatische Spracherkennung f\u00fcr Schweizerdeutsch eine fortlaufende Herausforderung.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#2-methoden","title":"2 Methoden","text":"<p>Es gibt jedoch einige Methoden zum Umgang mit Low Resource Languages wie Schweizerdeutsch beim Training von ASR-Modellen. Diese werden im Folgenden dargelegt.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#21-datenerfassung-und-annotation","title":"2.1 Datenerfassung und Annotation","text":"<p>Um robuste und genaue ASR-Modelle f\u00fcr Dialekte zu trainieren, werden gro\u00dfe Mengen an annotierten Daten ben\u00f6tigt. Die Datenerfassung in gro\u00dfem Umfang umfasst verschiedene Sprecher aus dem spezifischen dialektischen Hintergrund und erm\u00f6glicht es dem Modell, die Variationen und Nuancen zu erfassen, die in dieser spezifischen dialektischen Sprache vorhanden sind.</p> <p>Zu diesen Daten geh\u00f6ren Audioaufnahmen sowie deren Transkriptionen und dialektische Labels, die f\u00fcr das Training und die Bewertung der Spracherkennungsmodelle wichtig sind. Die Audioaufnahmen k\u00f6nnen aus verschiedenen Kan\u00e4len stammen, einschlie\u00dflich \u00f6ffentlicher Sprachdatens\u00e4tze, Audioarchiven, Interviews oder durch Aufnahme spezifischer Dialektsprecher in einer kontrollierten Umgebung. Die Datenannotation erfolgt oft auf zwei Arten, manuell und automatisch.</p> <p>Die manuelle Annotation erfordert, dass der Muttersprachler in diesem speziellen Dialekt die Audiodaten manuell transkribiert, was sehr zeit- und ressourcenintensiv ist, aber sehr genau. Die automatische Transkription hingegen verwendet bereits trainierte Modelle f\u00fcr die automatische Transkription. Dieser Ansatz ist jedoch oft nicht machbar im Falle von Dialekten. Es gibt noch nicht viele Open-Source-L\u00f6sungen, die Dialekte genau transkribieren k\u00f6nnen, und vorhandene Modelle, die auf Standardsprachen trainiert wurden, liefern ungenaue Ergebnisse f\u00fcr dialektische Daten. Aus diesen Gr\u00fcnden werden manuelle Transkriptionen in diesem Kontext oft bevorzugt.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#22-daten-augmentierung","title":"2.2 Daten-Augmentierung","text":"<p>Um die gesammelten Daten zu erweitern, kann man die Technik der Daten Augmentierung (eng: Data Augmentation) nutzen. Bei diesem Ansatz werden zus\u00e4tzliche Trainingsbeispiele durch Anwendung verschiedener Transformationen auf die vorhandenen Daten generiert. Diese Transformationen k\u00f6nnen Variationen in Geschwindigkeit, Tonh\u00f6he oder Hintergrundger\u00e4uschen sein4. Indem das ASR-Modell einem vielf\u00e4ltigeren Datensatz ausgesetzt wird, wird es widerstandsf\u00e4higer gegen verschiedene dialektische Variationen und generalisiert besser auf ungesehene Datenpunkte.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#23-domain-anpassung","title":"2.3 Domain-Anpassung","text":"<p>Die Domain-Anpassung (eng: Domain Adaptation) befasst sich mit der Anpassung des ASR-Modells an eine spezifische Dom\u00e4ne oder Anwendung, in der die Daten von den urspr\u00fcnglichen Trainingsdaten abweichen k\u00f6nnten5. Im Kontext von Dialekten wird die Domain-Anpassung entscheidend, da sich die akustischen Eigenschaften, der Wortschatz und die sprachlichen Muster erheblich von der Standardsprache unterscheiden k\u00f6nnen, auf der das urspr\u00fcngliche Modell trainiert wurde.</p> <p>Techniken zur Domain-Anpassung zielen darauf ab, die L\u00fccke zwischen den dialektischen Daten und dem vortrainierten Modell mithilfe von Transfer-Learning zu \u00fcberbr\u00fccken. Das vortrainierte Modell, das allgemeine akustische und linguistische Darstellungen aus High Resource Languages gelernt hat, kann als Ausgangspunkt f\u00fcr das Training des ASR-Modells auf dialektischen Daten verwendet werden.</p> <p>Durch die Initialisierung des Modells mit diesen vortrainierten Gewichten kann es von dem Wissen profitieren, das aus der Standardsprache gelernt wurde, und sich dann durch weiteres Training an den spezifischen Dialekt anpassen. Aufgrund dessen kann man eine kleinere Menge an in-domain-dialektischen Daten nutzen, um das Modell zu optimieren. Dies hilft dem Modell, sich besser auf die spezifischen dialektischen Variationen einzustellen und verbessert seine Leistung in diesem Bereich.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#3-stand-der-forschung","title":"3 Stand der Forschung","text":"<p>Im folgenden Abschnitt werden Ressourcen f\u00fcr den Umgang mit Schweizerdeutsch in der ASR, wie Datens\u00e4tze und Modelle, vorgestellt.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#31-datensatze","title":"3.1 Datens\u00e4tze","text":"<p>Modelle f\u00fcr maschinelles Lernen sind \"garbage in, garbage out\". Daher ist es von gr\u00f6\u00dfter Bedeutung, gute Daten zu haben, aus denen die Modelle lernen k\u00f6nnen. In diesem Teil werden die verf\u00fcgbaren schweizerdeutschen Datens\u00e4tze vorgestellt. Alle genannten Datens\u00e4tze sind \u00f6ffentlich zug\u00e4nglich.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#schweizer-dialektsammlung-2022","title":"Schweizer Dialektsammlung (2022)","text":"<p>Die Schweizer-Dialektsammlung6 umfasst insgesamt 200 Stunden gesprochenes Audiomaterial von 4000 Sprechern verschiedener Dialekte, Altersgruppen und Geschlechter. Die Audiodaten sind mit Sprecherangaben und Transkriptionen in Hochdeutsch annotiert. Die gesprochenen Texte stammen von S\u00e4tzen aus dem deutschen Common Voice Corpus sowie aus Schweizer Nachrichtenquellen.</p> <p>Inspiriert durch das Mozilla CommonVoice Projekt haben die Kuratoren des Korpus ein Open-Source-Tool verwendet, um die Aufnahme und Validierung der gesprochenen Audiodaten per Crowdfunding zu erm\u00f6glichen. Die Datensammlung wurde stark beworben und spielerisch gestaltet, da die Autoren mit Schweizer Medien zusammenarbeiteten, um eine gro\u00dfe Reichweite zu erzielen. Es wurden Wettbewerbe veranstaltet, um zur Teilnahme zu ermutigen, wie z.B. ein kantons\u00fcbergreifender Wettbewerb, bei dem es darum ging, welcher Kanton die meisten Aufnahmen erstellen konnte.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#swissdial-2021","title":"SwissDial (2021)","text":"<p>SwissDial7 ist ein paralleles Audiokorpus, das 8 Hauptdialekte abdeckt, wovon jeder Dialekt mindestens 3 Stunden an Audiodaten enth\u00e4lt. Parallel bedeutet, dass das Korpus gesprochene \u00c4u\u00dferungen von den identischen S\u00e4tzen je Dialekt enth\u00e4lt. Jeder Datenpunkt ist mit entsprechenden Transkriptionen sowohl im Dialekt als auch im Hochdeutschen annotiert. Die Dom\u00e4ne erstreckt sich \u00fcber ein breites Spektrum von Themen wie Kultur, Wirtschaft, Wissenschaft und Politik. Das Korpus wurde erstellt, indem f\u00fcr jeden Dialekt ein Annotator ausgew\u00e4hlt und professionelle Aufnahmen durchgef\u00fchrt wurden, um ihre Stimmen einzufangen.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#honorable-mentions","title":"Honorable Mentions","text":"<p>Die oben genannten Datens\u00e4tze stellen aufgrund ihrer Breite und ihres Umfangs die aktuelle Cr\u00e8me de la Cr\u00e8me dar. Sie k\u00f6nnen leicht f\u00fcr die Entwicklung datengetriebener Schweizerdeutscher NLP-Anwendungen verwendet werden. Andere verf\u00fcgbare Datens\u00e4tze wie das Swiss Parliament Corpus8, ArchiMob9 und Radio Rottu Oberwallis10 sind entweder fachspezifisch, enthalten keine hochdeutschen Transkripte oder haben zu wenig Daten f\u00fcr ein optimales Modell-Finetuning.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#32-modelle","title":"3.2 Modelle","text":"<p>In diesem Abschnitt werden zwei g\u00e4ngige End-to-End-Modelle, Whisper und XLSR, kurz erl\u00e4utert. End-to-End-Modelle verwenden Audiodaten als Eingabe und geben transkribierten Text aus dem Audiomaterial aus. Dies macht sie f\u00fcr Praktiker leicht anwendbar, da Zwischenschritte wie das Chunking von Audiodaten in 30-Sekunden-Intervallen und die Generierung von Log-Mel-Spektrogrammen in das Modelltraining integriert sind und w\u00e4hrend der Inferenz nicht separat durchgef\u00fchrt werden m\u00fcssen.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#whisper","title":"Whisper","text":"<p>Whisper11 ist ein Sequence to Sequence Spracherkennungs- und \u00dcbersetzungsmodell, das von OpenAI ver\u00f6ffentlicht wurde. Es wurde mit 680.000 Stunden gelabelter Audiotranskriptionsdaten trainiert, von denen 243.000 nicht-englische Daten sind. Dank der riesigen Menge an Trainingsdaten ist Whisper robust gegen\u00fcber Akzenten, Hintergrundger\u00e4uschen und Fachsprache. Es zeichnet sich durch Zero-Shot-Learning aus und erfordert oft kein vorheriges Fine-Tuning f\u00fcr dom\u00e4nenspezifische Aufgaben. Aufgrund seiner hervorragenden Leistung und Benutzerfreundlichkeit wurde Whisper in beliebte kommerzielle Produkte wie Auphonic, ein Podcasting-Tool mit KI, integriert.</p> <p>Der Archiktekur von Whisper ist unten abgebildet12.</p> <p> </p> Fig. 1: Der Encoder-Decoder-Transformer Architektur von Whisper."},{"location":"Themen/T09_Dialekte_Spracherkennung/#xlsr","title":"XLSR","text":"<p>XLSR13 basiert auf Wav2Vec 2.0 und ist ein mehrsprachiges Modell, das auf 60.000 Stunden ungelabelter Audiodaten trainiert wurde. Es f\u00fchrt un\u00fcberwachte maskierte Vorhersagen durch, d.h. es lernt Zwischenzuordnungen von Sprachaudio zu versteckten Zust\u00e4nden im Modell. XLSR muss finetuned werden, um kontinuierliche Zuordnungen f\u00fcr Transkriptionen zu erzeugen.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#33-kommerzielle-dienste","title":"3.3 Kommerzielle Dienste","text":"<p>Es gibt auch Unternehmen, die Sprache-zu-Text-Dienste als Online-Dienste f\u00fcr Kunden anbieten. Diese werden im Anschluss beschrieben.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#cloud-anbieter","title":"Cloud-Anbieter","text":"<p>Die drei gro\u00dfen Cloud-Anbieter (AWS, Azure und GCP) bieten alle kostenpflichtige APIs f\u00fcr ihre Text-to-Speech-Dienste an. Die APIs richten sich jedoch eher an Softwareentwickler, um ASR-Produkte mit ihren Diensten zu entwickeln, und sind nicht auf Endbenutzer ausgerichtet. Alle unterst\u00fctzen die Transkription von schweizerdeutschem Audio.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#toggl","title":"T\u00f6ggl","text":"<p>T\u00f6ggl ist ein kostenpflichtiger TTS-Dienst von recapp, einem Schweizer Unternehmen, das sich auf die Spracherkennung von Dialekten spezialisiert hat. Es handelt sich um eine Webanwendung, die aus hochgeladenen Audiodateien Texttranskriptionen erstellt. T\u00f6ggl richtet sich an Studenten und Journalisten, die in der Schweiz leben. Gem\u00e4ss den AGB d\u00fcrfen n\u00e4mlich nur in der Schweiz wohnhafte Personen den Dienst nutzen. Im Gegensatz zu den Cloud-Anbietern ist T\u00f6ggl datenschutzbewusst und bietet ma\u00dfgeschneiderte On-Premise-L\u00f6sungen f\u00fcr Unternehmen an.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#4-anwendungen","title":"4 Anwendungen","text":"<p>Die oben dargestellten vortrainierten Modelle zeigen beeindruckende Ergebnisse. Es lohnt sich jedoch zu pr\u00fcfen, ob durch Fine-Tuning eines bereits vorhandenen Modells mit einem der oben beschriebenen schweizerdeutschen Datens\u00e4tze noch Verbesserungen erzielt werden k\u00f6nnen. Eine m\u00f6gliche Verbesserung w\u00e4re, dass das Modell in der Lage ist, Transkriptionen in Schweizerdeutsch anstelle von Hochdeutsch auszugeben.</p> <p>Dieser Teil erl\u00e4utert die Schritte, die f\u00fcr das Fine-Tuning eines Whisper-Modells erforderlich sind. Anschlie\u00dfend wird das finetuned Whisper-Modell mit den nicht-finetuned Versionen der Modelle Whisper und XLSR verglichen. Der ganze Vorgang basiert auf einer Anleitung von HuggingFace14.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#41-vorbereitende-schritte","title":"4.1 Vorbereitende Schritte","text":"<p>Bevor das Modell tats\u00e4chlich finetuned wird, m\u00fcssen einige vorbereitende Schritte durchgef\u00fchrt werden. Dazu geh\u00f6ren das Laden und Verarbeiten des Datensatzes sowie das Laden einiger Hilfsklassen. Dieser Prozess wird in den folgenden Unterabschnitten beschrieben.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#laden-des-datensatzes","title":"Laden des Datensatzes","text":"<p>F\u00fcr das Finetuning wurde der SwissDial-Datensatz gew\u00e4hlt, da er ein grosses Audio-Korpus mit schweizerdeutschen und hochdeutschen Transkriptionen enth\u00e4lt. Dies erm\u00f6glicht einen einfachen Vergleich. SwissDial wurde bereits oben beschrieben.</p> <p>Da der Datensatz 8 verschiedene Dialekte enth\u00e4lt, wurde nur der Z\u00fcrcher Dialekt (ZH) f\u00fcr das Finetuning ausgew\u00e4hlt, da er die meisten Audiodaten enth\u00e4lt, insgesamt 4,55 Stunden. Die Daten wurden in einem Verh\u00e4ltnis von 80:20 in Trainings- und Testdatens\u00e4tze aufgeteilt, was zu 3,64 Stunden Trainingsdaten und den verbleibenden 0,91 Stunden f\u00fcr Tests f\u00fchrt. Ein Validierungs-Split wurde nicht durchgef\u00fchrt, da nur wenige Daten zur Verf\u00fcgung standen.</p> <p>Der Datensatz muss zun\u00e4chst verarbeitet werden. Er besteht aus einer Reihe von .wav-Audiodateien, die jeweils einen anderen Satz repr\u00e4sentieren. Zus\u00e4tzlich gibt es eine JSON-Datei, in der die Metadaten gespeichert sind. Die Metadaten enthalten Transkriptionen sowohl im Dialekt als auch im Hochdeutsch, sowie weitere Informationen wie die Dom\u00e4ne des Satzes.</p> <p>Mit der Python-Bibliothek pandas wurden die Metadaten in einen DataFrame importiert, manipuliert und bereinigt. Der endg\u00fcltige DataFrame bestand aus 3 Spalten: Die hochdeutsche Transkription, die Z\u00fcrcher Dialekttranskription, sowie der Pfad zur entsprechenden Audiodatei. Dieser finale DataFrame umfasst die Metadaten unseres Korpus.</p> <p> </p> Fig. 2: Der finale, bereinigte DataFrame. <p>Mit Hilfe der HuggingFace datasets Bibliothek werden die Metadaten und die Audiodateien zu einem einzigen Audio-Dataset-Objekt zusammengefasst, um sie sp\u00e4ter leichter verarbeiten zu k\u00f6nnen.</p> <pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\"audiofolder\", data_dir=\"../dataset/zh/\")\n</code></pre>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#merkmalsextraktion","title":"Merkmalsextraktion","text":"<p>Sprachsignale k\u00f6nnen als eindimensionale Arrays betrachtet werden, die sich mit der Zeit ver\u00e4ndern. Jeder Wert entspricht der Amplitude des Signals zu einem bestimmten Zeitpunkt. Kontinuierliche Sprache enth\u00e4lt eine unendliche Anzahl von Amplitudenwerten. F\u00fcr den Computer ist dies ein Problem, da er diskrete Signale erwartet.</p> <p>Um dieses Problem zu l\u00f6sen, muss das Sprachsignal in festen Zeitschritten gesampelt werden. Das Intervall, in dem die Audiodaten abgetastet werden, wird als Abtastrate (eng: Sample Rate) bezeichnet. Diese wird normalerweise in Samples pro Sekunde oder Hertz (Hz) gemessen.</p> <p>Die Abtastrate der Audiodaten muss mit der von Whisper erwarteten Rate identisch sein. Andernfalls kann es zu unerwarteten Ergebnissen kommen, da Audiosignale mit unterschiedlichen Abtastraten unterschiedlich verteilt werden. F\u00fcr Whisper werden f\u00fcr die Trainingsdaten Audiodaten mit einer Abtastrate von 16 kHz ben\u00f6tigt.</p> <p>Die HuggingFace Transformers Bibliothek bietet praktische Hilfsklassen f\u00fcr das Finetuning und Training eines Whisper-Modells. Hier wird die <code>WhisperFeatureExtractor</code> Klasse f\u00fcr folgende Aufgaben verwendet:</p> <ul> <li>Resampling des Eingangssignals auf 16 kHz.</li> <li>Auff\u00fcllen oder K\u00fcrzen von Audiosamples auf eine feste L\u00e4nge von 30 Sekunden.</li> <li>Umwandlung von Audio-Samples in Log-Mel-Spektrogramme.</li> </ul> <p>Log-Mel-Spektrogramme sind eine visuelle Darstellung der Frequenz eines Signals und sind das, was Whisper als Eingabe erwartet. Ein beispielhaftes Spektrogramm ist unten abgebildet15.</p> <p> </p> Fig. 3: Die Umwandlung von einem Audiosignal in ein Log-Mel-Spektrogramm."},{"location":"Themen/T09_Dialekte_Spracherkennung/#tokenisierung-von-text","title":"Tokenisierung von Text","text":"<p>Whisper gibt eine Serie von Text-Tokens aus, die f\u00fcr Menschen unverst\u00e4ndlich sind. Diese Zahlenreihen stellen Indizes f\u00fcr bestimmte W\u00f6rter im internen W\u00f6rterbuch von Whisper dar und sind byte-pair kodiert. Diese Kodierungen werden mit Hilfe eines so genannten Tokenizers in echten Text umgewandelt.</p> <pre><code>&gt;&gt;&gt; encoded_tokens\n[32, 84, 368, 40512, 271, 948, 20731, 302, ... 13]\n\n&gt;&gt;&gt; decoded_tokens\nAu de Bewis enthaltet analytischi Schw\u00e4che wo erscht sp\u00f6ter h\u00e4nd ch\u00f6ne besitigt werde.\n</code></pre> <p>Weitere Informationen zu Byte-Paar-Kodierungen gibt es in diesem HuggingFace-Tutorial.</p> <p>Hier erledigt die Hilfsklasse <code>WhisperTokenizer</code> genau das.</p> <pre><code>from transformers import WhisperTokenizer\n\ntokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"de\", task=\"transcribe\")\n</code></pre> <p>Um zu kontrollieren, ob der Tokenizer korrekt arbeitet, kann man einen bestimmten Datenpunkt heraussuchen und den Eingabe-String mit seiner tokenisierten und dekodierten Variante vergleichen.</p> <pre><code>input_str = dataset_tt[\"train\"][1][\"ch_zh\"]\nlabels = tokenizer(input_str).input_ids\ndecoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n\n&gt;&gt;&gt; input_str\nDe Nils Puk h\u00e4t ihn lut ahgschroue und gschw\u00f6rt sich zr\u00e4che.\n\n&gt;&gt;&gt; decoded_str\nDe Nils Puk h\u00e4t ihn lut ahgschroue und gschw\u00f6rt sich zr\u00e4che.\n</code></pre>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#kombination-von-feature-extraktor-und-tokenizer","title":"Kombination von Feature Extraktor und Tokenizer","text":"<p>Der Feature Extractor und der Tokenizer k\u00f6nnen zur einfacheren Verwendung in einer einzigen Prozessor-Klasse kombiniert werden. Dies wird mit der Utility-Klasse WhisperProcessor gemacht. Dadurch wird das sp\u00e4tere Training vereinfacht, da nur der Prozessor und das Modell verfolgt werden m\u00fcssen.</p> <pre><code>from transformers import WhisperProcessor\n\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"de\", task=\"transcribe\")\n</code></pre>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#aufbereitung-der-daten","title":"Aufbereitung der Daten","text":"<p>Hier kommt der zuvor vorbereitete Audio-Dataset ins Spiel.</p> <p>Jeder Datenpunkt enth\u00e4lt Folgendes:</p> <ul> <li>den Pfad zur Audiodatei.</li> <li>ein eindimensionales Array, das die Audiodaten repr\u00e4sentiert.</li> <li>Sample-Rate (22kHz).</li> <li>Transkription in Hochdeutsch.</li> <li>Transkription im Z\u00fcrcher Dialekt.</li> </ul> <pre><code>&gt;&gt;&gt; single_data_point\n\n{\n  \"audio\": {\n    \"path\": \"dataset/zh/ch_zh_2239.wav\",\n    \"array\": array([1.50373509e-07, ..., -3.17052582e-07]),\n    \"sampling_rate\": 22050\n  },\n  \"de\": \"Der Nis Puk schrie ihn laut an und schwor, sich zu r\u00e4chen.\",\n  \"ch_zh\": \"De Nils Puk h\u00e4t ihn lut ahgschroue und gschw\u00f6rt sich zr\u00e4che.\"\n}\n</code></pre> <p>Jeder Datenpunkt muss so verarbeitet werden, dass:</p> <ul> <li>die Sample-Rate auf 16 kHz reduziert wird.</li> <li>das Audio-Array in ein Log-Mel-Spektrogramm umgewandelt wird.</li> <li>die Z\u00fcrcher Dialekttranskription tokenisiert wird.</li> </ul> <p>Die Funktion <code>prepare_dataset</code> erledigt genau das. Zus\u00e4tzlich erm\u00f6glicht die <code>map</code> Funktion die parallele Verarbeitung des Datensatzes, was die Vorverarbeitung beschleunigt.</p> <pre><code>def prepare_dataset(batch):\n    # load and resample audio data from 22050Hz to 16kHz\n    audio = batch[\"audio\"]\n\n    # compute log-Mel input features from input audio array\n    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n\n    # encode taget text to label ids\n    batch[\"labels\"] = tokenizer(batch[\"ch_zh\"]).input_ids\n\n    return batch\n\ndataset_after_prep = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"train\"], num_proc=4)\n</code></pre>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#definieren-eines-data-collators","title":"Definieren eines Data Collators","text":"<p>Ein Data Collator fasst Inputdaten zu einem Batch zusammen, der sp\u00e4ter f\u00fcr das Training verwendet wird. Wenn der Collator aufgerufen wird, werden die oben beschriebenen Vorverarbeitungsschritte f\u00fcr jeden Batch durchlaufen. Anstatt den Feature Extractor und den Tokenizer separat zu verwenden, kann hier der kombinierte WhisperProcessor genutzt werden. Es gibt auch einige zus\u00e4tzliche Schritte f\u00fcr die Umwandlung in PyTorch-Tensoren und das Padding von Labels.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#definieren-einer-evaluierungsmetrik","title":"Definieren einer Evaluierungsmetrik","text":"<p>Zur Bewertung der Leistung des Modells wird die Word-Error-Rate (WER) verwendet. Die WER ist eine g\u00e4ngige Metrik zur Beurteilung von ASR-Modellen und wird wie folgt berechnet:</p> \\[ WER = \\frac{S + D + I}{N} \\] <p>Wobei:</p> <ul> <li>S - Anzahl der substituierten W\u00f6rter (in der Ausgabe ersetzte W\u00f6rter)</li> <li>D - Anzahl der gel\u00f6schten W\u00f6rter (in der Ausgabe fehlende W\u00f6rter)</li> <li>I - Anzahl der eingef\u00fcgten W\u00f6rter (in der Ausgabe hinzugef\u00fcgte W\u00f6rter)</li> <li>N - Gesamtzahl der W\u00f6rter</li> </ul> <p>Je niedriger der WER-Wert, desto besser ist die Leistung des Modells, da es weniger Inkonsistenzen zwischen der generierten Ausgabe und der Ground Truth gibt.</p> <p>Die HuggingFace evaluate Bibliothek stellt eine fertige Implementierung der WER-Metrik zur Verf\u00fcgung. Diese kann dann in eine Hilfsfunktion, <code>compute_metrics</code>, verpackt werden, die die Prognosen verarbeitet und das endg\u00fcltige WER berechnet.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#42-training","title":"4.2 Training","text":"<p>Vor dem Training sind noch einige Schritte notwendig. Zun\u00e4chst muss ein vortrainierter Checkpoint des Whisper-Modells aus dem Hugging Face Hub geladen werden. Danach m\u00fcssen einige Trainingsargumente definiert werden, z. B. die Lernrate, die Schritte f\u00fcr das Checkpointing und der Speicherplatz f\u00fcr die Logs.</p> <p>Schlie\u00dflich k\u00f6nnen die Trainingsargumente, das vortrainierte Modell, der Datensatz, der Data Collator und der Metrikrechner an eine Trainer Klasse \u00fcbergeben werden.</p> <p>Das Training wird dann einfach mit einem Aufruf von <code>trainer.train()</code> durchgef\u00fchrt.</p> <p>Je nach Hardware kann das Training etwa 10-20 Stunden dauern.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#43-auswertung","title":"4.3 Auswertung","text":"<p>Das finetuned Modell f\u00fcr den Z\u00fcrcher Dialekt ist auf dem Hugging Face Hub verf\u00fcgbar.</p> <p>Vor dem Finetuning lag der WER auf dem Testdatensatz bei 77%, was ziemlich hoch ist. Nach dem Finetuning sinkt dieser Wert jedoch auf einer wesentlich handlicheren 30%.</p> <p>Zur Erinnerung: Je niedriger der WER, desto besser, da das Modell genauere Transkriptionen liefert. Etwa 30% der mit dem finetuned Modell ermittelten Ergebnisse stimmen nicht mit dem Ground Truth \u00fcberein.</p> <p>Der WER f\u00fcr die Trainingsdaten liegt jedoch bei sehr niedrigen 9,41%. Dies k\u00f6nnte bedeuten, dass Overfitting stattgefunden hat und das Modell schlecht generalisieren kann, so dass es bei fremden, vorher ungesehenen Daten schlechter abschneidet.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#44-modellvergleich","title":"4.4 Modellvergleich","text":"<p>In diesem Abschnitt werden die generierten Transkriptionen des finetuned Whisper-Modells mit zwei anderen Modellen verglichen: dem kleinen Whisper Modell und einem XLSR-Modell, das f\u00fcr Deutsch finetuned wurde16. Da w\u00e4hrend des Trainings kein Holdout-Set definiert wurde, wurden 5 zuf\u00e4llige Datenpunkte aus einem anderen Dialekt (Graub\u00fcnden, GR) zum Vergleich genommen.</p> <p> </p> Fig. 4: Tabelle zum Vergleich von 3 Modellen. <p>Die obenstehende Tabelle zeigt die 5 Datenpunkte, zusammen mit den Ground Truth Labels und den vorhergesagten Transkriptionen. Die zweite bis vierte Spalte zeigen die Ground Truth Labels f\u00fcr Hochdeutsch, den Z\u00fcrcher Dialekt und den Graub\u00fcndner Dialekt, w\u00e4hrend die letzten drei Spalten die Vorhersagen der jeweiligen Modelle f\u00fcr den Graub\u00fcndner Dialekt enthalten.</p> <p>Wenn man einen anderen Dialekt zum Vergleich benutzt, gibt es einige Punkte, die zu beachten sind. Erstens sind einige lexikalische Abweichungen zu beobachten - im Z\u00fcrcher Dialekt wird \"Momentan\" verwendet, w\u00e4hrend das entsprechende Wort in Graub\u00fcnden stattdessen \"Derziit\" hei\u00dft. Auch in der Schreibweise gibt es Unterschiede, zum Beispiel beim Wort \"nicht\", das in Z\u00fcrich als \"n\u00f6d\" und in Graub\u00fcnden als \"nit\" transkribiert wird.</p> <p>Das kleine Whisper-Modell ist in der Lage, weitgehend genaue Transkriptionen zu liefern, die die grobe Bedeutung des Satzes erfassen. Allerdings wird S\u00e4tze auf Hochdeutsch transkribiert, da noch kein Fine-Tuning f\u00fcr Schweizerdeutsch vorgenommen wurde. Nach dem Fine-Tuning des Modells stimmen die Vorhersagen besser mit dem Label \u00fcberein und k\u00f6nnen die meisten Besonderheiten des Dialekts ber\u00fccksichtigen. Dies korrespondiert mit einer niedrigeren Word-Error-Rate.</p> <p>Das XLSR-Modell (in der Tabelle als Wave2Vec bezeichnet) weist die meisten Unterschiede auf. Der Tokenizer des Modells spielt hier die entscheidende Rolle. Der Tokenizer von Whisper kann Satzzeichen und Gro\u00dfschreibung verarbeiten, w\u00e4hrend der Tokenizer von XLSR dies nicht tut. Dies f\u00fchrt zu Vorhersagen, die weniger mit den Labels \u00fcbereinstimmen, und somit zu einem h\u00f6heren WER. Eine m\u00f6gliche Verbesserung w\u00e4re jedoch, auch das XLSR-Modell finezutunen, um festzustellen, wie es dann abschneidet.</p> <p>Insgesamt sind alle drei Modelle in der Lage, einen schweizerdeutschen Dialekt mit unterschiedlicher Genauigkeit zu transkribieren. Das finetuned Whisper-Modell zeigt die F\u00e4higkeit, die besonderen Merkmale des Dialekts zu erfassen.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#5-fazit","title":"5 Fazit","text":"<p>Zusammenfassend l\u00e4sst sich sagen, dass Dialekte erhebliche Herausforderungen f\u00fcr automatische Spracherkennungssysteme darstellen, da sie von den Standardsprachen abweichen. Die einzigartigen Eigenschaften dialektaler Variationen, wie lexikalische, akustische und orthografische Unterschiede, stellen aktuelle ASR-Modelle vor Schwierigkeiten. Allerdings werden diese Herausforderungen durch laufende Forschung und Fortschritte auf diesem Gebiet angegangen.</p> <p>Zu den Key-Takeaways geh\u00f6rt die deutliche Verbesserung, die durch das Finetuning des Whisper-Modells speziell f\u00fcr schweizerdeutsche Dialekte erzielt wurde. Durch das Training des Modells anhand des SwissDial-Datensatzes, der annotierte Audioaufnahmen und Transkriptionen sowohl in Schweizerdeutsch als auch in Hochdeutsch enth\u00e4lt, zeigte das finetuned Whisper-Modell eine bemerkenswerte Reduzierung der Word-Error-Rate. Diese Verbesserung weist darauf hin, dass das finetuned Modell besser in der Lage ist, die einzigartigen phonetischen und sprachlichen Merkmale von schweizerdeutschen Dialekten zu erfassen.</p> <p>Dar\u00fcber hinaus spielen \u00f6ffentlich verf\u00fcgbare Datens\u00e4tze wie die Schweizer Dialektsammlung und SwissDial eine wichtige Rolle zur Verbesserung von ASR-Modellen bei. Besonders umfangreich ist die Schweizer Dialektsammlung mit 200 Stunden Audiomaterial von 4000 Sprechern unterschiedlicher Dialekte, Altersgruppen und Geschlechter. Modellen wie Whisper und XLSR lassen sich leicht auf Dialekte finetunen, sofern gen\u00fcgend Daten vorhanden sind. Der einfache Zugang zu solchen Datens\u00e4tzen und Modellen erm\u00f6glicht es Praktikern, ASR-Systeme, die mit Dialekte umgehen k\u00f6nnen, aufzubauen.</p> <p>Zudem bieten Techniken wie Daten-Augmentierung und Domain-Anpassung die M\u00f6glichkeit, mit Dialekten zu arbeiten. Die Daten-Augmentierung macht ASR-Modelle robuster gegen\u00fcber dialektalen Variet\u00e4ten, indem die Daten auf verschiedene Weise transformiert werden, um zus\u00e4tzliche Trainingsbeispiele zu erstellen. Die Domain-Anpassung setzt Transfer-Learning ein, um gelernte akustischen und sprachlichen Muster von High-Resource Languages auf Dialekten anzuwenden und somit bessere Leistungen f\u00fcr Dialekten zu liefern.</p> <p>\u200b\u200bEs ist noch ein weiter Weg, bis ASR-Systeme Dialekte problemlos verstehen und verarbeiten k\u00f6nnen. Die in diesem Bericht vorgestellten Methoden und Anwendungen sind jedoch bedeutende Fortschritte in diesem Bereich. In der Zukunft wird es sicherlich noch weitere spannende Entwicklungen geben.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#6-weiterfuhrendes-material","title":"6 Weiterf\u00fchrendes Material","text":""},{"location":"Themen/T09_Dialekte_Spracherkennung/#61-podcast","title":"6.1 Podcast","text":"<p>Hier Link zum Podcast.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#62-talk","title":"6.2 Talk","text":"<p>Hier einfach Youtube oder THD System embedden.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#63-demo","title":"6.3 Demo","text":"<p>Hier Link zum Demo Video.</p> <p>Der Repository befindet sich hier.</p>"},{"location":"Themen/T09_Dialekte_Spracherkennung/#7-literaturliste","title":"7 Literaturliste","text":"<p>Die Literatur in den Fu\u00dfnoten sind hier vollst\u00e4ndig zitiert.</p> <p>[2] \u2018Sprachen | Bundesamt f\u00fcr Statistik\u2019. https://www.bfs.admin.ch/bfs/de/home/statistiken/bevoelkerung/sprachen-religionen/sprachen.html (accessed Jun. 27, 2023).</p> <p>[3] Scherrer, Y., &amp; Rambow, O. (2010). Natural language processing for the Swiss German dialect area. Semantic Approaches in Natural Language Processing-Proceedings of the Conference on Natural Language Processing 2010 (KONVENS), 93\u2013102.</p> <p>[4] T.-S. Nguyen, S. St\u00fcker, J. Niehues, and A. Waibel, \u2018Improving Sequence-To-Sequence Speech Recognition Training with On-The-Fly Data Augmentation\u2019, in ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), May 2020, pp. 7689\u20137693. doi: 10.1109/ICASSP40776.2020.9054130.</p> <p>[5] T. Wang, Z. Ding, W. Shao, H. Tang, and K. Huang, \u2018Towards Fair Cross-Domain Adaptation via Generative Learning\u2019, in 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), Waikoloa, HI, USA: IEEE, Jan. 2021, pp. 454\u2013463. doi: 10.1109/WACV48630.2021.00050.</p> <p>[6] M. Pl\u00fcss et al., \u2018SDS-200: A Swiss German Speech to Standard German Text Corpus\u2019. arXiv, May 19, 2022. doi: 10.48550/arXiv.2205.09501.</p> <p>[7] P. Dogan-Sch\u00f6nberger, J. M\u00e4der, and T. Hofmann, \u2018SwissDial: Parallel Multidialectal Corpus of Spoken Swiss German\u2019. arXiv, Mar. 21, 2021. Accessed: Jun. 27, 2023. [Online]. Available: http://arxiv.org/abs/2103.11401</p> <p>[8] M. Pl\u00fcss, L. Neukom, C. Scheller, and M. Vogel, \u2018Swiss Parliaments Corpus, an Automatically Aligned Swiss German Speech to Standard German Text Corpus\u2019. arXiv, Jun. 09, 2021. Accessed: Jun. 27, 2023. [Online]. Available: http://arxiv.org/abs/2010.02810</p> <p>[9] T. Samard\u017ei\u0107, Y. Scherrer, and E. Glaser, \u2018ArchiMob - A Corpus of Spoken Swiss German\u2019, in Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC\u201916), Portoro\u017e, Slovenia: European Language Resources Association (ELRA), May 2016, pp. 4061\u20134066. Accessed: Jun. 27, 2023. [Online]. Available: https://aclanthology.org/L16-1641</p> <p>[10] P. N. Garner, D. Imseng, and T. Meyer, Eds., \u2018Automatic Speech Recognition and Translation of a Swiss German Dialect: Walliserdeutsch\u2019, Proceedings of Interspeech, 2014, doi: 10.21437/Interspeech.2014-480.</p> <p>[11] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u2018Robust Speech Recognition via Large-Scale Weak Supervision\u2019. arXiv, Dec. 06, 2022. doi: 10.48550/arXiv.2212.04356.</p> <p>[12] \u2018Introducing Whisper\u2019. https://openai.com/research/whisper (accessed Jun. 29, 2023).</p> <p>[13] A. Conneau, A. Baevski, R. Collobert, A. Mohamed, and M. Auli, \u2018Unsupervised Cross-lingual Representation Learning for Speech Recognition\u2019. arXiv, Dec. 15, 2020. doi: 10.48550/arXiv.2006.13979.</p> <p>[14] \u2018Fine-Tune Whisper For Multilingual ASR with \ud83e\udd17 Transformers\u2019. https://huggingface.co/blog/fine-tune-whisper (accessed Jun. 30, 2023).</p> <p>[15] \u2018SpecAugment: A New Data Augmentation Method for Automatic Speech Recognition\u2019, Apr. 22, 2019. https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html (accessed Jun. 29, 2023).</p> <p>[16]    J. Grosman, Fine-tuned XLSR-53 large model for speech recognition in German. https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-german, 2021. [Online]. Available: https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-german</p> <ol> <li> <p>Hej Siri, ruf den Alex mal an!\u00a0\u21a9</p> </li> <li> <p>\u2018Sprachen | Bundesamt f\u00fcr Statistik\u2019.\u00a0\u21a9</p> </li> <li> <p>Scherrer, Y., &amp; Rambow, O. (2010). Natural language processing for the Swiss German dialect area.\u00a0\u21a9</p> </li> <li> <p>T.-S. Nguyen, S. St\u00fcker, J. Niehues, and A. Waibel, \u2018Improving Sequence-To-Sequence Speech Recognition Training with On-The-Fly Data Augmentation\u2019.\u00a0\u21a9</p> </li> <li> <p>T. Wang, Z. Ding, W. Shao, H. Tang, and K. Huang, \u2018Towards Fair Cross-Domain Adaptation via Generative Learning\u2019.\u00a0\u21a9</p> </li> <li> <p>M. Pl\u00fcss et al., \u2018SDS-200: A Swiss German Speech to Standard German Text Corpus\u2019.\u00a0\u21a9</p> </li> <li> <p>P. Dogan-Sch\u00f6nberger, J. M\u00e4der, and T. Hofmann, \u2018SwissDial: Parallel Multidialectal Corpus of Spoken Swiss German\u2019.\u00a0\u21a9</p> </li> <li> <p>M. Pl\u00fcss, L. Neukom, C. Scheller, and M. Vogel, \u2018Swiss Parliaments Corpus, an Automatically Aligned Swiss German Speech to Standard German Text Corpus\u2019.\u00a0\u21a9</p> </li> <li> <p>T. Samard\u017ei\u0107, Y. Scherrer, and E. Glaser, \u2018ArchiMob - A Corpus of Spoken Swiss German\u2019.\u00a0\u21a9</p> </li> <li> <p>P. N. Garner, D. Imseng, and T. Meyer, Eds., \u2018Automatic Speech Recognition and Translation of a Swiss German Dialect: Walliserdeutsch\u2019.\u00a0\u21a9</p> </li> <li> <p>A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u2018Robust Speech Recognition via Large-Scale Weak Supervision\u2019.\u00a0\u21a9</p> </li> <li> <p>Bildquelle: \u2018Introducing Whisper\u2019.\u00a0\u21a9</p> </li> <li> <p>A. Conneau, A. Baevski, R. Collobert, A. Mohamed, and M. Auli, \u2018Unsupervised Cross-lingual Representation Learning for Speech Recognition\u2019.\u00a0\u21a9</p> </li> <li> <p>\u2018Fine-Tune Whisper For Multilingual ASR with \ud83e\udd17 Transformers\u2019. https://huggingface.co/blog/fine-tune-whisper (accessed Jun. 30, 2023).\u00a0\u21a9</p> </li> <li> <p>Bildquelle: \u2018SpecAugment: A New Data Augmentation Method for Automatic Speech Recognition\u2019.\u00a0\u21a9</p> </li> <li> <p>J. Grosman, Fine-tuned XLSR-53 large model for speech recognition in German.\u00a0\u21a9</p> </li> </ol>"},{"location":"Themen/T12_feature-extraction/","title":"Feature Extraction","text":"<p>von Robert Gess, Roxana Buder und Saed Abed</p>"},{"location":"Themen/T12_feature-extraction/#abstract","title":"Abstract","text":"<p>Die Extraktion von Merkmalen spielt eine wichtige Rolle in der Datenanalyse und Modellierung. Bei der Feature Extraction geht es darum, relevante Informationen aus einem Datensatz zu identifizieren und in komprimierter Form zu repr\u00e4sentieren. In diesem Artikel werden drei g\u00e4ngige Methoden der Feature Extraction vorgestellt: PCA (Principal Component Analysis), CNN (Convolutional Neural Networks) und Autoencoder. PCA erm\u00f6glicht die Reduzierung der Dimensionalit\u00e4t eines Datensatzes, indem es die Richtungen identifiziert, in denen die Daten streuen. CNNs sind in der Lage, automatisch Merkmale aus Daten zu extrahieren, ohne dass explizit definierte Merkmale festgelegt werden m\u00fcssen. Sie lernen hierarchische Repr\u00e4sentationen der Daten und k\u00f6nnen komplexe Merkmale erfassen. Autoencoder sind neuronale Netzwerke, die versuchen, eine komprimierte Darstellung der Daten im latenten Raum zu erzeugen. Diese Methoden der Feature Extraction finden in verschiedenen Anwendungsbereichen Anwendung, wie der Bildverarbeitung, der Sprachverarbeitung, der Genetik und vielen anderen. Sie bieten die M\u00f6glichkeit, relevante Merkmale zu identifizieren, die zur Verbesserung von Modellen und zur Gewinnung wertvoller Erkenntnisse beitragen k\u00f6nnen.</p>"},{"location":"Themen/T12_feature-extraction/#vorwort","title":"Vorwort","text":"<p>Bei der Extraktion von Merkmalen ist es wichtig, zwischen Feature extraction und feature selection zu unterscheiden. W\u00e4hrend Feature extraction den Prozess der Umwandlung von Daten in neue repr\u00e4sentative Merkmale beschreibt, bezieht sich feature selection auf die Auswahl der relevantesten vorhandenen Merkmale.  In diesem Artikel werden drei Methoden der Feature Extraction betrachtet: PCA (Principal Component Analysis), CNN (Convolutional Neural Networks) und Autoencoder.</p>"},{"location":"Themen/T12_feature-extraction/#methoden","title":"Methoden","text":"<p> * PCA: Die Principal Component Analysis (PCA) ist eine h\u00e4ufig verwendete Methode zur Reduzierung der Dimensionalit\u00e4t eines Datensatzes. Sie basiert auf der Berechnung der Hauptkomponenten, die die Eigenvektoren der Kovarianzmatrix des Datensatzes sind. Die Hauptkomponenten erfassen die Richtungen, in denen die Daten streuen, und k\u00f6nnen als neue Features verwendet werden. PCA hat den Vorteil, dass sie eine lineare Abbildung der Daten erm\u00f6glicht und eine komprimierte Darstellung der Informationen liefert.  * CNN: Convolutional Neural Networks (CNNs) sind in der Lage, automatisch Merkmale aus Daten zu extrahieren, ohne dass explizit definierte Merkmale festgelegt werden m\u00fcssen. Durch das Training eines CNNs lernen die Hidden Layer im Netzwerk, Merkmale auf verschiedenen Abstraktionsebenen zu erkennen. Diese hierarchische Repr\u00e4sentation der Daten erm\u00f6glicht die Extraktion von komplexen Merkmalen. CNNs werden h\u00e4ufig dann genutzt, wenn man Daten hat die sich in 2D darstellen lassen, doch das ist keine zwingende Voraussetzung.  * Autoencoder: Ein Autoencoder ist ein spezieller Typ neuronaler Netzwerke, der versucht, die Eingabedaten m\u00f6glichst genau zu rekonstruieren. W\u00e4hrend des Trainings lernt der Autoencoder eine komprimierte Darstellung der Daten im sogenannten latenten Raum. Der Encoder reduziert die Dimensionalit\u00e4t der Daten und erzeugt eine komprimierte Darstellung, w\u00e4hrend der Decoder die Daten aus diesem Code rekonstruiert. Autoencoder haben den Vorteil, dass sie ohne Labels trainiert werden k\u00f6nnen und eine effiziente Methode zur Dimensionalit\u00e4tsreduktion darstellen. Die Tatsache dass Autoencoder auf dem Input selbst optimiert werden hat allerdings nicht immer Vorteile.  </p>"},{"location":"Themen/T12_feature-extraction/#interpretierbarkeit","title":"Interpretierbarkeit","text":"<p>Die Interpretierbarkeit der resultierenden Features kann abh\u00e4ngig von dem Ziel welches man erreichen m\u00f6chte von gro\u00dfer Wichtigkeit sein. Daher wird bis heute an neuartigen M\u00f6glichkeiten gesucht um diese besser interpretieren zu k\u00f6nnen. Im Folgenden geht es um den aktuellsten Stand der Dinge.</p> <ul> <li>PCA: Die Hauptkomponenten der PCA sind Linearkombinationen der urspr\u00fcnglichen Variablen. Daher k\u00f6nnen wir die Beitr\u00e4ge der einzelnen Variablen zur Gesamtvarianz der Daten analysieren und interpretieren. Einige urspr\u00fcngliche Merkmale tragen einen positiven Beitrag bei, w\u00e4hrend andere weniger relevant sind.  </li> <li>CNN: Im Gegensatz zur PCA sind CNNs aufgrund ihrer komplexen Architektur und der Verwendung nichtlinearer Aktivierungsfunktionen schwerer zu interpretieren. Die Merkmale, die von den Hidden Layern gelernt werden, sind abstrakt und schwer in nat\u00fcrlicher Sprache zu beschreiben. In den ersten Layern werden oft einfache Muster wie Kanten erkannt, w\u00e4hrend in den sp\u00e4teren Layern komplexere Merkmale wie z. B. ganze Gesichter erkannt werden.  </li> <li>Autoencoder: Die Interpretierbarkeit von Autoencodern liegt zwischen PCA und CNN. Obwohl der latente Raum des Autoencoders keine direkte physikalische Bedeutung hat, k\u00f6nnen wir dennoch versuchen, bestimmte Merkmale zu identifizieren, indem wir die Werte im latenten Raum analysieren und Muster erkennen. Die Interpretation ist jedoch meist subjektiver und weniger klar als bei der PCA.  </li> </ul> <p>Die M\u00f6glichkeiten diese Features zu analysieren halten sich bisher bei den meisten Methoden in Grenzen, doch kann man mit gro\u00dfen M\u00fchen in der Regel zumindest ein bisschen transparenz schaffen.</p>"},{"location":"Themen/T12_feature-extraction/#effizienz","title":"Effizienz","text":"<p>Unter Effizienz verstehen wir im Folgenden zwei Dinge, und zwar die offensichtliche, also wie viel Rechenaufwand eine Methode zur Folge hat und als zweites wie viele Daten ben\u00f6tigt werden um ein Ergebnis zu erhalten.</p> <ul> <li>PCA: PCA ist in der Regel schnell zu berechnen, da es auf einfachen mathematischen Operationen basiert. Die Effizienz h\u00e4ngt jedoch von der Datenqualit\u00e4t ab und davon, ob ausreichend Datenpunkte f\u00fcr eine zuverl\u00e4ssige Extraktion vorhanden sind.  </li> <li>CNN: Convolutional Neural Networks erfordern normalerweise eine gro\u00dfe Menge an Trainingsdaten welche zudem noch gelabelt sein m\u00fcssen, um gute Ergebnisse zu erzielen. Das Training eines CNNs kann zeitaufw\u00e4ndig sein, da viele Parameter optimiert werden m\u00fcssen. Dar\u00fcber hinaus erfordert das Training von CNNs in der Regel leistungsstarke Hardware-Ressourcen wie GPUs.  </li> <li>Autoencoder: Autoencoder sind effizient in Bezug auf die Datenbeschaffung, da sie unsupervised Modelle sind und keine Labels f\u00fcr das Training ben\u00f6tigen. Sie k\u00f6nnen mit einer relativ kleinen Datenmenge arbeiten. Das Training eines Autoencoders kann jedoch je nach Netzwerkarchitektur, Datengr\u00f6\u00dfe und Dimensionalit\u00e4t ebenfalls Zeit in Anspruch nehmen.  </li> </ul>"},{"location":"Themen/T12_feature-extraction/#robustheit","title":"Robustheit","text":"<p>Die Robustheit der Feature Extraction bezieht sich auf die F\u00e4higkeit der Methoden, mit verschiedenen Herausforderungen wie Rauschen, Variationen (also Skalierung und Rotation) und Ausrei\u00dfern umzugehen. Hier sind die Robustheitsaspekte der einzelnen Methoden:</p> <ul> <li>PCA: PCA ist empfindlich gegen\u00fcber Rauschen, da es dazu f\u00fchren kann, dass die Varianz auf bestimmte Komponenten verteilt wird, die das Rauschen widerspiegeln, anstatt die tats\u00e4chlich relevanten Merkmale abzubilden. Es gibt jedoch Variationen von PCA, die robust gegen\u00fcber Rauschen und Ausrei\u00dfern sind, z. B. durch die Sch\u00e4tzung der Kovarianzmatrix.  </li> <li>CNN: CNNs sind robuster gegen\u00fcber Variationen im r\u00e4umlichen Kontext, aber sie k\u00f6nnen Schwierigkeiten haben, mit stark unterschiedlich skalierten Daten oder h\u00e4ufigen Rotationen umzugehen. Die Effizienz von CNNs kann durch geeignete Datenaufbereitungstechniken (vor Allem Data Augmentation) verbessert werden.  </li> <li>Autoencoder: Autoencoder sind in der Regel robust gegen\u00fcber Rauschen und k\u00f6nnen es im Rekonstruktionsprozess herausfiltern. Bei Variationen und Ausrei\u00dfern kann die Robustheit von Autoencodern variieren und h\u00e4ngt von der Netzwerkarchitektur und den verwendeten Trainingsstrategien ab.  </li> </ul> <p>Generell muss man dazu aber sagen, dass Datens\u00e4tze mit rauschenden Daten, vielen Ausrei\u00dfern, wenig Variation und Varianz bei den aller meisten Methoden zu eher weniger robusten Modellen f\u00fchrt.</p>"},{"location":"Themen/T12_feature-extraction/#probleme","title":"Probleme","text":"<p>Anders als im Pr\u00e4senzvortrag werden nun ein paar generelle Probleme vorgestellt die so auf die meisten Methden zutreffen. Dieser andere Ansatz ist gew\u00e4hlt worden, da im Pr\u00e4senzvortrag nicht genug Zeit war um ausf\u00fchrlich dar\u00fcber zu reden.</p>"},{"location":"Themen/T12_feature-extraction/#datenqualitat","title":"Datenqualit\u00e4t","text":"<p>Die Qualit\u00e4t der Daten ist ein entscheidender Faktor bei der Feature Extraction. Wenn die Daten fehlerhaft, unvollst\u00e4ndig oder mit Rauschen behaftet sind, kann dies zu unzuverl\u00e4ssigen oder irref\u00fchrenden Merkmalsrepr\u00e4sentationen f\u00fchren. Rauschen kann die Korrelationen zwischen den Merkmalen st\u00f6ren und zu einer schlechten Extraktion der relevanten Informationen f\u00fchren. Daher ist es wichtig, Datenbereinigungsschritte durchzuf\u00fchren und Rauschen zu reduzieren, bevor man mit der Feature Extraction beginnt.</p>"},{"location":"Themen/T12_feature-extraction/#korrelation-und-kollinearitat","title":"Korrelation und Kollinearit\u00e4t","text":"<p>Korrelation und Kollinearit\u00e4t zwischen den Merkmalen k\u00f6nnen ebenfalls ein Problem darstellen. Wenn zwei oder mehr Merkmale stark miteinander korreliert sind oder eine hohe Kollinearit\u00e4t aufweisen, kann dies zu Redundanz in den extrahierten Merkmalen f\u00fchren. Dies kann die Interpretation und Leistung des Modells beeintr\u00e4chtigen. Es ist wichtig, korrelierte Merkmale zu identifizieren und gegebenenfalls Ma\u00dfnahmen zu ergreifen, um die Korrelation zu reduzieren oder zu eliminieren, zum Beispiel durch den Einsatz von Techniken wie der Kovarianzmatrixanalyse.</p>"},{"location":"Themen/T12_feature-extraction/#over-und-underfitting","title":"Over und Underfitting","text":"<p>Overfitting und Underfitting sind Probleme, die bei der Feature Extraction auftreten k\u00f6nnen und die Leistung des Modells beeintr\u00e4chtigen. Overfitting tritt auf, wenn das Modell zu stark auf die spezifischen Merkmale des Trainingsdatensatzes abgestimmt ist und daher bei neuen Daten schlechte Vorhersagen macht. Underfitting hingegen tritt auf, wenn das Modell zu einfach ist und nicht in der Lage ist, die relevanten Informationen aus den Daten zu extrahieren. Um Overfitting und Underfitting zu vermeiden, m\u00fcssen geeignete Regularisierungstechniken und Modellevaluationstechniken angewendet werden. Unter diesem Problem leider unter den verglichenen Methoden aber vor allem der Autoencoder aufgrund seines Funktionsprinzips den loss zu berechnen indem der input als referenz verwendet wird.  </p>"},{"location":"Themen/T12_feature-extraction/#skalierbarkeit","title":"Skalierbarkeit","text":"<p>Die Skalierbarkeit der Feature Extraction-Methoden kann ebenfalls ein Problem sein, insbesondere wenn gro\u00dfe Datens\u00e4tze verarbeitet werden m\u00fcssen. Manche Methoden erfordern umfangreiche Berechnungen und k\u00f6nnen bei gro\u00dfen Datenmengen zeitaufw\u00e4ndig sein. Es ist wichtig, effiziente Algorithmen und Implementierungen zu w\u00e4hlen, um die Skalierbarkeit sicherzustellen und die Verarbeitungszeit zu minimieren. Hier sind vor allem Methoden die auf Basis von Neuronalen Netzwerken arbeiten gemeint, da diese sehr schnell gro\u00df werden k\u00f6nnen und dann sehr viel Rechenleistung ben\u00f6tigen k\u00f6nnen.  </p>"},{"location":"Themen/T12_feature-extraction/#subjektivitat","title":"Subjektivit\u00e4t","text":"<p>Die Interpretation der extrahierten Merkmale kann subjektiv sein und von verschiedenen Personen unterschiedlich ausgelegt werden. Ein Merkmal kann f\u00fcr eine Person offensichtlich sein, w\u00e4hrend es f\u00fcr eine andere Person nicht intuitiv erscheint. Die Subjektivit\u00e4t der Interpretation kann zu Unsicherheiten und unterschiedlichen Schlussfolgerungen f\u00fchren. Es ist wichtig, die Interpretation der Merkmale zu dokumentieren und bei Bedarf Expertenwissen hinzuzuziehen, um eine objektive und konsistente Interpretation zu gew\u00e4hrleisten.</p>"},{"location":"Themen/T12_feature-extraction/#anwendungen","title":"Anwendungen","text":"<ul> <li>PCA: PCA wird h\u00e4ufig verwendet, um die Dimensionalit\u00e4t von Datens\u00e4tzen zu reduzieren und relevante Merkmale zu extrahieren. Sie findet Anwendung in der Bildverarbeitung, der Sprachverarbeitung, der Genetik und anderen Bereichen, in denen die Reduzierung der Dimensionalit\u00e4t und die Identifizierung von Hauptkomponenten von Bedeutung sind.  </li> <li>CNN: CNNs sind besonders n\u00fctzlich bei der Verarbeitung von Bildern und visuellen Daten. Sie werden in der Bilderkennung, der Objekterkennung, der Gesichtserkennung und vielen anderen Bildverarbeitungsaufgaben eingesetzt, bei denen die Extraktion von Merkmalen aus Bildern wichtig ist.  </li> <li>Autoencoder: Autoencoder finden Anwendung in der Datenkompression, der Rekonstruktion fehlender oder besch\u00e4digter Daten und der Generierung von neuen Datenbeispielen. Sie werden auch in der Anomalieerkennung und der Dimensionalit\u00e4tsreduktion eingesetzt, um eine kompakte Darstellung der Daten zu erzeugen.  </li> </ul>"},{"location":"Themen/T12_feature-extraction/#fazit","title":"Fazit","text":"<p>Die Extraktion von Merkmalen ist ein wesentlicher Schritt in der Datenanalyse und Modellierung. Die vorgestellten Methoden der Feature Extraction, wie PCA, CNN und Autoencoder, bieten verschiedene Ans\u00e4tze, um relevante Merkmale aus einem Datensatz zu extrahieren. Jede Methode hat ihre eigenen St\u00e4rken und Schw\u00e4chen, und die Wahl der Methode h\u00e4ngt von den spezifischen Anforderungen und Eigenschaften des Datensatzes ab. Indem wir die richtige Methode ausw\u00e4hlen und anwenden, k\u00f6nnen wir eine komprimierte und repr\u00e4sentative Darstellung der Daten erhalten, die zur Verbesserung von Modellen und zur Gewinnung wertvoller Erkenntnisse beitr\u00e4gt.</p>"},{"location":"Themen/T12_feature-extraction/#weiterfuhrendes-material","title":"Weiterf\u00fchrendes Material","text":""},{"location":"Themen/T12_feature-extraction/#podcast","title":"Podcast","text":"<p>Hier Link zum Podcast.</p>"},{"location":"Themen/T12_feature-extraction/#talk","title":"Talk","text":"<p>Hier einfach Youtube oder THD System embedden.</p>"},{"location":"Themen/T12_feature-extraction/#demo","title":"Demo","text":"<p>Hier Link zum Demo Video </p>"},{"location":"Themen/T12_feature-extraction/#code-demo","title":"Code Demo.","text":""},{"location":"Themen/T12_feature-extraction/#literaturliste","title":"Literaturliste","text":""},{"location":"Themen/T12_feature-extraction/#hesami-mohsen-jones-a-2020-application-of-artificial-intelligence-models-and-optimization-algorithms-in-plant-cell-and-tissue-culture-applied-microbiology-and-biotechnology-101007s00253-020-10888-2","title":"Hesami, Mohsen &amp; Jones, A.. (2020). Application of artificial intelligence models and optimization algorithms in plant cell and tissue culture. Applied Microbiology and Biotechnology. 10.1007/s00253-020-10888-2.","text":""},{"location":"Themen/T12_feature-extraction/#httpswwwopensourceagendacomprojectssaliency-detection-convolutional-autoencoder","title":"https://www.opensourceagenda.com/projects/saliency-detection-convolutional-autoencoder","text":""},{"location":"Themen/T12_feature-extraction/#khoshdeli-mina-cong-richard-parvin-bahram-2017-detection-of-nuclei-in-he-stained-sections-using-convolutional-neural-networks","title":"Khoshdeli, Mina &amp; Cong, Richard &amp; Parvin, Bahram. (2017). Detection of Nuclei in H&amp;E Stained Sections Using Convolutional Neural Networks.","text":""},{"location":"Themen/T12_feature-extraction/#httpstowardsdatasciencecomautoencoders-vs-pca-when-to-use-which-73de063f5d7","title":"https://towardsdatascience.com/autoencoders-vs-pca-when-to-use-which-73de063f5d7","text":""},{"location":"Themen/T12_feature-extraction/#httpswwwkagglecomcompetitionsvsb-power-line-fault-detectiondata","title":"https://www.kaggle.com/competitions/vsb-power-line-fault-detection/data","text":""},{"location":"Themen/T12_feature-extraction/#httpsarxivorgpdf210304874pdf","title":"https://arxiv.org/pdf/2103.04874.pdf","text":""},{"location":"Themen/T12_feature-extraction/#paul-s-symons-d-d-altmanninger-m-ertel-w-2020-autoencoder-based-feature-learning-for-human-activity-recognition","title":"Paul, S., Symons, D. D., Altmanninger, M., &amp; Ertel, W. (2020). Autoencoder Based Feature Learning for Human Activity Recognition.","text":""},{"location":"Themen/T12_feature-extraction/#httpswwwresearchgatenetpublication301632899_an_overview_of_convolutional_and_autoencoder_deep_learning_algorithm","title":"https://www.researchgate.net/publication/301632899_An_overview_of_Convolutional_and_AutoEncoder_Deep_Learning_Algorithm","text":""},{"location":"Themen/T12_feature-extraction/#sainath-t-n-kingsbury-b-saon-g-soltau-h-mohamed-a-r-dahl-g-ramabhadran-b-2013-deep-convolutional-neural-networks-for-large-scale-speech-tasks","title":"Sainath, T. N., Kingsbury, B., Saon, G., Soltau, H., Mohamed, A. R., Dahl, G., &amp; Ramabhadran, B. (2013). Deep Convolutional Neural Networks for Large-scale Speech Tasks.","text":""},{"location":"Themen/T12_feature-extraction/#analytics-vidhya-2018-august-feature-engineering-for-machine-learning-a-comprehensive-overview-httpswwwanalyticsvidhyacomblog201808dimensionality-reduction-techniques-python","title":"Analytics Vidhya. (2018, August). Feature engineering for machine learning: A comprehensive overview. https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/","text":""},{"location":"Themen/TextToSpeech/","title":"Text-to-Speech","text":"<p>von Lea Wagner und Michael Schmidbauer</p>"},{"location":"Themen/TextToSpeech/#abstract","title":"Abstract","text":"<p>In diesem Blogbeitrag widmen wir uns dem Thema Text-to-Speech (TTS). Bei Text-to-Speech handelt es sich um eine vielseitig einsetzbare und faszinierende Technologie zur computergest\u00fctzten Generierung von nat\u00fcrlicher und menschen\u00e4hnlicher Sprache.</p> <p>In dem Beitrag betrachten wir einerseits die vielf\u00e4ltigen Einsatzm\u00f6glichkeiten von TTS wie beispielsweise die Unterst\u00fctzung von sehbehinderten Personen. Andererseits beleuchten wir den Aufbau und die Funktionsweise dieser Technologie genauer. F\u00fcr die Erkl\u00e4rung betrachten wir die beiden Modelle Tacotron 2 und Vall-E.</p>"},{"location":"Themen/TextToSpeech/#einleitung-motivation","title":"Einleitung / Motivation","text":"<p>Die Text-to-Speech-Technologie hat, wie viele Anwendungen der K\u00fcnstlichen Intelligenz, in den letzten Jahren erhebliche Fortschritte gemacht. Ihr Einfluss auf verschiedene Bereiche unseres t\u00e4glichen Lebens ist beachtlich. Prominente Beispiele f\u00fcr ihre Anwendung w\u00e4ren Sprachassistenten wie beispielsweise Siri oder Alexa, aber sie waren l\u00e4ngst nicht die ersten. Schon lange wird die TTS-Technologie beispielweise in Navigationsger\u00e4ten verwendet.</p> <p>Mit Hilfe dieser Technologie wird es Maschinen erm\u00f6glicht, Text in nat\u00fcrliche Sprache umzuwandeln. Dabei wurde erst mit Einsatz von K\u00fcnstlicher Intelligenz ein nahezu menschlicher Klang erm\u00f6glicht. Doch gibt es noch immer Stolpersteine wie beispielsweise Dialekte, die von den Systemen heute noch nicht perfekt erzeugt werden k\u00f6nnen.</p> <p>Nichtsdestotrotz ist es unbestreitbar, dass die Text-to-Speech-Technologie ein wesentlicher Bestandteil der heutigen Mensch-Computer-Interaktion geworden ist. Sie erh\u00f6ht die Sicherheit im Stra\u00dfenverkehr oder erm\u00f6glicht es, sehbehinderten Personen besser an der zunehmend digitalen Welt teilzunehmen.</p> <p>Gerade f\u00fcr diese Personengruppe kann die TTS-Technologie einen deutlich besseren Zugang zu digitalen Informationen erm\u00f6glichen. Mit TTS k\u00f6nnen ihnen digitale Inhalte vorgelesen werden und damit der Zugang zum digitalen Leben erleichtert werden.</p> <p>Dar\u00fcber hinaus bietet die Integration der Text-to-Speech-Technologie f\u00fcr die Unterhaltungsindustrie viele neue M\u00f6glichkeiten. H\u00f6rb\u00fccher, Podcasts, Videospiele, Filme und Serien sind einige der Einsatzgebiete, bei denen TTS-Systeme heute oder in naher Zukunft eine gro\u00dfe Rolle spielen k\u00f6nnen. So w\u00e4re es m\u00f6glich ein deutlich immersiveres Spieleerlebnis zu erzeugen oder eine Sprachfassung eines Films f\u00fcr eine Sprache zu generieren, bei der heutzutage der Markt zu klein w\u00e4re, um die Kosten zu rechtfertigen.</p> <p>Durch die ganzen M\u00f6glichkeiten sollte man allerdings nicht au\u00dferacht lassen, welche potenziellen Gefahren diese Technologie mit sich bringen kann. Gerade mit dem Fortschreiten von Zero-Shot-Systemen, die mit nur wenigen Sekunden Audio eine Stimme nachahmen k\u00f6nnen, entsteht auch ein gro\u00dfes Gef\u00e4hrdungspotential, dass von Identit\u00e4tsdiebstahl bis hin zu politischer Einflussnahme reicht.</p> <p>In diesem Block werden wir und mit den technischen Grundlagen von Text-to-Speech-Systemen befassen. Au\u00dferdem werden wir zwei moderne Systeme genauer betrachten.</p>"},{"location":"Themen/TextToSpeech/#stand-der-forschung","title":"Stand der Forschung","text":"<p>In diesem Abschnitt m\u00f6chten wir ein end-to-end TTS System und ein Zero-Shot TTS System vorstellen.</p> <p>Tacotron 2: Tacotron ist ein end-to-end neurales Text-to-Speech System, welches 2018 im Auftrag von Google entwickelt wurde. Es handelt sich dabei um eine Technologie, die einen Text in Sprache umwandeln kann. Systeme wie Tacotron 2 sind entscheidend f\u00fcr Anwendungen wie sprachgesteuerte Systeme und assistive Technologien. Tacotron 2 sollte beispielsweise in der Zukunft f\u00fcr die Sprachsynthese in Google Translate und Google Home verwendet werden. Aber auch f\u00fcr die barrierefreiheit f\u00fcr Sehbehinderte Menschen stellen solche Systeme eine Hilfestellung und Bereicherung der Lebensqualit\u00e4t dar.</p> <p>Die Besonderheit von Tacotron 2 liegt in der nat\u00fcrlich klingenden Sprache. Viele TTS Systeme weisen Probleme bei der Betonung, Prosodie und Semantik auf. Die Beispiele von Tacotron 2 weisen eine nahezu menschlich klingende Sprachausgabe auf.</p> <p>Durch die Anwendung von vielschichtigen Deep Learning Algorithmen kann das System komplexe Muster in der Sprache erfassen und so eine m\u00f6glichst nat\u00fcrliche Sprache erzeugen. Bei einer Bewertung menschlicher Zuh\u00f6rer erzielte das System einen MOS (mean opinion score) von 4.53 verglichen mit 4.58 f\u00fcr professionell aufgenommene Audios. MOS bedeutet, dass eine bestimmte Anzahl von Menschen bewertet, wie gut sich die Audio anh\u00f6rt.</p> <p>Funktionsweise von Tacotron 2: Tacotron 2 besteht aus zwei Hauptkomponenten, n\u00e4mlich einem Encoder und einem Decoder. Der Encoder wandelt eine Textsequenz in eine Hidden Feature Repr\u00e4sentation um, w\u00e4hrend der Decoder basierend auf der enkodierten Sequenz Frame f\u00fcr Frame ein Mel Spektrogramm erstellt. Im Blockdiagramm unten ist der Encoder in blau dargestellt und der Decoder in orange.</p> <p> Abb. 1 https://pytorch.org/assets/images/tacotron2_diagram.png</p> <p>Als Input erh\u00e4lt das System einen beliebigen Text. Daraus werden Character Embeddings generiert. Hier wurde zuvor ein Modell trainiert, welches jedem Buchstaben einen Vektor zuweist. In diesem Fall hat ein Vektor 512 Dimensionen, in dem die sprachlichen Eigenschaften dieses Buchstabens festgehalten werden. Diese Vektoren werden anschlie\u00dfend in einer Matrix zusammengefasst und an ein 3-schichtiges Convolutional Neural Network \u00fcbergeben. Dieses CNN ist darauf ausgelegt, n-grams mit l\u00e4ngerfristigem Kontext zu modellieren. Dieser Output geht dann weiter an ein bi-directional LSTM. In einem normalen LSTM wird ein Zustand zum Zeitpunkt t berechnet auf dem Input und auf dem Zustand des vorherigen Zeitpunktes t-1. In diesem LSTM werden die Daten vorw\u00e4rts und r\u00fcckw\u00e4rts verarbeitet, wobei kontextuelle Informationen sehr gut erfasst werden k\u00f6nnen. Die Ausgabe dieses LSTM stellt die Encoder Ausgabe dar, welche jetzt high-level Informationen \u00fcber die Textsequenz enth\u00e4lt.</p> <p>Das (location sensitive) Attention Network nimmt den Output des LSTM und einen Output des Decoder Teils zum Zeitpunkt t-1, um relevante Informationen zu erhalten, mit welchen dann die Vorhersage zum Zeitpunkt t erstellt wird. Diese wird an ein 2-schichtiges LSTM \u00fcbergeben. F\u00fcr jeden Zeitschritt dieses LSTM wird ein Mel-Spektrogramm-Vektor vorhergesagt. Diese Ausgabe geht an eine lineare Projektion. Diese wird einmal verwendet f\u00fcr den Stop Token. Hier wird die Wahrscheinlichkeit berechnet, dass die Output Sequenz fertig generiert wurde. So kann das Modell dynamisch bestimmen, wann die Generierung beendet wird und ist nicht an eine feste Vorgabe von Iterationen gebunden.</p> <p>Die Ausgabe der linearen Projektion geht au\u00dferdem an ein Pre-Net, welches seinen Output wieder an das 2-schichtige LSTM \u00fcbergibt, um den n\u00e4chsten Frame vorherzusagen. Das 5-schichtige Post-Net am Schluss berechnet einen bestimmten Restwert, welcher f\u00fcr ein glattes Mel Spektrogramm verantwortlich ist. Sobald alle Frames durchlaufen wurden, enth\u00e4lt man dann ein komplettes Mel Spektrogramm. Das Mel Spektrogramm wird dann an das WaveNet gegeben, welches als Vocoder agiert und eine Wellenform synthetisiert.</p> <p>Limitationen von Tacotron 2: Obwohl Sprachmodelle wie Tacotron 2 erstaunliche Fortschritte im Bereich der Aussprache gemacht haben, zeigen sich hier immer wieder Probleme auf. Schwierigkeiten bei der Aussprache von W\u00f6rtern mit komplexer Phonologie oder ungew\u00f6hnlicher Betonung bleiben auch bei modernen Text-to-Speech Systemen wie Tacotron 2 bestehen.</p> <p>Ein weiterer Punkt ist die Generierung von Audios in Echtzeit. Da die Text-to-Speech Synthese des Systemns auf einer komplexen Architektur mit vielen Schichten beruht, ist eine Generierung in Echtzeit derzeit noch nicht m\u00f6glich. Deshalb wird dieses System momentan auch nicht f\u00fcr die Sprachsynthese in Google Translate und Google Home benutzt.</p> <p>Dar\u00fcber hinaus ist es bisher nicht m\u00f6glich, die Emotionen der generierten Sprache gezielt zu steuern. Obwohl Tacotron 2 in der Lage ist, nat\u00fcrliche Sprachausgauben zu erzeugen, fehlt die F\u00e4higkeit, die emotionale Ausrucksweise bewusst und gezielt zu beeinflussen. Dies stellt jedoch einen eigenen Bereich der Text-to-Speech Forschung dar.</p> <p>VALL-E: VALL-E ist ein Zero-Shot TTS System, welches 2023 von Microsoft vorgestellt wurde. Das System kann basierend auf einem Text Prompt und einem Audio Prompt einen Text in Sprache mit der Stimme des Audio Prompts umwandeln. Das bedeutet, VALL-E kann Stimmen imitieren, welche nicht in den Trainingsdaten vorkommen. Auch die akustische Umgebung kann ber\u00fccksichtigt werden. Wenn der Audio Prompt sich beispielsweise anh\u00f6rt, als w\u00fcrde die Stimme aus einem Telefon kommen, kann VALL-E auch das imitiere. Die Trainingsdaten stammen aus dem LibriLight Datensatz von Meta und enthalten insgesamt 60K Stunden Audio Material, welches gr\u00f6\u00dftenteils aus H\u00f6rb\u00fcchern stammt. Dadurch kann ein System wie VALL-E in Zukunft Anwendung in der Welt der Podcasts und H\u00f6rb\u00fccher finden. Die Bewertung der von VALL-E generierten Audios erzielte sogar leicht bessere Ergebnisse als die Ground Truths.</p> <p>Die Besonderheit des VALL-E Systems ist der extrem kurze ben\u00f6tigte Audio Input. W\u00e4hrend das Vorg\u00e4nger System noch einen Input von 30 Minuten ben\u00f6tigte, ben\u00f6tigt VALL-E lediglich 3 Sekunden. Durch diese erhebliche Verbesserung entsteht nicht nur eine vereinfachte Anwendung, sondern auch ein vergr\u00f6\u00dfertes Missbrauchspotzenzial. Darauf wird im Abschnitt Limitationen/Ethik nochmal genauer eingegangen.</p> <p>Funktionsweise von VALL-E: \u00c4hnlich wie Tacotron 2, nutzt VALL-E eine Encoder-Decoder-Architektur.</p> <p> Abb. 2 https://www.chip.de/ii/1/2/6/7/6/9/4/0/3/3b6003cc590f29a5.jpg</p> <p>Es gibt zwei Inputs, den Text Prompt und den Acoustic Prompt. Der Text Prompt wird zun\u00e4chst in Phoneme und dann in entsprechende Embeddings umgewandelt. Der Audio Prompt geht an den Encoder. Hierbei handelt es sich um den Audio Codec Encoder von Facebook Research. Dieser stellt das \u201cArbeitstier\u201d hinter VALL-E dar und hat nochmal einen eigenen Encoder und Decoder, wie man im Blockdiagramm erkennen kann.</p> <p> Abb. 3 https://github.com/facebookresearch/encodec/raw/main/architecture.png</p> <p>Der Encoder nimmt die Wellenform und f\u00fchrt eine Convolution durch f\u00fcr Downsampling. Darauffolgend wird ein LSTM genutzt f\u00fcr die Sequenz Modellierung. Das Ergebnis dieses Encoders ist eine kompaktere Repr\u00e4sentation mit 75 beziehungsweise 100 latenten Zeitschritten im Vergleich zu 24.000 beziehungsweise 48.000 im Input. Der Decoder ist eine gespiegelte Form des Encoders, welcher wieder ein Upsampling durchf\u00fchrt und daraus eine Wellenform erzeugt. Dazwischen befindet sich der Quantizer.</p> <p>F\u00fcr diesen gibt es 8 sogenannte Codebooks. Codebooks sind Dictionaries gef\u00fcllt mit Vektoren, woraus sich 1024 Eintr\u00e4ge ergeben. Der Input Vektor wird repr\u00e4sentiert, indem er auf den \u00e4hnlichsten Vektor im Codebook gemapt wird. Diese \u00c4hnlichkeit wird gemessen mit dem euklidischen Abstand. Dadurch gehen Informationen verloren, welche man aber gerne erhalten m\u00f6chte. Mit Hilfe der Residual Vector Quantization (RVQ) wird der Restwert berechnet. Dieser wird dann auf einen weiteren Vektor im Codebook gemapt. Die finale Repr\u00e4sentation ist eine Liste der Indexe, auf die die Vektoren gemapt wurden.</p> <p>Sobald der Audio Codec Encoder seine Arbeit erledigt hat, wird die Repr\u00e4sentation an den Decoder von VALL-E \u00fcbergeben.</p> <p> Abb. 4 https://miro.medium.com/v2/resize:fit:1400/format:webp/1zZmUzjNyvSa3a-b-c7bdXQ.png*</p> <p>Dieser besteht aus einem Non-Auto-Regressive (NAR) und aus einem Auto-Regressive (AR) Decoder. Der AR Decoder ist daf\u00fcr verantwortlich, die Input Daten des ersten Codebooks zu verarbeiten. Der NAR Decoder ist f\u00fcr die restlichen Codebooks verwendet. Hier wird aus diesen Repr\u00e4sentationen der Codebooks die Wellenform generiert, aus der die Output Sprache entsteht. </p> <p>Limitationen und Ethik von VALL-E: Trotz der herausragenden Ergebnisse von VALL-E gibt es dennoch einige Einschr\u00e4nkungen, die es zu beachten gibt. Einerseits k\u00f6nnen manche W\u00f6rter unklar oder schwer verst\u00e4ndlich sein. Dar\u00fcber hinaus ist die Leistung des Systems bei Sprechern mit Akzent schlechter im Vergleich zu den Sprechern ohne Akzent. Dies liegt an den Trainingsdaten, die zu einem sehr gro\u00dfen Teil aus H\u00f6rbuchmaterial bestehen. Auch kann VALL-E die Emotionen der Sprache noch nicht gezielt beeinflussen. Abschlie\u00dfend bestehen ethische Risiken, beispielsweise im Zusammenhang mit Impersonation und Spoofing. Im Zusammenhang mit Deep Fake Videos k\u00f6nnten mit Hilfe von VALL-E falsche Informationen verbreitet werden. Auch k\u00f6nnte VALL-E genutzt werden, um beispielsweise \u00fcber Telefon an sensible Daten zu gelangen.</p> <p>Microsoft \u00e4u\u00dfert sich zu diesen m\u00f6glichen negativen Folgen in ihrem Paper. Es wird eine M\u00f6glichkeit genannt, ein System zu erstellen, welches klassifizieren kann, ob eine Audio von VALL-E generiert wurde oder nicht. Ein Solches System gibt es zum jetzigen Stand aber noch nicht. </p> <p>Fazit: Tacotron 2 und VALL-E: Trotz ihrer Unterschiede in den Systemen und Anwendungsbereichen teilen Vall-E und Tacotron 2 Gemeinsamkeiten in ihrer Architektur und Technologie. Zum einen ist das wie Verwendung einer Encoder-Decoder-Architektur, welche es erm\u00f6glicht, den Input zun\u00e4chst in eine diskrete Repr\u00e4sentation umzuwandeln um daraus anschlie\u00dfend die Sprachausgabe zu generieren. Eine weitere Gemeinsamkeit ist die Verwendung von Mel-Spektrogrammen. In Tacotron 2 wird dieses jedoch direkt als intermedi\u00e4re Repr\u00e4sentation verwendet, bei VALL-E aber nur im Encodec Teil. Zuletzt nutzen beide Systeme autoregressive Technologien, um die Sprachausgabe schrittweise zu generieren und so eine nat\u00fcrliche und fl\u00fcssige Sprache zu erzuegen. </p>"},{"location":"Themen/TextToSpeech/#methoden","title":"Methoden","text":"<p>In diesem Abschnitt geben wir einen kurzen \u00dcberblick \u00fcber weitere Methoden f\u00fcr Text-to-Speech Systeme.</p> <p>Hidden Markov Modele: Hidden Markov Modelle (HMMs) sind eine der grundlegendsten Methoden in der Sprachverarbeitung und waren auch bei TTS-Systemen ma\u00dfgeblich an der Entwicklung beteiligt. Es handelt sich um eine Methode, die statistische Eigenschaften von Sprache modelliert und die Beziehungen zwischen Text und Sprachesignal erfasst.</p> <p>Das zugrunde liegende mathematische Modell eines HMMs besteht aus einer Menge von Zust\u00e4nden, \u00dcberg\u00e4ngen zwischen den Zust\u00e4nden und Emissionen, die mit den Zust\u00e4nden verkn\u00fcpft sind. F\u00fcr die Anwendung von HMMs im Bereich der TTS-Synthese werden typischerweise drei Arten von Zust\u00e4nden definiert. Die Zust\u00e4nde des Emissionsmodells repr\u00e4sentieren die Kl\u00e4nge oder Phoneme, die in der Sprache vorhanden sind. Jeder Zustand ist mit einer Wahrscheinlichkeitsverteilung \u00fcber die m\u00f6glichen akustischen Merkmale verbunden, die f\u00fcr das jeweilige Phonem charakteristisch sind. Die Zust\u00e4nde des \u00dcbergangsmodells repr\u00e4sentieren die linguistische Struktur des Textes. Sie k\u00f6nnen Worte, Silben oder andere linguistische Einheiten sein. Die \u00dcberg\u00e4nge zwischen den Zust\u00e4nden des \u00dcbergangsmodells modellieren die statistische Wahrscheinlichkeit, mit der eine bestimmte linguistische Einheit auf eine andere folgt. Der Anfangszustand repr\u00e4sentiert den Beginn des Textes oder der Sprachsequenz. Er gibt an, welche linguistische Einheit zuerst erzeugt wird.</p> <p>Die grundlegende Idee hinter HMMs besteht darin, dass der \u00dcbergang von einem Zustand zum n\u00e4chsten stochastisch erfolgt, basierend auf den \u00dcbergangswahrscheinlichkeiten zwischen den Zust\u00e4nden. Zus\u00e4tzlich zu den Zustands\u00fcberg\u00e4ngen emittiert jeder Zustand eine bestimmte Wahrscheinlichkeitsverteilung \u00fcber die akustischen Merkmale.</p> <p>Bei der TTS-Synthese wird das HMM-Modell verwendet, um akustische Modelle zu erzeugen, die die Beziehung zwischen Text und Sprachsignalen erfassen. Der Text wird in eine Sequenz von Zust\u00e4nden des Emissionsmodells \u00fcbersetzt, und die HMM-\u00dcbergangswahrscheinlichkeiten werden verwendet, um die Reihenfolge und Dauer der Zust\u00e4nde zu bestimmen. Anhand der Wahrscheinlichkeitsverteilungen der akustischen Merkmale k\u00f6nnen dann Sprachsignale erzeugt werden, die dem Text entsprechen.</p> <p>Obwohl HMMs eine bew\u00e4hrte Methode in der Sprachverarbeitung sind, haben sie auch ihre Einschr\u00e4nkungen. Insbesondere k\u00f6nnen sie Schwierigkeiten haben, komplexe linguistische Ph\u00e4nomene und Variabilit\u00e4t in den Sprachsignalen genau zu modellieren.</p> <p>Deep Learning und NNs: Deep Learning und neuronale Netzwerke verwenden mehrschichtige neuronale Netzwerke, um komplexe Funktionen zu erlernen und hochdimensionale Daten zu verarbeiten. Im Bereich der TTS-Synthese k\u00f6nnen diese verwendet werden, um direkt Text auf Sprachsignale abbilden zu k\u00f6nnen, ohne den Umweg \u00fcber diskrete Zust\u00e4nde wie es bei Hidden Markov Modellen der Fall ist.</p> <p>Eine h\u00e4ufig verwendete Architektur sind Recurrent Neural Networks (RNNs). Diese Modelle haben die F\u00e4higkeit, Sequenzdaten effektiv zu modellieren und k\u00f6nnen auf den TTS-Kontext angepasst werden, um Text in akustische Merkmale zu \u00fcbersetzen.</p> <p>Eine Weiterentwicklung des RNN sind die sogenannten Long-Short-Term-Memory (LSTM) Architekturen. Hierbei wird das Modell mit einem Speicher ausgestattet, der es ihm erm\u00f6glicht auch \u00fcber l\u00e4ngere Abschnitte Zusammenh\u00e4nge zu erkennen.</p> <p>F\u00fcr die Generierung des Sprachmodels wird h\u00e4ufig ein Convolutional Neural Network (CNN) verwendet. Diese Architektur ist besonders geeignet, um Muster in der gesprochenen Sprache zu erkennen und diese auch zu modellieren.</p> <p>Transformer-basierte Modelle: Die Transformer Technologie ist eine der modernen Architekturen im Bereich der nat\u00fcrlichen Sprachverarbeitung. Dabei finden sie ihre Anwendungen sowohl bei Large-Language-Modellen wie ChatGPT oder GitHubCopilot, als auch bei Text-to-Speech Systemen. So sind Transformer beispielsweise Bestandteil des oben Vorgestellten Vall-E Systems.</p> <p>Im Gegensatz zu herk\u00f6mmlichen end-to-end Methoden wie dem oben vorgestellten Tacotron 2 sind Transformer-basierte Ans\u00e4tze effizienter und besser darin, Langzeitabh\u00e4ngigkeiten zu modellieren. Ebenfalls ist es m\u00f6glich mit Transformer-basierten Modellen deutlich gr\u00f6\u00dfere Datenmengen zu verarbeiten, als bei den vorhergehenden Ans\u00e4tzen.</p> <p>Im Vergleich zu herk\u00f6mmlichen RNN-basierten Modellen haben Tranformer-basierte Modelle noch weitere Vorteile. Durch den Wegfall der rekurrenten Verbindungen ist es m\u00f6glich das Training zu parallelisieren und durch die Self-Attention der Transformer kann ein globaler Kontext einer Sequenz zu jedem Ausschnitt hinzugef\u00fcgt werden. Dadurch ist es m\u00f6glich, l\u00e4ngerfristigen Kontext zu modellieren.</p> <p>F\u00fcr eine ausf\u00fchrlichere Erkl\u00e4rung dieser Technologie verweisen wir auf den Blogbeitrag zu Large-Language-Models.</p> <p>Transfer Learning: Bei der Methode des Transfer Learnings werden Modelle auf gro\u00dfen allgemeinen Datens\u00e4tzen trainiert, um sp\u00e4ter auf den spezifischen Anwendungsfall abgestimmt zu werden. In unserem Fall wird ein Modell beispielsweise mit Englischsprachigen Audiodateien trainiert und sp\u00e4ter auf die gew\u00fcnschte Stimme angepasst.</p> <p>Mit dieser Methode kann die Trainingszeit des Modells erheblich reduziert werden. Allerdings verschlechtern sich die Ergebnisse, je gr\u00f6\u00dfer die Unterschiede zwischen den Trainingsdaten des vortrainierten Modells und des Anwendungsfalls werden.</p> <p>F\u00fcr genauere und umfangreichere Informationen zu diesem Thema verweisen wir auf den Blogbeitrag zu Transfer Learning.</p>"},{"location":"Themen/TextToSpeech/#anwendungen","title":"Anwendungen","text":"<p>Die Anwendungsm\u00f6glichkeiten f\u00fcr Text-to-Speech Systeme sind vielseitig. Sie haben schon lange ihren Weg in unseren Alltag gefunden. Zu h\u00e4ufigen Anwendungen derartiger Systeme geh\u00f6ren unter anderen:</p> <ul> <li>Navigationsger\u00e4te</li> <li>Automatische ansagen in Z\u00fcgen oder an Bahnh\u00f6fen</li> <li>Sprachassistenten wie beispielsweise Siri oder Alexa</li> <li>Barrierefreiheitsfunktionen wie beispielsweise das Vorlesen dessen, was auf dem Bildschirm angezeigt wird.</li> <li>Vorlesen der Text Ein- oder Ausgabe bei \u00dcbersetzungssoftware</li> </ul> <p>Beispiele f\u00fcr Anwendungen bei denen Text-to-Speech bereits eingesetzt wird oder ein zuk\u00fcnftiger Einsatz denkbar w\u00e4re:</p> <ul> <li>Nachbearbeitung von Podcasts, um einzelne W\u00f6rter zu \u00e4ndern</li> <li>Aufnahme von Podcasts oder H\u00f6rb\u00fcchern</li> <li>Erstellen von Tonaufnahmen in unterschiedlichen Sprachen f\u00fcr Videospiele, Filme und Serien</li> </ul> <p>Gerade mit dem Blick auf Zero-Shot-Systeme sollte man die Einsatzgebiete nicht au\u00dfer Acht lassen, die nicht dem Wohle des Gro\u00dfteils der Bev\u00f6lkerung dienen. Dabei werden Tonspuren oder Videos erstellt, um gezielt Personen, Unternehmen oder L\u00e4ndern zu schaden. Beispielsweise, um Berichte oder Aussagen zu erstellen, die Wahlen oder den Aktienkurs in eine bestimmte Richtung dr\u00e4ngen sollen.</p>"},{"location":"Themen/TextToSpeech/#fazit","title":"Fazit","text":"<p>Text-to-Speech ist eine faszinierende und vielversprechende Technologie, mit gro\u00dfem Potential und vielseitigen Anwendungsm\u00f6glichkeiten. Die Entwicklung der letzten Jahre hat inzwischen Systeme zur Generierung nahezu nat\u00fcrlicher menschlicher Sprache hervorgebracht.</p> <p>Die Einsatzgebiete sind dabei sehr vielseitig. Von dem Vorlesen von Texten auf dem Bildschirm f\u00fcr Sehbehinderte Personen, \u00fcber die Unterst\u00fctzung zum lernen von Sprachen, bis hin zur sicheren Verwendung von Navigationsger\u00e4ten. Dar\u00fcber hinaus sind die Anwendungsm\u00f6glichkeiten in der Werbebranche, dem Kundenservice oder der Unterhaltungsindustrie nahezu grenzenlos.</p> <p>Allerdings gibt es auch einige Herausforderungen im Zusammenhang mit TTS. Die Generierung von nat\u00fcrlichen Stimmen erfordert eine komplexe Verarbeitung von Sprache, Intonation und Betonung. Obwohl TTS-Systeme bereits erstaunlich realistische Ergebnisse erzielen k\u00f6nnen, gibt es immer noch Raum f\u00fcr Verbesserungen, insbesondere in Bezug auf die emotionale Ausdrucksst\u00e4rke und die Anpassungsf\u00e4higkeit an unterschiedliche Textarten.</p> <p>Dar\u00fcber hinaus sollten ethische Aspekte bei der Entwicklung und Anwendung von TTS-Technologien ber\u00fccksichtigt werden. Insbesondere der potenzielle Missbrauch von TTS f\u00fcr F\u00e4lschungen oder Manipulationen von Audioinhalten ist ein ernstzunehmendes Risiko. Es ist wichtig, Richtlinien und Standards zu entwickeln, um die Verbreitung von gef\u00e4lschten oder irref\u00fchrenden Stimmen zu verhindern und die Integrit\u00e4t von Audioquellen zu gew\u00e4hrleisten. Dabei sollte nicht au\u00dfer Acht gelassen werden, dass die Risiken sowohl auf gesellschaftlicher als auch privater Ebene bestehen. Besonderes Augenmerk sollte man dabei auch auf die Zero-Shot Systeme legen.</p> <p>Insgesamt bietet TTS enorme Vorteile und Chancen, aber auch Herausforderungen und ethische \u00dcberlegungen. Die weitere Forschung und Entwicklung auf diesem Gebiet sind von gro\u00dfer Bedeutung, um die Qualit\u00e4t der generierten Stimmen zu verbessern, neue Anwendungsbereiche zu erschlie\u00dfen und sicherzustellen, dass TTS-Technologien verantwortungsbewusst eingesetzt werden. Mit den richtigen Anstrengungen und Ma\u00dfnahmen kann Text-to-Speech dazu beitragen, die Kommunikation und den Zugang zu Informationen f\u00fcr Menschen weltweit zu verbessern.</p>"},{"location":"Themen/TextToSpeech/#weiterfuhrendes-material","title":"Weiterf\u00fchrendes Material","text":""},{"location":"Themen/TextToSpeech/#podcast","title":"Podcast","text":"<p>Hier Link zum Podcast.</p>"},{"location":"Themen/TextToSpeech/#talk","title":"Talk","text":"<p>Hier einfach Youtube oder THD System embedden.</p>"},{"location":"Themen/TextToSpeech/#demo","title":"Demo","text":"<p>Hier Link zum Demo Video + Link zum GIT Repository mit dem Demo Code.</p>"},{"location":"Themen/TextToSpeech/#literaturliste","title":"Literaturliste","text":"<ol> <li> <p>https://ieeexplore.ieee.org/abstract/document/10057419    T. Yanagita, S. Sakti, und S. Nakamura, \u201eJapanese Neural Incremental Text-to-Speech Synthesis Framework With an Accent Phrase Input\u201c, IEEE Access, Bd. 11, S. 22355\u201322363, 2023, doi: 10.1109/ACCESS.2023.3251657.</p> </li> <li> <p>https://vall-e.io/    C. Wang u. a., \u201eNeural Codec Language Models are Zero-Shot Text to Speech Synthesizers\u201c. arXiv, 5. Januar 2023. Zugegriffen: 1. Mai 2023. [Online]. Verf\u00fcgbar unter: http://arxiv.org/abs/2301.02111</p> </li> <li> <p>https://www.researchgate.net/profile/Hazem-El-Bakry/publication/228673642_An_overview_of_text-to-speech_synthesis_techniques/links/553fa8270cf2320416eb23ed/An-overview-of-text-to-speech-synthesis-techniques.pdf    M. Rashad, H. El-Bakry, R. Isma, und N. Mastorakis, \u201eAn overview of text-to-speech synthesis techniques\u201c, International Conference on Communications and Information Technology - Proceedings, Juli 2010.</p> </li> <li> <p>J. Shen u.\u00a0a., \u201eNatural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions\u201c. arXiv, 15. Februar 2018. doi: 10.48550/arXiv.1712.05884.</p> </li> <li> <p>\u201eAudio samples from \u201aNatural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions\u2018\u201c. https://google.github.io/tacotron/publications/tacotron2/ (zugegriffen 4. Juli 2023).</p> </li> <li> <p>\u201eEnCodec: High Fidelity Neural Audio Compression\u201c. Meta Research, 5. Juli 2023. Zugegriffen: 5. Juli 2023. [Online]. Verf\u00fcgbar unter: https://github.com/facebookresearch/encodec</p> </li> <li> <p>N. Li, S. Liu, Y. Liu, S. Zhao, M. Liu, und M. Zhou, \u201eNeural Speech Synthesis with Transformer Network\u201c. arXiv, 30. Januar 2019. Zugegriffen: 6. Juli 2023. [Online]. Verf\u00fcgbar unter: http://arxiv.org/abs/1809.08895</p> </li> <li> <p>Y. Jia u. a., \u201eTransfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis\u201c. arXiv, 2. Januar 2019. Zugegriffen: 6. Juli 2023. [Online]. Verf\u00fcgbar unter: http://arxiv.org/abs/1806.04558</p> </li> <li> <p>K. Tokuda, Y. Nankaku, T. Toda, H. Zen, J. Yamagishi, und K. Oura, \u201eSpeech Synthesis Based on Hidden Markov Models\u201c, Proc. IEEE, Bd. 101, Nr. 5, S. 1234\u20131252, Mai 2013, doi: 10.1109/JPROC.2013.2251852.</p> </li> </ol>"},{"location":"Themen/Zeitserienanalyse/","title":"Zeitserienanalyse","text":"<p>von Andreas Greiner, Julian Ivanov und Stephan Zahnweh</p>"},{"location":"Themen/Zeitserienanalyse/#abstract","title":"Abstract","text":"<p>Sei es in der Produktion, der Energieversorgung oder im Finanzwesen, heutzutage ist es einfacher denn je gro\u00dfe Datenmengen aufzunehmen und als Zeitreihen abzuspeichern. Neben klassischen Methoden gibt es auch im Bereich Deep Learning viele neue M\u00f6glichkeiten, diese Daten sinnvoll zu nutzen. Daher wurde in dieser Arbeit einen Podcast so wie ein Fachvortrag und eine Code-Demonstration zum Thema Zeitreihenanalyse erarbeitet und eine schriftliche Ausarbeitung dazu erstellt.</p> <p>Der Podcast liefert oberfl\u00e4chliche Informationen zu Zeitserien, deren Verarbeitung mit klassischen und modernen Methoden und Anwendungsgebiete. Er dient dazu, jedermann einen groben Einblick in das Thema zu bieten und etwaiges Interesse zu wecken. Dabei ist er gr\u00f6\u00dftenteils an Personen ohne Vorwissen in Datenverarbeitung oder Deep Learning adressiert. </p> <p>Der Fachvortrag hingegen liefert tiefe Einblicke in den Aufbau und die Merkmale von Zeitserien.  Es werden klassische Ans\u00e4tze wie exponentielles Gl\u00e4tten oder SARIMA vorgestellt und zum Vergleich auch moderne Methoden wie Recurrent Neural Networks und LSTMs pr\u00e4sentiert. Insgesamt geht der Fachvortrag deutlich tiefer in die Materie hinein und stellt auch die Mathematik hinter den Algorithmen vor. </p> <p>Als letzter Teil der Arbeit werden verschiedene Methoden anhand einer Code-Demonstration vorgestellt und miteinander verglichen. Es wird vorgestellt, wie man einen Beispieldatensatz mittels SARIMA-Modell, Prophet-Algorithmus von Facebook und LSTM verarbeiten kann.  Dazu werden mit den jeweiligen Modellen Vorhersagen erstellt und die Genauigkeit deren miteinander verglichen. Das Code-Beispiel steht frei zur Verf\u00fcgung und kann von jedem ausgef\u00fchrt und abge\u00e4ndert werden.</p>"},{"location":"Themen/Zeitserienanalyse/#1-einleitung-motivation","title":"1 Einleitung / Motivation","text":"<p>Die Zeitserienanalyse ist eine sehr effektive Methode der Analyse und Interpretation von Daten, welche sich im Laufe der Zeit \u00e4ndern. Sie erm\u00f6glicht es, Ph\u00e4nomene wie Muster oder Trends in Daten zu verstehen und dieses Verst\u00e4ndnis f\u00fcr Vorhersagen und Prognosen oder zur Erkennung von Anomalien zu nutzen. Dieses Werkzeug kann in verschiedensten Bereichen der Daten wie etwa dem Finanzwesen, bei Wettervorhersagen, in der Medizin oder in etlichen weiteren Gebieten Anwendung finden. Hierbei ist das einzig Wichtige, dass sinnvolle Daten erhoben werden k\u00f6nnen und diese Daten ein zu interpretierendes Muster aufweisen. </p> <p>Anwendungsbeispiele sind zum Beispiel die Langzeit\u00fcberwachung von Herzfrequenz oder Blutdruck zur fr\u00fchzeitigen Erkennung von Krankheiten oder die Analyse von Kursverl\u00e4ufen von Aktien oder W\u00e4hrungen, um gezielt Vorhersagen treffen zu k\u00f6nnen und so am Finanzmarkt Profit zu erzielen. Auch in der Produktion findet die Zeitserienanalyse viele Anwendungsbereiche, etwa die Planung von Lieferketten, um im Zeitalter von Just-In-Time-Delivery die Produktionsketten so effizient wie m\u00f6glich zu gestalten. W\u00e4hrend der Corona-Pandemie wurden sicherlich auch Simulationen basierend auf Zeitserien durchgef\u00fchrt, um die Auswirkungen von Lockdowns, Impfungen und Booster-Impfungen zu analysieren und prognostizieren.</p> <p>In vielen dieser Bereiche wird Zeitserienanalyse bereits lange mit klassischen Modellen betrieben, und das oft auch sehr erfolgreich. Diese Modelle basieren auf statistischen Ans\u00e4tzen und der Annahme, dass die Muster in den Daten anhand mathematischer Funktionen beschrieben werden k\u00f6nnen. W\u00e4hrend diese Methoden eher als klassische Methoden bezeichnet werden, haben sich in den letzten Jahren auch immer mehr modernere Ans\u00e4tze mit Deep-Learning Methoden etabliert. Diese nutzen verschiedene Versionen von neuronale Netzen wie RNNs oder LSTMs, um auch komplexere Muster wie nicht-lineare Abh\u00e4ngigkeiten in den Daten zu erkennen und modellieren zu k\u00f6nnen. Im Gegensatz zu den klassischen Methoden ben\u00f6tigen diese Ans\u00e4tze jedoch meist gro\u00dfe Datens\u00e4tze, um gute Prognosen erstellen zu k\u00f6nnen.</p> <p>Damit eine Zeitreihenanalyse somit gute Ergebnisse erzielen kann, muss je nach Problemstellung und Datenlage ein sinnvolles Verfahren gew\u00e4hlt werden. Neben der Gr\u00f6\u00dfe des Datensatzes ist auch Expertenwissen \u00fcber die Daten entscheiden, um eine gute Auswahl der verf\u00fcgbaren Variablen treffen zu k\u00f6nnen. Au\u00dferdem besitzen Datens\u00e4tze oft Ausrei\u00dfer oder fehlende Datenpunkte und m\u00fcssen daher \u00fcberpr\u00fcft und angepasst werden. </p> <p>Um somit eine gute \u00dcbersicht \u00fcber verschiedene Ans\u00e4tze und ein Grundverst\u00e4ndnis \u00fcber die n\u00f6tige Datenaufbereitung zu haben, werden im Folgenden klassische sowie moderne Ans\u00e4tze pr\u00e4sentiert und anhand eines Beispiels eine Zeitreihenanalyse durchgef\u00fchrt.</p>"},{"location":"Themen/Zeitserienanalyse/#2-methoden","title":"2 Methoden","text":"<p>Bei der Recherche rund um das Thema Zeitserienanalyse st\u00f6\u00dft man immer wieder auf eine m\u00f6gliche Definition f\u00fcr Zeitserien.</p> <p>Definition</p> <p>Die Zeitserienanalyse tr\u00e4gt der Tatsache Rechnung, dass Datenpunkte, die im Laufe der Zeit aufgenommen wurden, eine interne Struktur aufweisen k\u00f6nnen (wie Autokorrelation, Trend oder saisonale Schwankungen), die ber\u00fccksichtigt werden sollte.</p> <p>Damit grenzt sich diese von anderen Datenanalysen ohne Zeitkomponente ab. Im Folgenden werden die internen Strukturen von Zeitserien n\u00e4her erl\u00e4utert.</p>"},{"location":"Themen/Zeitserienanalyse/#21-merkmale-von-zeitserien","title":"2.1 Merkmale von Zeitserien","text":"<p>Hier werden die Merkmale von Zeitserien (Autokorrelation, Trend und Saison) beschrieben.</p>"},{"location":"Themen/Zeitserienanalyse/#autokorrelation","title":"Autokorrelation","text":"<p>Autokorrelation bezieht sich auf die statistische Beziehung oder \u00c4hnlichkeit zwischen den Werten einer Zeitreihe und ihren verz\u00f6gerten Versionen. In einer Zeitreihe werden aufeinanderfolgende Beobachtungen in regelm\u00e4\u00dfigen Zeitintervallen erfasst. Autokorrelation misst die St\u00e4rke und Richtung des linearen Zusammenhangs zwischen den Werten einer Zeitreihe und den Werten zu verschiedenen Verz\u00f6gerungszeiten. Zum Veranschaulichen dieser und weiterer Methoden wird ein Beispiel Datensatz (Fig. 1) verwendet. Hierbei handelt es sich um einen Datensatz mit monatlich gemessenen CO2 Aussto\u00df \u00fcber mehrere Jahre hinweg. </p> <p> </p> Fig. 1 Beispiel Datensatz <p>Die Autokorrelationsfunktion (ACF) zeigt den Autokorrelationskoeffizient \\(r_k\\) f\u00fcr verschieden Verz\u00f6gerungen \\(k\\). Er ist ein Ma\u00df f\u00fcr die Autokorrelation und berechnet sich wie folgt: </p> \\[ r_k = \\frac{\\sum_{t=k+1}^{n} (y_t - \u04ef)(y_{t-k} - \u04ef)}{\\sum_{t=1}^{n} (y_t - \u04ef)^2} \\] \\[ \u04ef = \\frac{1}{n}\\sum_{t=1}^{n}(y_t) \\] <p>Er variiert zwischen -1 und 1. Ein Wert von 1 zeigt eine perfekte positive Autokorrelation an, was bedeutet, dass eine hohe \u00c4hnlichkeit zwischen den Werten der Zeitreihe und ihren verz\u00f6gerten Versionen besteht. Ein Wert von -1 zeigt eine perfekte negative Autokorrelation an, was darauf hindeutet, dass die Werte der Zeitreihe und ihre verz\u00f6gerten Versionen genau entgegengesetzte Muster aufweisen. Ein Wert von 0 deutet auf keine Autokorrelation hin, was bedeutet, dass es keine systematische Beziehung zwischen den Werten und ihren Verz\u00f6gerungen gibt. </p> <p> </p> Fig. 2 ACF Plot"},{"location":"Themen/Zeitserienanalyse/#trend","title":"Trend","text":"<p>Der Trend bezieht sich auf die langfristige Ver\u00e4nderung des Mittelwerts einer Zeitreihe im Laufe der Zeit. Es ist wichtig den Trend zu bestimmen, da einige Modelle nur mit station\u00e4ren Zeitserien umgehen k\u00f6nnen. In der Zeitreihenanalyse bedeutet \"station\u00e4r\", dass die statistischen Eigenschaften einer Zeitreihe im Laufe der Zeit konstant bleiben oder nicht von der Zeit abh\u00e4ngen. Um die Stationarit\u00e4t einer Zeitreihe zu \u00fcberpr\u00fcfen, k\u00f6nnen verschiedene statistische Tests wie der Augmented Dickey-Fuller (ADF)-Test verwendet werden. Dieser Test bewertet die Nullhypothese, dass eine Zeitreihe nicht station\u00e4r ist, und liefert Informationen dar\u00fcber, ob die Zeitreihe einer Transformation bedarf, um station\u00e4r zu werden. F\u00fcr den Beispiel Datensatz (Fig. 1) ergibt sich mit dem ADF-Test ein p-Wert von \u00fcber 0.9, was einen eindeutigen Trend belegt.</p>"},{"location":"Themen/Zeitserienanalyse/#saison","title":"Saison","text":"<p>Bei der Saison handelt es sich um wiederkehrende Muster oder periodische Schwankungen in den Daten einer Zeitreihe, die mit bestimmten Zeitr\u00e4umen zusammenh\u00e4ngen. Diese Muster k\u00f6nnen t\u00e4gliche, w\u00f6chentliche, monatliche, quartalsweise oder j\u00e4hrliche Perioden umfassen. F\u00fcr den Beispiel Datensatz (Fig. 1) l\u00e4sst sich eine Saison Komponente aus dem ACF Plot (Fig.2) sehr gut ablesen. Man erkennt ein wiederholtes lokales Maximum, dass in einem festen Intervall von 12 auftaucht. Dies deutet auf eine J\u00e4hrliche Periode hin.</p> <p> </p> Fig. 3 ACF Plot Saison"},{"location":"Themen/Zeitserienanalyse/#22-anwendungsgebiete","title":"2.2 Anwendungsgebiete","text":"<p>Die Zeitreihenanalyse ist ein leistungsstarkes Werkzeug zur Untersuchung und Extraktion von Informationen aus sequenziellen Daten. Sie findet in verschiedenen Anwendungsgebieten Anwendung, darunter die Analyse der Merkmale, Vorhersage, Gl\u00e4ttung und Anomalieerkennung. Jedes dieser Gebiete spielt eine wichtige Rolle bei der Auswertung und Interpretation von Zeitreihendaten.</p>"},{"location":"Themen/Zeitserienanalyse/#analyse-der-merkmale","title":"Analyse der Merkmale","text":"<p>Die Analyse der Merkmale befasst sich mit der Untersuchung und Identifizierung von Mustern, Trends, Saisonalit\u00e4t und anderen Charakteristika in einer Zeitreihe. Sie hilft dabei, grundlegende Informationen \u00fcber die Daten zu gewinnen und Einblicke in vergangene Muster und Ver\u00e4nderungen zu gewinnen. Die Werkzeuge zur Merkmalsanalyse wurden bereits im vorherigen Kapitel ausf\u00fchrlich erl\u00e4utert.</p>"},{"location":"Themen/Zeitserienanalyse/#vorhersage","title":"Vorhersage","text":"<p>Die Vorhersage ist ein wichtiger Anwendungsbereich der Zeitreihenanalyse. Sie befasst sich mit der Sch\u00e4tzung zuk\u00fcnftiger Werte oder Ereignisse aufgrund vergangener Daten. Durch die Analyse von Trends, saisonalen Mustern und anderen Zeitreiheneigenschaften k\u00f6nnen Modelle entwickelt werden, um Vorhersagen f\u00fcr die Zukunft zu treffen. Methoden wie ARIMA (Autoregressive Integrated Moving Average), Exponential Smoothing und Machine-Learning-Algorithmen werden h\u00e4ufig verwendet, um Prognosen zu generieren. Diese Ans\u00e4tze werden in den n\u00e4chsten Kapiteln genauer betrachtet.</p>"},{"location":"Themen/Zeitserienanalyse/#glattung","title":"Gl\u00e4ttung","text":"<p>Die Gl\u00e4ttung bezieht sich auf den Prozess der Reduzierung von Rauschen, Unregelm\u00e4\u00dfigkeiten oder kurzfristigen Schwankungen in einer Zeitreihe, um den zugrunde liegenden Trend oder das Muster deutlicher sichtbar zu machen. Durch die Anwendung von Gl\u00e4ttungstechniken k\u00f6nnen saisonale Effekte, Ausrei\u00dfer und zuf\u00e4llige Schwankungen gegl\u00e4ttet werden, um den Trend oder das langfristige Verhalten der Daten zu analysieren.</p>"},{"location":"Themen/Zeitserienanalyse/#anomalieerkennung","title":"Anomalieerkennung","text":"<p>Die Anomalieerkennung in Zeitreihen besch\u00e4ftigt sich mit der Identifizierung von ungew\u00f6hnlichen oder abweichenden Mustern in den Daten. Anomalien k\u00f6nnen Ausrei\u00dfer, unerwartete Ver\u00e4nderungen, Ausf\u00e4lle oder andere abnormale Ereignisse sein, die in der Zeitreihe auftreten. Die Zeitreihenanalyse kann verwendet werden, um solche Anomalien zu erkennen und zu charakterisieren, indem statistische Methoden, Mustererkennungsalgorithmen und maschinelles Lernen eingesetzt werden.</p>"},{"location":"Themen/Zeitserienanalyse/#23-klassische-ansatze","title":"2.3 Klassische Ans\u00e4tze","text":"<p>Bei der Zeitreihenanalyse gibt es verschiedene klassische Modelle, die zur Modellierung und Vorhersage von Zeitreihendaten verwendet werden. Diese Modelle basieren auf statistischen Methoden und Annahmen \u00fcber die Struktur der Daten. Holt-Winters exponentielles Gl\u00e4tten und Box-Jenkins SARIMA Modelle sind nach deren Erfindern benannt. Beide werden in den n\u00e4chsten Abschnitten vorgestellt.</p>"},{"location":"Themen/Zeitserienanalyse/#exponentielles-glatten","title":"Exponentielles Gl\u00e4tten","text":"<p>Exponentielles Gl\u00e4tten ist eine g\u00e4ngige Methode in der Zeitreihenanalyse, um saisonale Muster, Trends und kurzfristige Schwankungen zu reduzieren und den zugrunde liegenden Trend oder das Muster einer Zeitreihe deutlicher zu erkennen. Es handelt sich um eine einfache und effektive Methode, die auf der Annahme basiert, dass aktuelle Werte einer Zeitreihe st\u00e4rker gewichtet werden sollten als vergangene Werte.</p> <p>Der Hauptgedanke beim exponentiellen Gl\u00e4tten besteht darin, jedem Datenpunkt ein Gewicht zuzuweisen, wobei die Gewichte exponentiell abnehmen, je weiter der Datenpunkt in der Vergangenheit liegt. Das Gewichtungsschema wird durch einen Gl\u00e4ttungsfaktor (auch als Gl\u00e4ttungsparameter oder Alpha-Faktor bezeichnet) gesteuert, der zwischen 0 und 1 liegt.</p> <p>Der Prozess des einfachen exponentiellen Gl\u00e4ttens kann in mehreren Schritten zusammengefasst werden:</p> <ul> <li>Initialisierung:   Der erst m\u00f6gliche gesch\u00e4tzte Wert \\(S_2\\) wird als der erste Beobachtungswert \\(y_1\\) der Zeitreihe angenommen.</li> </ul> \\[ S_2 = y_1 \\] <ul> <li> <p>Gl\u00e4ttungsschritt:   F\u00fcr die folgenden Beobachtungen wird der gesch\u00e4tzte Wert durch die Kombination des aktuellen Beobachtungswerts und des vorherigen gesch\u00e4tzten Werts berechnet. Dies wird durch die Formel dargestellt:  </p> \\[ S_3 = \\alpha y_2 + (1 - \\alpha)S_2 \\] <p>Dabei ist \\(\\alpha\\) der Gl\u00e4ttungsfaktor, der angibt, wie stark der aktuelle Wert gewichtet werden soll. Ein kleinerer Wert von \\(\\alpha\\) gibt den vergangenen Werten ein h\u00f6heres Gewicht, w\u00e4hrend ein gr\u00f6\u00dferer Wert von \\(\\alpha\\) den aktuellen Wert st\u00e4rker ber\u00fccksichtigt.</p> </li> <li> <p>Wiederholung:   Dieser Gl\u00e4ttungsschritt wird f\u00fcr jede Beobachtung in der Zeitreihe wiederholt, wobei der gesch\u00e4tzte Wert bei jedem Schritt aktualisiert wird.</p> </li> </ul> \\[ S_t = \\alpha y_{t-1} + (1 - \\alpha)S_{t-1}; 0 &lt; \\alpha \\leq 1; t \\geq 3 \\] <p> </p> Fig. 4 Einfaches Exponentielles Gl\u00e4tten <p>Das linke Diagramm zeigt die gegl\u00e4ttete Zeitserie und das Rechte die Ergebnisse einer Vorhersage auf den in Test- und Trainingsdaten aufgeteilten Beispieldatensatz (Fig. 1). Die Vorhersagen sind nicht sonderlich gut, da sowohl Trend als auch Saison Komponenten nicht ber\u00fccksichtigt werden.</p> <p>Um den Trend zu erfassen, wird das Modell um eine Trendkomponente erweitert. Hier handelt es sich um das zweifache exponentielle Gl\u00e4tten.</p> \\[ S_t = \\alpha y_{t-1} + (1 - \\alpha)(S_{t-1} + b_{t-1}) \\] \\[ b_t = \\gamma (S_{t} - S_{t-1}) + (1 - \\gamma)b_{t-1} \\] <p> </p> Fig. 5 Zweifaches Exponentielles Gl\u00e4tten <p>Hier wird nun der Trend mitber\u00fccksichtigt. Man sieht bei der Gl\u00e4ttung, dass der Trend erkannt wird, jedoch eine \u00c4nderung im Trend erst verz\u00f6gert wahrgenommen wird. Auch die Vorhersagen k\u00f6nnen nur den aktuellen Trend weiterf\u00fchren und keine \u00c4nderungen vorhersagen. Das liegt daran, dass die Saison Komponente fehlt um brauchbare Vorhersagen treffen zu k\u00f6nnen.</p> <p>Beim dreifachen exponentiellen Gl\u00e4tten wird auch diese Ber\u00fccksichtigt. Nun ergibt sich das folgende Modell:</p> \\[ S_t = \\alpha y_{t-1} + (1 - \\alpha)(S_{t-1} + b_{t-1}) \\] \\[ b_t = \\gamma (S_{t} - S_{t-1}) + (1 - \\gamma)b_{t-1} \\] \\[ I_t = \\beta \\frac{y_{t}}{S_t} + (1 - \\beta)I_{t-L} \\] <p> </p> Fig. 6 Dreifaches Exponentielles Gl\u00e4tten <p>Das Ergebnis liefert eine gute Gl\u00e4ttung und brauchbare Vorhersagen. Es ist wichtig zu beachten, dass diese Methode f\u00fcr Zeitreihen geeignet ist, die keine komplexe Struktur oder starke saisonale Komponenten aufweisen. F\u00fcr Zeitreihen mit starken saisonalen Mustern oder anderen komplexen Eigenschaften k\u00f6nnen fortgeschrittenere Modelle wie ARIMA oder saisonale ARIMA (SARIMA) verwendet werden. Diese werden im n\u00e4chsten Abschnitt erl\u00e4utert.</p>"},{"location":"Themen/Zeitserienanalyse/#sarima","title":"SARIMA","text":"<p>Das saisonale ARIMA-Modell (SARIMA) ist eine Erweiterung des ARIMA-Modells (Autoregressive Integrated Moving Average) und wird verwendet, um Zeitreihendaten mit saisonalen Mustern zu modellieren und Vorhersagen zu generieren. SARIMA kombiniert Autoregression (AR), Integration (I) und Moving Average (MA) mit saisonalen Komponenten.</p> <p>Das SARIMA-Modell besteht aus mehreren Parametern, die seine Eigenschaften definieren:</p> <ul> <li> <p>Autoregressive (AR)-Komponente:   Die AR-Komponente des SARIMA-Modells ber\u00fccksichtigt die Abh\u00e4ngigkeit des aktuellen Werts einer Zeitreihe von vergangenen Werten. Der Parameter p gibt die Anzahl der vorherigen Werte an, die ber\u00fccksichtigt werden sollen. </p> </li> <li> <p>Differenzierung (I):   Die Differenzierungskomponente des SARIMA-Modells wird verwendet, um die Zeitreihe station\u00e4r zu machen. Stationarit\u00e4t bedeutet, dass der Mittelwert und die Varianz der Daten \u00fcber die Zeit konstant bleiben. Der Parameter d gibt die Anzahl der Differenzierungen an, die erforderlich sind, um die Stationarit\u00e4t zu erreichen.</p> </li> <li> <p>Moving Average (MA)-Komponente:   Die MA-Komponente des SARIMA-Modells bezieht sich auf den Einfluss der vorherigen Fehler auf den aktuellen Wert einer Zeitreihe. Der Parameter q gibt die Anzahl der vorherigen Fehler an, die ber\u00fccksichtigt werden sollen. </p> </li> <li> <p>Saisonale AR-Komponente:   Die saisonale AR-Komponente ber\u00fccksichtigt die saisonale Abh\u00e4ngigkeit einer Zeitreihe. Sie bezieht sich auf die Abh\u00e4ngigkeit des aktuellen Werts von vergangenen Werten, die eine bestimmte Anzahl von Zeitschritten vor der aktuellen Periode liegen. Der Parameter P gibt die Anzahl der saisonalen AR-Terme an.</p> </li> <li> <p>Saisonale Differenzierung:   Die saisonale Differenzierung wird verwendet, um die saisonale Komponente der Zeitreihe zu entfernen und die Daten station\u00e4r zu machen. Der Parameter D gibt die Anzahl der saisonalen Differenzierungen an.</p> </li> <li> <p>Saisonale MA-Komponente:   Die saisonale MA-Komponente bezieht sich auf den Einfluss der vergangenen saisonalen Fehler auf den aktuellen Wert einer Zeitreihe. Der Parameter Q gibt die Anzahl der saisonalen MA-Terme an.</p> </li> <li> <p>Saisonaler Index:   Der Saison Index m gibt die L\u00e4nge einer Periode an.</p> </li> </ul> <p>Zusammen definieren diese Parameter die Struktur des SARIMA-Modells. </p> \\[ SARIMA(p,d,q)(P,D,Q,m) \\] <p>Durch die Sch\u00e4tzung dieser Parameter und die Anpassung des Modells an die Daten k\u00f6nnen Vorhersagen f\u00fcr zuk\u00fcnftige Werte der Zeitreihe generiert werden. Dies erm\u00f6glicht die Ber\u00fccksichtigung von saisonalen Mustern und den Einfluss vergangener Werte und Fehler auf die Vorhersagen.</p> <p> </p> Fig. 7 SARIMA <p>Die Grafik zeigt eine Vorhersage mit Hilfe des SARIMA Modells f\u00fcr den Beispiel Datensatz (Fig. 1). Die Ergebnisse sind sehr gut, da es sich um eine nicht sonderlich komplexe Zeitserie handelt. Obwohl SARIMA-Modelle f\u00fcr die Modellierung von einfachen Zeitreihen mit saisonalen Mustern recht n\u00fctzlich sind, haben sie einige Nachteile beim Modellieren komplexer Strukturen. Hier haben moderne Ans\u00e4tze weitaus besser L\u00f6sungen, um komplexe Muster zu erfassen. </p>"},{"location":"Themen/Zeitserienanalyse/#24-moderne-ansatze","title":"2.4 Moderne Ans\u00e4tze","text":"<p>Moderne Ans\u00e4tze in der Zeitreihenanalyse haben in den letzten Jahren an Bedeutung gewonnen und bieten erweiterte M\u00f6glichkeiten zur Modellierung und Vorhersage von Zeitreihendaten. Neuronale Netze, insbesondere rekurrente neuronale Netze (RNNs) wie Long Short-Term Memory (LSTM) oder Gated Recurrent Units (GRUs), haben sich als leistungsstarke Werkzeuge f\u00fcr die Modellierung von Zeitreihendaten erwiesen. Diese Modelle k\u00f6nnen komplexe nichtlineare Muster erfassen und sind in der Lage, langfristige Abh\u00e4ngigkeiten in den Daten zu ber\u00fccksichtigen.</p>"},{"location":"Themen/Zeitserienanalyse/#recurrent-neural-networks","title":"Recurrent Neural Networks","text":"<p>Simple RNNs (Recurrent Neural Networks) sind eine Art von neuronalen Netzwerken, die f\u00fcr die Analyse von Zeitreihendaten verwendet werden. Im Gegensatz zu herk\u00f6mmlichen neuronalen Netzwerken, die nur eine sequenzielle Verarbeitung von Daten erm\u00f6glichen, haben RNNs die F\u00e4higkeit, Informationen \u00fcber vergangene Schritte beizubehalten und in zuk\u00fcnftigen Schritten zu verwenden.  </p> <p></p> <p>Die grundlegende Struktur eines einfachen RNNs besteht aus einer Schleife, die es erm\u00f6glicht, Informationen \u00fcber vergangene Zeitschritte zu speichern und zur\u00fcckzugeben. Bei der Verarbeitung von Zeitreihendaten wird das RNN f\u00fcr jeden Zeitschritt in der Sequenz iterativ ausgef\u00fchrt. F\u00fcr jeden Zeitschritt werden sowohl der aktuelle Eingabewert als auch der vorherige Zustand des RNNs als Eingabe verwendet, um den Ausgabewert zu generieren. Der Ausgabewert kann entweder als Vorhersage f\u00fcr den n\u00e4chsten Zeitschritt oder als Teil eines umfassenderen Vorhersagemodells verwendet werden.</p> <p> </p> Fig. 8 Simple RNN <p>Die Hauptvorteile von einfachen RNNs f\u00fcr die Zeitreihenanalyse liegen in ihrer F\u00e4higkeit, zeitliche Abh\u00e4ngigkeiten und Muster in den Daten zu erfassen. Durch die Verwendung des vorherigen Zustands als zus\u00e4tzliche Information kann das RNN kontextbezogene Vorhersagen treffen und komplexe Muster in den Daten erkennen. Beispielsweise w\u00fcrde ein simple RNN im Satz \"Die Wolken sind im _\", das Wort Himmel sehr leicht aus dem Kontext einf\u00fcgen k\u00f6nnen.</p> <p> </p> Fig. 9 Kurzer Kontext <p>Es gibt jedoch auch einige Herausforderungen bei der Verwendung von einfachen RNNs f\u00fcr Zeitreihendaten. Ein Problem ist das sogenannte \"Vanishing Gradient\"-Problem, bei dem die Gradienten w\u00e4hrend des Trainings exponentiell abnehmen und dazu f\u00fchren k\u00f6nnen, dass vergangene Informationen nicht gut in die Vorhersagen einbezogen werden. Dies kann die F\u00e4higkeit des RNNs zur Modellierung langfristiger Abh\u00e4ngigkeiten beeintr\u00e4chtigen. So wird es bei dem Satz \"Die Wolken, die verschiedene Gr\u00f6\u00dfen und Graustufen haben, sind im _\", sehr schwer das Wort Himmel aus dem Kontext einzusetzen.</p> <p> </p> Fig. 10 Langer Kontext <p>Um das Vanishing Gradient-Problem zu \u00fcberwinden und die Leistung von RNNs zu verbessern, wurden verschiedene Weiterentwicklungen vorgeschlagen, wie zum Beispiel Long Short-Term Memory (LSTM) und Gated Recurrent Units (GRU). Diese Modelle verwenden spezielle Strukturen, um das Ged\u00e4chtnis der RNNs zu verbessern und langfristige Abh\u00e4ngigkeiten besser zu erfassen.</p>"},{"location":"Themen/Zeitserienanalyse/#long-short-term-memory-rnns","title":"Long Short Term Memory RNNs","text":"<p>LSTM (Long Short-Term Memory) ist eine Weiterentwicklung von RNNs (Recurrent Neural Networks) und wurde entwickelt, um das Problem des \"Vanishing Gradient\" zu l\u00f6sen und langfristige Abh\u00e4ngigkeiten in Zeitreihendaten besser zu erfassen. LSTM-RNNs haben sich als \u00e4u\u00dferst effektiv f\u00fcr die Zeitreihenanalyse erwiesen.</p> <p> </p> Fig. 11 Simple RNN vs LSTM Architektur <p>Der wesentliche Unterschied zwischen LSTM und einfachen RNNs besteht darin, dass LSTM \u00fcber eine sogenannte \"Ged\u00e4chtniszelle\" und \"Gatter\" verf\u00fcgt. Diese erm\u00f6glichen es, Informationen \u00fcber lange Zeitschritte hinweg zu speichern und zu vergessen. Diese spezielle Architektur erm\u00f6glicht es LSTM Modellen, wichtige Informationen zu behalten und irrelevante Informationen zu verwerfen. Insgesamt ergibt sich daraus das folgende Modell:</p> <p></p> <ul> <li>Ged\u00e4chtniszelle (Memory Cell):   Die Memory Cell besteht aus einer internen Zellzustandsvariable, die Informationen \u00fcber den aktuellen Zustand der Ged\u00e4chtniszelle enth\u00e4lt. Diese Variable wird w\u00e4hrend der Verarbeitung der Zeitreihe aktualisiert und kann Informationen \u00fcber relevante Muster und Abh\u00e4ngigkeiten speichern.</li> </ul> <p> </p> Fig. 12 Memory Cell <ul> <li>Eingangsgatter (Input Gate):   Das Eingangsgatter \\(i_t=\\sigma(U_i h_{t-1} + W_i x_t + b_i)\\) regelt, welche Informationen aus dem aktuellen Zeitschritt in das Zellged\u00e4chtnis \u00fcbernommen werden sollen. Es verwendet eine Sigmoid-Aktivierungsfunktion, um zu bestimmen, welche Werte aktualisiert werden sollen.</li> </ul> <p> </p> Fig. 13 Input Gate <ul> <li>Vergessensgatter (Forget Gate):   Das Vergessensgatter \\(f_t=\\sigma(U_f h_{t-1} + W_f x_t + b_f)\\) bestimmt, welche Informationen aus dem vorherigen Zustand des LSTM verworfen werden sollen. Es hilft dabei, irrelevante Informationen zu vergessen und relevante Informationen beizubehalten. Es verwendet auch eine Sigmoid-Aktivierungsfunktion, um zu bestimmen, welche Werte verworfen werden sollen.</li> </ul> <p> </p> Fig. 14 Forget Gate <ul> <li>Ausgangsgatter (Output Gate):   Das Ausgangsgatter \\(i_t=\\sigma(U_o h_{t-1} + W_o x_t + b_o)\\) bestimmt, welche Informationen aus dem aktuellen Zeitschritt als Ausgabe verwendet werden sollen. Es verwendet sowohl die vorherigen Zust\u00e4nde des LSTM als auch die aktualisierten Werte des Eingangsgatters und der Zellaktivierungsfunktion, um die Ausgabe zu generieren.</li> </ul> <p> </p> Fig. 15 Output Gate <p>Durch die Verwendung dieser Architektur kann ein LSTM Informationen \u00fcber lange Zeitschritte hinweg behalten und langfristige Abh\u00e4ngigkeiten in den Daten erfassen. Es kann wichtige Muster und Zusammenh\u00e4nge in der Zeitreihe erkennen und diese Informationen zur Vorhersage zuk\u00fcnftiger Werte verwenden.</p>"},{"location":"Themen/Zeitserienanalyse/#25-vergleich","title":"2.5 Vergleich","text":"<p>Bei der Betrachtung von Modellen f\u00fcr Zeitreihenanalyse k\u00f6nnen sowohl klassische als auch moderne Methoden ihre eigenen Vor- und Nachteile bieten. Die Wahl des geeigneten Modells h\u00e4ngt von den spezifischen Anforderungen des Anwendungsfalls ab.</p> <p>Klassische Methoden zeichnen sich durch ihre Einfachheit in der Implementierung, interpretierbare Ergebnisse und die F\u00e4higkeit aus, mit wenigen Trainingsdaten gute Vorhersagen zu liefern. Diese Methoden, wie zum Beispiel ARIMA, eignen sich gut f\u00fcr station\u00e4re Daten und Anwendungen, bei denen es wichtig ist, die zugrunde liegende Struktur der Zeitreihe zu verstehen. Sie bieten solide Grundlagen und sind in vielen praktischen Szenarien immer noch effektiv einsetzbar.</p> <p>Auf der anderen Seite bieten moderne Methoden, wie zum Beispiel LSTM Modelle, erweiterte M\u00f6glichkeiten zur Modellierung von Zeitreihen. Sie sind in der Lage, auch komplexe Muster und nichtlineare Zusammenh\u00e4nge in den Daten zu erlernen. Moderne Methoden sind flexibler und k\u00f6nnen besser mit nicht station\u00e4ren Daten umgehen, was in vielen realen Anwendungen von Vorteil ist. Sie k\u00f6nnen auch langfristige Prognosen liefern und sind in der Lage, komplexe Strukturen in den Daten zu modellieren.</p>"},{"location":"Themen/Zeitserienanalyse/#3-anwendungen","title":"3 Anwendungen","text":"<p>Im Folgenden wird anhand eines konkreten Beispiels die Vorgehensweise bei der Implementierung von Zeitreihenanalysen erl\u00e4utert.</p> <p>Es geht um den st\u00fcndlichen Energieverbrauch in Amerika. Der Datensatz wurde zun\u00e4chst eingelesen und dargestellt. Er enth\u00e4lt zwei Spalten. Eine mit Datum und Zeit und eine mit den Energiewerten.</p> <p> </p> Fig. 16 Energy Use in MW <p>Als n\u00e4chstes wurden die Daten aufbereitet. Dabei wurden Ausrei\u00dfer Werte, die man oben in der Graphik erkennt, als auch fehlende Werte behandelt.</p> <p>Ein weiterer wichtiger Schritt bei der Aufbereitung der Daten es den Index richtig zu setzen. Da wir es mit einer Zeitreihe zu tun haben, m\u00fcssen wir die Spalte mit dem Datum und der Zeit (\"Datetime\") im Datensatz als unseren Index setzen.</p> <p> </p> Fig. 17 PJME data <p>Indem der Index auf die Datums-/Zeitspalte gesetzt wird, erm\u00f6glicht es uns pandas, zeitbasierte Operationen effizient durchzuf\u00fchren. Wir k\u00f6nnen auf einfache Weise auf bestimmte Zeitr\u00e4ume zugreifen, Daten nach Zeitintervallen aggregieren oder Zeitreihenplots erstellen.</p> <p>Daf\u00fcr m\u00fcssen wir die Zeitmerkmale noch konstruieren. Dies ist jedoch dank pandas schnell getan, nachdem wir den Index richtig gesetzt haben.</p> <pre><code>def create_features(df):\n\"\"\"\n    Create time series features based on time series index.\n    \"\"\"\n    df = df.copy()\n    df['hour'] = df.index.hour\n    df['dayofweek'] = df.index.dayofweek\n    df['quarter'] = df.index.quarter\n    df['month'] = df.index.month\n    df['year'] = df.index.year\n    df['dayofyear'] = df.index.dayofyear\n    df['dayofmonth'] = df.index.day\n    df['weekofyear'] = df.index.isocalendar().week\n    df['season'] = df['month'] % 12 // 3 + 1\n    return df\n\nseason_names = {\n    1: \"Winter\",\n    2: \"Spring\",\n    3: \"Summer\",\n    4: \"Fall\"\n}\n\ndf = create_features(df)\ndf['season'] = df['season'].map(season_names)\ndf.head()\n</code></pre> <p> </p> Fig. 17 PJME data with time features <p>Mit den neuen Zeitmerkmalen k\u00f6nnen wir nun neue Erkenntnisse aus unseren Daten gewinnen, indem wir eine explorative Datenanalyse durchf\u00fchren. Daf\u00fcr k\u00f6nnen wir die Daten nach verschiedenen Zeitmerkmalen gruppieren und aggregieren.</p> <p> </p> Fig. 18 Energy consumption by hour <p> </p> Fig. 19 Energy consumption by day of week <p> </p> Fig. 19 Energy consumption by month <p> </p> Fig. 20 Energy consumption by year <p> </p> Fig. 20 Energy consumption by season <p> </p> Fig. 20 Energy consumption in 2010 <p>Folgende Erkenntnisse k\u00f6nnen wir aus den Graphiken gewinnen:</p> <ul> <li>Unsere Daten zeigen eine saisonale Komponente.</li> <li>Der t\u00e4gliche H\u00f6chstwert liegt gegen 18 Uhr, w\u00e4hrend der niedrigste Wert um 4 Uhr morgens auftritt.</li> <li>Der geringste Energieverbrauch findet an Wochenenden (Samstag/Sonntag) statt.</li> <li>Der h\u00f6chste Energieverbrauch im Jahr tritt entweder am Jahresende oder in der Mitte des Jahres auf.</li> <li>Es gibt keinen signifikanten Trend oder Ver\u00e4nderung im Gesamtenergieverbrauch im Zeitraum von 2002 bis 2018.</li> <li>Der h\u00f6chste Energieverbrauch tritt im Sommer und dann im Winter auf.</li> </ul> <p>Kommen wir nun zum Modellieren. Wir werden drei Ans\u00e4tze verfolgen. SARIMA, Prophet und LSTM. Zun\u00e4chst brauchen wir jedoch die zus\u00e4tzlichen Zeitmerkmale nicht mehr. Wir k\u00f6nnen sie also entfernen, sodass wir wieder nur zwei Spalten im Datensatz haben. Anschlie\u00dfend resamplen wir die Daten auf t\u00e4gliche Werte. Dies hat den Hintergrund, dass wir sonst zu viele Datenpunkte haben und die Modelle nicht mehr effizient trainiert werden k\u00f6nnen. Vor allem SARIMA und das LSTM ben\u00f6tigen viel Zeit zum Trainieren. Nach dem resamplen teilen wir die Daten noch in Trainings- und Testdaten auf. Wir verwenden 80% der Daten f\u00fcr das Training und 20% f\u00fcr das Testen.</p> <p> </p> Fig. 21 Daily energy consumption <p> </p> Fig. 22 Train/Test split"},{"location":"Themen/Zeitserienanalyse/#31-sarima","title":"3.1 SARIMA","text":"<p>Um ein (S)ARIMA Modell zu implementieren sollte man folgende Schritte durchf\u00fchren:</p> <ol> <li> <p>\u00dcberpr\u00fcfung der Stationarit\u00e4t: Bestimmen Sie, ob die Zeitreihe einen Trend oder eine Saisonalit\u00e4t aufweist. Falls dies der Fall ist, stellen Sie sicher, dass sie vor der Verwendung von ARIMA zur Vorhersage station\u00e4r ist.</p> </li> <li> <p>Differenzierung: Wenn die Zeitreihe nicht station\u00e4r ist, wenden Sie Differenzierung an, um sie station\u00e4r zu machen. Nehmen Sie die erste Differenz und pr\u00fcfen Sie auf Stationarit\u00e4t. Wiederholen Sie dies gegebenenfalls, einschlie\u00dflich saisonaler Differenzierung.</p> </li> <li> <p>Aufteilung der Validierungsstichprobe: Reservieren Sie einen Teil der Daten f\u00fcr die Validierung, um die Genauigkeit des Modells zu bewerten. Verwenden Sie eine Aufteilung der Daten in Trainings- und Testdaten.</p> </li> <li> <p>Auswahl der AR- und MA-Terme: Analysieren Sie die Autokorrelationsfunktion (ACF) und die partielle Autokorrelationsfunktion (PACF), um festzustellen, welche AR-Terme, MA-Terme oder beides im Modell enthalten sein sollten.</p> </li> <li> <p>Modellerstellung: Konstruieren Sie das ARIMA-Modell und legen Sie die Anzahl der Perioden fest, die basierend auf Ihren Anforderungen vorhergesagt werden sollen (N).</p> </li> <li> <p>Validierung des Modells: Vergleichen Sie die vorhergesagten Werte mit den tats\u00e4chlichen Werten in der Validierungsstichprobe.</p> </li> </ol> <p>Die Bibliothek statsmodels bietet uns alle Funktionen, die wir f\u00fcr die Implementierung eines SARIMA-Modells ben\u00f6tigen. Mit <code>seasonal_decompose</code> zerlegen wir die Zeitreihe in ihre Trend-, saisonale und Restkomponenten. </p> <p> </p> Fig. 23 Seasonal decompose <p>Der Datensatz wei\u00dft eine sehr hohe saisonale Komponente auf.</p> <p>Die Funktion <code>adfuller</code> wird verwendet, um die Stationarit\u00e4t der Zeitreihe zu \u00fcberpr\u00fcfen. Ist der p-Wert kleiner als 0.05, so ist die Zeitreihe station\u00e4r.</p> <p> </p> Fig. 23 Adfuller test <p>Da unser Datensatz bereits station\u00e4r ist, m\u00fcssen wir keine weitere Differenzierung durchf\u00fchren. Die Aufteilung in Trainings- und Testdaten ist ebenso bereits erledigt. Um die Werte f\u00fcr p, d und q zu bestimmen, k\u00f6nnen die ACF und PACF Plots verwenden. Es bietet sich jedoch noch eine effektivere M\u00f6glichkeit an. Die Funktion <code>auto_arima</code> der Bibliothek pmdarima. Diese Funktion f\u00fchrt eine Rastersuche durch, um die optimalen Parameter f\u00fcr unser Modell zu finden. Wir geben der Funktion auch an, dass wir eine saisonale Komponente haben.</p> <p><pre><code>from pmdarima.arima import auto_arima\n\nauto_model = auto_arima(train, \n           start_p=0, start_q=0, max_p=10, max_q=10, \n           seasonal=True, m=7,\n           d=None, D=None, trace=True, \n           error_action='ignore', suppress_warnings=True, \n           stepwise=True, seasonal_test='ch')\n\nprint(auto_model.summary())\n</code></pre> Das \"m\" steht f\u00fcr die Anzahl der Perioden pro Saison. In unserem Fall haben wir 7 Tage gew\u00e4hlt. 30 Tage w\u00e4re auch m\u00f6glich gewesen, jedoch ist die Berechnung dann sehr aufwendig.</p> <p>Diese Funktion liefert uns folgende Parameter: SARIMAX(2, 0, 0)x(1, 1, [1], 7) Diese Parameter k\u00f6nnen wir nun in unser Modell einsetzen.</p> <p><pre><code>model = SARIMAX(train, \n                order=(2, 0, 0),\n                seasonal_order=(1, 1, 1, 7),\n                enforce_stationarity=False, \n                enforce_invertibility=False)\n\nresult = model.fit()\nresult.summary()\n</code></pre> Nun k\u00f6nnen wir unser Modell auf die Testdaten anwenden.</p> <pre><code># Convert the datetime index of test data to numeric index\ntest_numeric_index = range(len(test))\n\n# Predict using SARIMAX model\npredictions = result.predict(start=test_numeric_index[0], end=test_numeric_index[-1])\n\n# Assign the converted numeric index to predictions\npredictions.index = test.index\n</code></pre> <p>Anschlie\u00dfend k\u00f6nnen wir die Vorhersage mit den tats\u00e4chlichen Werten vergleichen.</p> <p> </p> Fig. 24 SARIMA prediction <p>Um das Modell zu evaluieren und mit anderen Modellen vergleichen zu k\u00f6nnen haben wir noch den RMSE und MAPE berechnet.</p> <p>SARIMA RMSE:  126971.22</p> <p>SARIMA MAPE: 13.22%</p> <p>Das bedeutet, dass unser SARIMA Modell im Durchschnitt um 13.22% von den tats\u00e4chlichen Werten abweicht.</p>"},{"location":"Themen/Zeitserienanalyse/#32-prophet","title":"3.2 Prophet","text":"<p>Das Prophet-Modell ist ein vorausschauendes Zeitreihenmodell, das von Facebook entwickelt wurde. Es basiert auf einer Additiven Modellierung, die Trends, saisonale Effekte und Feiertage ber\u00fccksichtigt. Prophet verwendet ein Modell, das aus drei Hauptkomponenten besteht.</p> <ol> <li> <p>Trendkomponente: Prophet verwendet einen nichtlinearen Trendansatz, der saisonale Effekte und Ver\u00e4nderungen im Verlauf der Zeit ber\u00fccksichtigt.</p> </li> <li> <p>Saisonale Komponente: Das Modell erfasst saisonale Muster, indem es periodische Effekte in der Zeitreihe identifiziert und modelliert.</p> </li> <li> <p>**Feiertage: Prophet erm\u00f6glicht die Ber\u00fccksichtigung von spezifischen Feiertagen und Ereignissen, die Auswirkungen auf die Zeitreihe haben k\u00f6nnen.</p> </li> </ol> <p>Das Modell verwendet auch zus\u00e4tzliche Anpassungsparameter, um Unsicherheiten in den Daten zu modellieren und robuste Prognosen zu generieren. Prophet ist bekannt f\u00fcr seine Benutzerfreundlichkeit und seine F\u00e4higkeit, mit unvollst\u00e4ndigen oder fehlenden Daten umzugehen. Es bietet auch eine einfache Syntax und unterst\u00fctzt die automatische Erkennung von saisonalen Mustern.</p> <p>Angesichts dessen, was die Implementierung sehr einfach. Wir mussten lediglich unsere Spaltennamen anpassen, sodass Prophet diese versteht. Die erste Spalte muss den Namen \"ds\" haben und die zweite Spalte den Namen \"y\". Die Spalte \"ds\" enth\u00e4lt die Zeitstempel und die Spalte \"y\" enth\u00e4lt die Werte der Zeitreihe. Daraufhin kann man das Model auch schon trainieren.</p> <pre><code>from prophet import Prophet\n\n# Format data for prophet model using ds and y\npjme_train_prophet = train.reset_index() \\\n    .rename(columns={'Datetime':'ds',\n                     'PJME_MW':'y'})\nmodel = Prophet()\nmodel.fit(pjme_train_prophet)\n\n# Predict on test set with model\npjme_test_prophet = test.reset_index() \\\n    .rename(columns={'Datetime':'ds',\n                     'PJME_MW':'y'})\n\npjme_test_fcst = model.predict(pjme_test_prophet)\n</code></pre> <p>Wenn man sich den Kopf der Daten anschaut, hat Prophet viele neue Spalten hinzugef\u00fcgt, dessen Werte man jedoch schwer interpretieren kann. Plottet man nun die Vorhersagen, erhalten wir folgende Graphik:</p> <p> </p> Fig. 25 Prophet predictions <p>Die Vorhersagen sehen sehr gut aus. Prophet hat die Trends und Saisonalit\u00e4ten sehr gut erkannt.</p> <p> </p> Fig. 26 Prophet predictions vs actuals <p>Die Funktion <code>plot_components</code> zeigt die einzelnen Komponenten des Modells an. Die Komponenten sind der Trend, die saisonalen Effekte und die Feiertage (\u00e4hnlich wie beim SARIMA decompose).</p> <p>Der RMSE und MAPE f\u00fcr das Prophet Modell sind: 78333.89 und 7.48%. Dies ist eine deutliche Verbesserung gegen\u00fcber dem SARIMA Modell.</p> <p>Dank Prophet k\u00f6nnen wir auch einfach die Ferientage in unserem Modell ber\u00fccksichtigen. Dazu m\u00fcssen wir lediglich die Ferientage in ein Dataframe laden und Prophet mitteilen, dass es diese ber\u00fccksichtigen soll.</p> <pre><code>from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n\ncal = calendar()\ntrain_holidays = cal.holidays(start=train.index.min(),\n                              end=train.index.max())\ntest_holidays = cal.holidays(start=test.index.min(),\n                             end=test.index.max())\n\n# Create a dataframe with holiday, ds columns\ndf['date'] = df.index.date\ndf['is_holiday'] = df.date.isin([d.date() for d in cal.holidays()])\nholiday_df = df.loc[df['is_holiday']] \\\n    .reset_index() \\\n    .rename(columns={'Datetime':'ds'})\nholiday_df['holiday'] = 'USFederalHoliday'\nholiday_df = holiday_df.drop(['PJME_MW','date','is_holiday'], axis=1)\n\nholiday_df['ds'] = pd.to_datetime(holiday_df['ds'])\n\n# Setup and train model with holidays\nmodel_with_holidays = Prophet(holidays=holiday_df)\nmodel_with_holidays.fit(train.reset_index() \\\n                            .rename(columns={'Datetime':'ds',\n                                             'PJME_MW':'y'}))\n# Predict on training set with model\npjme_test_fcst_with_hols = \\\n    model_with_holidays.predict(df=test.reset_index() \\\n                                    .rename(columns={'Datetime':'ds'}))\n</code></pre> <p>Berechnen wir nun erneut den RMSE, stellen wir fest, dass sich dieser nicht verbessert hat.</p> <p>RMSE mit Ferientagen: 78439.56</p> <p>RMSE ohne Ferientage: 78333.89</p> <p>Er hat sich sogar etwas verschlechtert. Dies liegt daran, dass die Ferientage in unserem Datensatz nicht sehr aussagekr\u00e4ftig sind, weil unser Datensatz zu gro\u00df ist. Es sind zu viele Datenpunkte vorhanden, weswegen die Ferientage eher als Rauschen betrachtet werden.</p> <p>Prophet bietet auch noch eine einfach Funktion <code>make_future_dataframe</code>, um einen zuk\u00fcnftigen Datenrahmen zu erstellen und Vorhersagen zu treffen. Man gibt im Parameter \"periods\" an, wie gro\u00df der Datenrahmen sein soll. Als Beispiel haben wir 5 Jahre genommen (365 * 24 * 5).</p> <pre><code>future = model.make_future_dataframe(periods=365*24*5, freq='h', include_history=False)\nforecast = model_with_holidays.predict(future)\n</code></pre> <p> </p> Fig. 27 Prophet prediction on future dataframe"},{"location":"Themen/Zeitserienanalyse/#33-lstm","title":"3.3 LSTM","text":"<p>Bevor wir mit dem LSTM Modell starten, m\u00fcssen wir unsere Daten nochmals etwas vorbereiten. Wir m\u00fcssen die Daten normalisieren, damit das Modell besser trainiert werden kann. Dazu verwenden wir die MinMaxScaler Funktion von sklearn.</p> <pre><code>from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom kerastuner.tuners import RandomSearch\nfrom keras.callbacks import EarlyStopping\n\n# Data preprocessing\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled_train = scaler.fit_transform(train)  # Scale the training data between 0 and 1\n\n# Create the training data\nX_train = []\ny_train = []\nfor i in range(60, len(train)):\n    X_train.append(scaled_train[i-60:i, 0])  # Create sequences of 60 previous values as input (lookback period)\n    y_train.append(scaled_train[i, 0])  # Current value as output\nX_train, y_train = np.array(X_train), np.array(y_train)\n\n# Reshape the data\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n# Reshape the input data to be 3-dimensional (samples, timesteps, features) for LSTM model\n</code></pre> <p>Anschlie\u00dfend k\u00f6nnen wir das Modell erstellen. Wir verwenden hierf\u00fcr die Keras Tuner Library, um die besten Hyperparameter zu finden. Dazu m\u00fcssen wir eine Funktion erstellen, die das Modell erstellt. Diese Funktion wird dann vom Keras Tuner aufgerufen und die Hyperparameter werden \u00fcbergeben. Wir verwendet zwei LSTM Layer mit jeweils einem Dropout Layer. Die Anzahl der Neuronen und die Dropout Rate werden vom Keras Tuner optimiert. Am Ende wird noch ein Dense Layer mit einem Neuron verwendet als Output Layer.</p> <pre><code>def build_model(hp):\n    model = keras.Sequential()\n    model.add(layers.LSTM(units=hp.Int('units', min_value=32, max_value=128, step=32), \n                          return_sequences=True, \n                          input_shape=(X_train.shape[1], 1)))\n    model.add(layers.Dropout(rate=hp.Float('dropout', min_value=0.1, max_value=0.5, step=0.1)))\n    model.add(layers.LSTM(units=hp.Int('units', min_value=32, max_value=128, step=32), \n                          return_sequences=False))\n    model.add(layers.Dropout(rate=hp.Float('dropout', min_value=0.1, max_value=0.5, step=0.1)))\n    model.add(layers.Dense(units=1))\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    return model\n\n# Initialize Keras Tuner\ntuner = RandomSearch(\n    build_model,\n    objective='val_loss',\n    max_trials=5,  # how many model configurations would you like to test?\n    executions_per_trial=3,  # how many trials per variation? (same model could perform differently)\n    directory='project',\n    project_name='Energy Consumption LSTM')\n\n# Summary of the search space\ntuner.search_space_summary()\n\n# Perform hyperparameter search\ntuner.search(X_train, y_train, epochs=5, validation_split=0.2)\n\n# Summary of the results\ntuner.results_summary()\n</code></pre> <p>Als Ausgabe erhalten wir die besten Modelle mit den jeweiligen Hyperparametern. Nun nehmen wir ein Modell und trainieren es mit \"Early Stopping\". Early Stopping stoppt das Training, wenn der Validierungsfehler nicht mehr sinkt. Dadurch wird Overfitting verhindert. Wir nutzen 50 Epochen zum Trainieren. Anschlie\u00dfend plotten wir den Trainings- und Validierungsfehler. Anhand des Plots k\u00f6nnen wir erkennen, ob das Model overfittet ist oder nicht und ggf. ein anderes Modell ausw\u00e4hlen.</p> <pre><code>from keras.callbacks import EarlyStopping\n\n# Choose the best model\nbest_model = tuner.get_best_models(num_models=5)[3]\n\n# Define early stopping\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\n# Fit the model\nhistory = best_model.fit(X_train, y_train, epochs = 50, validation_split=0.2, callbacks=[early_stop])\n</code></pre> <p> </p> Fig. 28 Training and Validation Loss <p>Wir k\u00f6nnen erkennen, dass unser Modell nicht overfittet ist. Der Validierungsfehler und Trainingsfehler nehmen kontinuierlich ab und konvergieren schlie\u00dfen. Sie \u00fcberschneiden sich mehrmals und halten dasselbe Niveau bis zum Ende der 50 Epochen. Das bedeutet, dass das Modell unsere Test Daten genauso gut vorhersagen kann wie die Trainingsdaten.</p> <p>Um nun Vorhersagen treffen zu k\u00f6nnen, m\u00fcssen wir unsere Testdaten genauso vorbereiten wie die Trainingsdaten.</p> <pre><code># Prepare the test data similarly to the training data\n\n# Get the inputs for the test data\ninputs = univariate_df[len(univariate_df) - len(test) - 60:].values\ninputs = inputs.reshape(-1, 1)  # Reshape the input data to have a single feature column\n\ninputs = scaler.transform(inputs)  # Scale the test data using the same scaler used for training\n\nX_test = []\nfor i in range(60, inputs.shape[0]):\n    X_test.append(inputs[i-60:i, 0])  # Create sequences of 60 previous values as input for the test data (lookback period)\nX_test = np.array(X_test)\n\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n# Reshape the test data to be 3-dimensional (samples, timesteps, features) for LSTM model\n</code></pre> <p>Anschlie\u00dfend k\u00f6nnen wir die Vorhersagen treffen und die Ergebnisse plotten.</p> <pre><code># Make predictions with the best model\npredicted_energy_consumption = best_model.predict(X_test)\n\n# Inverse transform to get real values\npredicted_energy_consumption = scaler.inverse_transform(predicted_energy_consumption)\n</code></pre> <p> </p> Fig. 29 LSTM predictions <p>Das LSTM trifft sehr pr\u00e4zise vorhersagen. Wir erhalten folgende Werte f\u00fcr den RMSE und den MAPE:</p> <p>LSTM RMSE: 48014.44</p> <p>LSTM MAPE 4.71%</p> <p>Unser LSTM hast das Prophet Modell um 3% Abweichung geschlagen. Das ist ein sehr gutes Ergebnis. Das LSTM schlie\u00dft von allen Modellen am besten ab.</p> <p>Hier nochmal der Vergleich der drei Modelle:</p> <p> </p> Fig. 30 Vergleich SARIMA, Prophet and LSTM"},{"location":"Themen/Zeitserienanalyse/#4-fazit","title":"4 Fazit","text":"<p>Zeitserien haben drei wichtige Merkmale: Autokorrelation, Trend und Saison. Autokorrelation beschreibt den Zusammenhang zwischen den Werten einer Zeitreihe und ihren verz\u00f6gerten Versionen. Der Trend bezieht sich auf die langfristige Ver\u00e4nderung des Mittelwerts der Zeitreihe. Die Saison bezieht sich auf wiederkehrende Muster in den Daten, die mit bestimmten Zeitr\u00e4umen zusammenh\u00e4ngen.</p> <p>Die Zeitreihenanalyse wird in verschiedenen Anwendungsgebieten eingesetzt, darunter Merkmalsanalyse, Vorhersage, Gl\u00e4ttung und Anomalieerkennung. Bei der Analyse der Merkmale werden Muster und Trends in der Zeitreihe identifiziert. Die Vorhersage befasst sich mit der Sch\u00e4tzung zuk\u00fcnftiger Werte basierend auf vergangenen Daten. Die Gl\u00e4ttung reduziert Rauschen und Schwankungen, um den Trend deutlicher zu erkennen. Die Anomalieerkennung identifiziert abnormale Muster oder Ausrei\u00dfer in der Zeitreihe.</p> <p>Klassische Ans\u00e4tze in der Zeitreihenanalyse umfassen das exponentielle Gl\u00e4tten und das SARIMA-Modell. Das exponentielle Gl\u00e4tten reduziert kurzfristige Schwankungen und erfasst den Trend, aber ber\u00fccksichtigt keine Saisonkomponente. Das SARIMA-Modell modelliert Zeitreihen mit saisonalen Mustern und ber\u00fccksichtigt Autoregression, Differenzierung und Moving Average mit saisonalen Komponenten.</p> <p>SARIMA-Modelle zeichnen sich durch ihre einfache Interpretierbarkeit und die geringe Anzahl an Tuning-Parametern im Vergleich zu maschinellen Lernmodellen aus. Allerdings erfordern sie station\u00e4re Daten und haben Schwierigkeiten bei der Bew\u00e4ltigung mehrerer saisonaler Muster. Zudem sind sie rechenintensiv f\u00fcr gro\u00dfe Datens\u00e4tze.</p> <p>Moderne Ans\u00e4tze in der Zeitreihenanalyse haben in den letzten Jahren an Bedeutung gewonnen. Sie umfassen maschinelle Lernverfahren (XGBoost, Prophet) und Deep Learning (LSTM). Diese Ans\u00e4tze k\u00f6nnen komplexe Muster erfassen und pr\u00e4zisere Vorhersagen liefern.</p> <p>Prophet eignet sich gut f\u00fcr Zeitreihen mit starken saisonalen Effekten und mehreren Saisons an historischen Daten. Es kann mehrere Saisonalit\u00e4ten gut handhaben und erm\u00f6glicht die flexible Einbeziehung von Feiertagseffekten und zus\u00e4tzlichen Regressoren. Die Anwendung von Prophet erfordert weniger Verst\u00e4ndnis \u00fcber die zugrunde liegenden Implementierungen. Allerdings sind die Komponenten der Vorhersage nicht so leicht interpretierbar wie bei ARIMA und das Modell ist weniger effektiv f\u00fcr hochfrequente Daten.</p> <p>Das LSTM ist eine Art von rekurrentem neuronalen Netzwerk, das komplexe nichtlineare Beziehungen modellieren kann. Es eignet sich gut f\u00fcr Zeitreihenprognosen, bei denen langfristige Abh\u00e4ngigkeiten eine Rolle spielen. Es kann auch mehrere saisonale Muster wie Prophet handhaben. Allerdings ist das Training von LSTM-Modellen langsam, insbesondere f\u00fcr gro\u00dfe Datens\u00e4tze, und es besteht die Gefahr der \u00dcberanpassung ohne sorgf\u00e4ltige Gestaltung und Regularisierung. Die Vorhersagen von LSTM-Modellen sind schwer interpretierbar und erfordern gro\u00dfe Datenmengen f\u00fcr das Training.</p> <p>Bei der Auswahl des geeigneten Modells f\u00fcr die Zeitreihenprognose ist es wichtig, die Merkmale der Daten und die Priorit\u00e4ten der Aufgabenstellung zu ber\u00fccksichtigen. ARIMA eignet sich gut f\u00fcr interpretierbare Vorhersagen, w\u00e4hrend LSTMs bei komplexen Mustern und gro\u00dfen Datens\u00e4tzen \u00fcberlegen sein k\u00f6nnen. Prophet bietet eine flexible und benutzerfreundliche L\u00f6sung f\u00fcr die Bew\u00e4ltigung mehrerer Saisonalit\u00e4ten. Letztendlich gibt es kein universell bestes Modell f\u00fcr alle Arten von Zeitreihendaten. Die Wahl h\u00e4ngt von den spezifischen Anforderungen und Eigenschaften der Daten ab.</p>"},{"location":"Themen/Zeitserienanalyse/#5-weiterfuhrendes-material","title":"5 Weiterf\u00fchrendes Material","text":""},{"location":"Themen/Zeitserienanalyse/#51-podcast","title":"5.1 Podcast","text":"<p>Der Campus Talk \u2013 Silicon Forest \u2013 Folge 4</p>"},{"location":"Themen/Zeitserienanalyse/#52-talk","title":"5.2 Talk","text":"<p>Hier einfach Youtube oder THD System embedden.</p>"},{"location":"Themen/Zeitserienanalyse/#53-demo","title":"5.3 Demo","text":"<p>Link zur Code Demonstration: </p> <p>Link zum Repository: https://github.com/Julian-Ivanov/Energy-Consumption-TSA.git</p>"},{"location":"Themen/Zeitserienanalyse/#6-literaturliste","title":"6 Literaturliste","text":"<p>Fathi M. Salem. (2022). Recurrent Neural Networks From Simple to Gated Architectures. Springer.</p> <p>Huang, C (2022). Applied Time Series Analysis and Forecasting with Python. Springer.</p> <p>National Institute of Standards and Technologies. Introduction to Time Series Analysis.</p>"},{"location":"Themen/bayesian_modeling/","title":"Bayesian Modeling","text":"<p>von Serife-Nur \u00d6zdemir, Sanamjeet Meyer und Anna Postnikova</p> <p>Die bayesianische Modellierung basiert auf der bayesianischen Statistik. Im Vergleich zum frequentistischen Ansatz spiegelt der bayesianische Ansatz eher die menschliche Denkweise wider. Der bayesianische Ansatz ber\u00fccksichtigt das Vorwissen bei der Inferenz, was besonders hilfreich ist, wenn bereits Expertenwissen oder Vorkenntnisse in einem Bereich vorhanden sind. Nahezu alle klassischen Modelle des maschinellen Lernens k\u00f6nnen in bayesianische Modelle umgewandelt werden. In dieser Arbeit gehen wir darauf ein, was bayesianische Modellierung ist, wie sie funktioniert und welche Vor- und Nachteile sie hat.</p>"},{"location":"Themen/bayesian_modeling/#einfuhrung-in-die-bayes-statistik","title":"Einf\u00fchrung in die Bayes Statistik","text":""},{"location":"Themen/bayesian_modeling/#bayes-theorem","title":"Bayes-Theorem","text":"<p>Das Bayes-Theorem ist ein grundlegendes Konzept der Wahrscheinlichkeitstheorie, das es uns erm\u00f6glicht, unsere \u00dcberzeugungen \u00fcber ein Ereignis basierend auf neuen Beweisen zu aktualisieren. Es kann wie folgt formuliert werden:</p> <ul> <li>P(A|B) repr\u00e4sentiert die Wahrscheinlichkeit, dass Ereignis A eintritt, unter der Bedingung, dass Ereignis B eingetreten ist.</li> <li>P(B|A) ist die Wahrscheinlichkeit, dass Ereignis B eintritt, unter der Bedingung, dass Ereignis A eingetreten ist.</li> <li>P(A) ist die Wahrscheinlichkeit, dass Ereignis A eintritt.</li> <li>P(B) bezeichnet die Wahrscheinlichkeit, dass Ereignis B eintritt.</li> </ul> \\[ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} \\] \\[ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} \\] <p>Das Bayes-Theorem bietet eine M\u00f6glichkeit, die Wahrscheinlichkeit von Ereignis A unter Ber\u00fccksichtigung vorhandener Kenntnisse und neuer Beweise zu berechnen. Es wird in verschiedenen Bereichen eingesetzt, darunter Statistik, maschinelles Lernen und Datenanalyse.</p>"},{"location":"Themen/bayesian_modeling/#anwendungsbeispiel-medizinischer-test","title":"Anwendungsbeispiel Medizinischer Test","text":"<ul> <li>In 99,5% der F\u00e4lle f\u00e4llt der Test positiv aus.</li> <li>Sollte die Krankheit nicht vorliegen, betr\u00e4gt die Wahrscheinlichkeit f\u00fcr einen positiven Test 1%.</li> <li>Laut einer Studie leidet eine von vier Personen an der betreffenden Krankheit.</li> <li>Wie gro\u00df ist die Wahrscheinlichkeit, dass jemand an der Krankheit leidet, obwohl der Test ein negatives Ergebnis zeigt?</li> </ul>"},{"location":"Themen/bayesian_modeling/#modell-annahmen","title":"Modell-Annahmen","text":"<p>K = Person ist krank T = Test f\u00e4llt positiv aus</p> \\[P(T|K) = 0.995\\] \\[P(T|\\overline{K}) = 0.01\\] \\[P(K) = 0.25\\]"},{"location":"Themen/bayesian_modeling/#gesucht-wird-die-wahrscheinlichkeit","title":"Gesucht wird die Wahrscheinlichkeit","text":"\\[P(K|\\overline{T})\\]"},{"location":"Themen/bayesian_modeling/#anwendungsbeispiel-bayes-theorem","title":"Anwendungsbeispiel Bayes Theorem","text":""},{"location":"Themen/bayesian_modeling/#bayes-theorem_1","title":"Bayes Theorem","text":"\\[P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\] \\[P(K|\\overline{T}) = \\frac{P(\\overline{T}|K)P(K)}{P(\\overline{T}|K)P(K) + P(\\overline{T}|\\overline{K})P(\\overline{K})}\\] \\[ = \\frac{(1-0.995)\\cdot 0.25}{(1-0.995)\\cdot 0.25 + (1-0.01)\\cdot 0.75}\\] \\[ = 0.00185 \\]"},{"location":"Themen/bayesian_modeling/#diagnostik-seltener-ereignisse","title":"Diagnostik seltener Ereignisse","text":"<p>Vorsorgliche Untersuchung auf einer Krankheit</p> <p>Bekannte Informationne:</p> <ul> <li>\\(0,1\\%\\) der m\u00e4nnlichen Bev\u00f6lkerung ist mit der Krankheit infiziert</li> <li>\\(95\\%\\) Wahrscheinlichkeit f\u00fcr korrekte Diagnose bei erkrankter Person</li> <li>\\(98\\%\\) Wahrscheinlichkeit f\u00fcr negatives Ergebnis bei gesunder Person</li> </ul> <p>Fragestellung:</p> <ul> <li>Wie gro\u00df ist die Wahrscheinlichkeit, dass der Mann tats\u00e4chlich krank ist?</li> <li>Intuitive Antwort von \\(95\\%\\) ist falsch, da Ursache und Ereignis vertauscht werden</li> </ul> <p>Modell-Annahmen:</p> <p>Gegebene Informationen:</p> <ul> <li> <p>Hypothesen: \\(K\\) = \"Der Mann ist krank\", \\(\\overline{K}\\) = \"Der Mann ist gesund\"</p> </li> <li> <p>A-priori-Wahrscheinlichkeiten: \\(P(K) = 0,001\\), \\(P(\\overline{K}) = 0,999\\)</p> </li> <li> <p>Likelihoods: \\(P(P|K) = 0,95\\), \\(P(P|\\overline{K}) = 0,02\\)</p> </li> </ul> <p>Gesucht: </p> <ul> <li>A-posteriori-Wahrscheinlichkeit \\(P(K|P)\\)</li> </ul>"},{"location":"Themen/bayesian_modeling/#anwendung-bayes-theorem","title":"Anwendung Bayes Theorem:","text":"<p>\\[P(K|P) = \\frac{{P(K) \\cdot P(P|K)}}{{P(K) \\cdot P(P|K) + P(\\overline{K}) \\cdot P(P|\\overline{K})}}\\] \\[P(K|P) = \\frac{{0.001 \\cdot 0.95}}{{0.001 \\cdot 0.95 + 0.999 \\cdot 0.02}}\\] \\[P(K|P) \\approx 0.05 \\]</p> <p>Hier ist ein konkretes Beispiel, das verdeutlicht, wie stark sich die berechnete Wahrscheinlichkeit von unserer intuitiven Annahme unterscheiden kann. In diesem Beispiel verwenden wir relative H\u00e4ufigkeiten anstelle von Wahrscheinlichkeiten, um das \u00fcberraschend kleine Ergebnis verst\u00e4ndlicher zu machen. In der Regel arbeiten wir jedoch mit Wahrscheinlichkeiten, was im folgenden Abschnitt genauer erl\u00e4utert wird.</p>"},{"location":"Themen/bayesian_modeling/#rechnen-mit-wahrscheinlichkeitsverteilungen","title":"Rechnen mit Wahrscheinlichkeitsverteilungen","text":""},{"location":"Themen/bayesian_modeling/#alternative-form-der-darstellung","title":"Alternative Form der Darstellung:","text":"\\[ P(\\theta|data) = \\frac{P(data|\\theta) \\cdot P(\\theta)}{P(data)} \\] \\[ Posteriori = \\frac{Likelihood \\cdot Priori}{Normalisierungskonstante} \\]"},{"location":"Themen/bayesian_modeling/#fairer-munzwurf","title":"Fairer M\u00fcnzwurf","text":"<p>Am besten lassen sich die Begriffe Likelihood, Priori und Posteriori am Beispiel eines fairen M\u00fcnzwurfs erkl\u00e4ren. Wir m\u00f6chten herausfinden, ob eine M\u00fcnze \"fair\" ist, d.h., ob Kopf oder Zahl beim M\u00fcnzwurf bevorzugt wird. In diesem Beispiel konzentrieren wir uns auf die Kopfseite. Unser Vorwissen (Priori) besagt, dass die Wahrscheinlichkeit f\u00fcr Kopf 50% betr\u00e4gt. Als Priori-Verteilung w\u00e4hlen wir die Beta-Verteilung. Um zu verstehen, warum gerade die Beta-Verteilung gew\u00e4hlt wurde und keine andere, betrachten wir das Konzept der Konjugiertheit.</p>"},{"location":"Themen/bayesian_modeling/#konjugiertheit","title":"Konjugiertheit","text":"<p>Eine konjugierte Priori-Verteilung geh\u00f6rt zur gleichen Verteilungsfamilie wie die Posteriori-Verteilung. Wenn Priori-Verteilung und Likelihood-Verteilung konjugiert sind, kann die Posteriori-Verteilung analytisch bestimmt werden.</p> <p>Hier sind einige Beispiele f\u00fcr konjugierte Priori-Verteilungen und die zugeh\u00f6rige Likelihood-Verteilung:</p> Likelihood-Verteilung Konjugierte Priorverteilung Binomial Beta Normal (bekannte Varianz) Normal Normal (unbekannte Varianz) Normal-Gamma Exponential Gamma Poisson Gamma <p>Wenn wir wieder auf das M\u00fcnzwurf-Beispiel zur\u00fcckkommen, k\u00f6nnen wir die Binomial-Verteilung als Likelihood-Verteilung verwenden. Die Binomial-Verteilung eignet sich besonders gut, da unsere Daten nur aus Nullen (Zahl) und Einsen (Kopf) bestehen. </p> <p>Folgende informationen liegen uns vor:</p>"},{"location":"Themen/bayesian_modeling/#priori","title":"Priori","text":"<p>\\[P(\\theta) = \\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta - 1}}{B(\\alpha, \\beta)}\\]</p> <ul> <li>\\(B(\\alpha, \\beta)\\) ist die Betafunktion (Normalisierungkonstante)</li> <li>\\(B(\\alpha, \\beta) = \\int_{0}^{1} \\theta^{\\alpha-1} \\cdot (1-\\theta)^{\\beta-1} dt\\)</li> </ul>"},{"location":"Themen/bayesian_modeling/#likelihood","title":"Likelihood","text":"<p>\\[ P(y \\mid \\theta) \\propto \\underbrace{\\theta^y (1-\\theta)^{n-y}}_{\\text Binomialverteilung} \\]</p>"},{"location":"Themen/bayesian_modeling/#normalisierungskonstante","title":"Normalisierungskonstante","text":"<p>Die Normalisierungskonstante in diesem Beispiel, \\(B(\\alpha, \\beta)\\), stellt sicher, dass es sich um eine valide Wahrscheinlichkeitsverteilung handelt und hat keinen direkten Einfluss auf \\(\\theta\\). In diesem Fall k\u00f6nnen wir das \\(\\propto\\)-Zeichen (sprich: \"proportional zu\") verwenden. Es bedeutet, dass die linke und rechte Seite bis auf eine Konstante \u00fcbereinstimmen.</p> \\[ P(\\theta) \\propto \\theta^{\\alpha-1}(1-\\theta)^{\\beta - 1} \\]"},{"location":"Themen/bayesian_modeling/#posteriori","title":"Posteriori","text":"<p>\\[P(\\theta \\mid y) \\propto \\theta^{\\textcolor{blue}{y}} (1-\\theta)^{\\textcolor{black}{\\textcolor{red}{n-y}}} \\cdot \\theta^{\\textcolor{blue}{\\alpha-1}}(1-\\theta)^{\\textcolor{red}{\\beta - 1}}\\]</p> <p>\\[P(\\theta \\mid y) \\propto \\theta^{\\textcolor{blue}{y+\\alpha-1}} (1-\\theta)^{\\textcolor{red}{n-y+\\beta-1}}\\]</p> <p>\\[P(\\theta \\mid y) \\propto \\theta^{\\textcolor{blue}{\\alpha_n}-1} (1-\\theta)^{\\textcolor{red}{\\beta_n}-1}\\]</p> <p>\\[P(\\theta \\mid y) = \\frac{\\theta^{\\textcolor{blue}{\\alpha_n}-1} (1-\\theta)^{\\textcolor{red}{\\beta_n}-1}}{B(\\textcolor{blue}{\\alpha_n},\\textcolor{red}{\\beta_n})}\\]</p> <p>*\\(\\textcolor{blue}{\\alpha_n} = y + \\alpha , \\textcolor{red}{\\beta_n} = n-y+\\beta\\)</p> <p>Wir erkennen, dass die Posteriori-Verteilung eine Beta-Verteilung ist. Dies ist ein Beweis daf\u00fcr, dass die Beta-Verteilung konjugiert zur Binomialverteilung ist.</p>"},{"location":"Themen/bayesian_modeling/#visualizierung","title":"Visualizierung","text":"<p>In dieser Abbildung ist die Priori-Verteilung rot, die Likelihood-Verteilung gr\u00fcn und die Posteriori-Verteilung blau dargestellt. In diesem Beispiel entspricht die Likelihood bzw. Binomialverteilung \\(Binomial(10,7)\\). Das bedeutet, bei \\(y=10\\) W\u00fcrfen ergab sich \\(n=7\\) Mal \"Kopf\". Die Beta-Verteilung ist als \\(Beta(30,30)\\) mit \\(\\alpha=30\\) und \\(\\beta=30\\) definiert und \u00e4hnelt einer Normalverteilung. Dies dr\u00fcckt aus, dass die Wahrscheinlichkeit f\u00fcr \\(\\theta\\) bei 0,5 bzw. 50% am h\u00f6chsten ist. Die Posteriori-Verteilung ist leicht nach rechts verschoben, da das Ergebnis h\u00e4ufiger \"Kopf\" als \"Zahl\" war. Daraus l\u00e4sst sich ableiten, dass die Wahrscheinlichkeit, beim n\u00e4chsten Wurf \"Kopf\" zu erhalten, minimal gestiegen ist.</p>"},{"location":"Themen/bayesian_modeling/#interpretation-der-posterioriverteilung","title":"Interpretation der Posterioriverteilung","text":"<p>Die Posteriori-Verteilung erm\u00f6glicht Sch\u00e4tzungen wie den Mittelwert oder Durchschnitt sowie die Bildung eines Glaubw\u00fcrdigkeitsintervalls f\u00fcr den Wert \\(\\theta\\). Die Entscheidung f\u00fcr eine dieser M\u00f6glichkeiten h\u00e4ngt vom Ziel und der vorliegenden Posteriori-Verteilung ab. In der obigen Abbildung sehen wir die Posteriori-Verteilung f\u00fcr unser M\u00fcnzwurf-Beispiel. Die Linien f\u00fcr den Mittelwert und den Durchschnitt \u00fcberlappen sich, und das 95%-Glaubw\u00fcrdigkeitsintervall deckt nur einen relativ schmalen Bereich ab. Wenn wir uns also f\u00fcr eine Punkt- oder Sch\u00e4tzungsgr\u00f6\u00dfe wie den Mittelwert oder den Durchschnitt entscheiden, haben wir eine hohe Sicherheit, dass dieser Wert korrekt ist. Im folgenden Beispiel sehen wir jedoch, dass eine Posteriori-Verteilung auch anders aussehen kann.</p> <p></p> <p>In diesem Beispiel sehen wir, dass der Mittelwert und der Durchschnitt nicht \u00fcbereinstimmen, sondern etwas voneinander entfernt sind. Wenn man sich f\u00fcr einen dieser Werte entscheidet, muss man jedoch ber\u00fccksichtigen, dass dieser Wert nicht unbedingt dem tats\u00e4chlichen Wert f\u00fcr \\(\\theta\\) entspricht, da das \\(95%\\) HDI einen gro\u00dfen Bereich der Verteilung abdeckt. Die Quantifizierung von Unsicherheiten ist einer der gro\u00dfen Vorteile des Bayesianischen Modellierens.</p>"},{"location":"Themen/bayesian_modeling/#mcmc","title":"MCMC","text":"<p>In dem obigen Beispiel war es m\u00f6glich, die Posteriori-Verteilung analytisch zu bestimmen, da die Beta-Verteilung (Priori) konjugiert zur Binomialverteilung (Likelihood) ist. In der Praxis ist dies jedoch selten der Fall, da man mit verschiedenen Verteilungen arbeiten muss. In solchen F\u00e4llen muss die Posteriori-Verteilung numerisch oder approximativ bestimmt werden. Eine der bekanntesten Methoden daf\u00fcr ist MCMC (Monte Carlo Markov Chain). MCMC ist eine Gruppe von verschiedenen Algorithmen, wobei eines der einfachsten Methoden Metropolis-Hastings ist. Schauen wir uns anhand des M\u00fcnzwurf-Beispiels an, wie Metropolis-Hastings funktioniert.</p>"},{"location":"Themen/bayesian_modeling/#monte-carlo","title":"Monte Carlo","text":"<p>Die Monte Carlo Methode erzeugt Pseudozufallszahlen (im Folgenden als Zufallszahlen bezeichnet).</p> <p></p> <p>In der Abbildung sehen wir die Proposal Distribution, den Trace-Plot und das Dichte-Diagramm. Die Proposal Distribution ist eine Normalverteilung mit dem Median \\(\\mu = 0,5\\) und einer beliebigen Varianz \\(\\sigma\\). Aus der Proposal Distribution werden Zufallszahlen f\u00fcr \\(\\theta\\) generiert. Im Trace-Plot sieht man die Zufallszahlen in der Reihenfolge, in der sie erzeugt wurden. Das Dichte-Diagramm ist ein Histogramm der Stichprobe, das um 90 Grad gedreht ist. Der gr\u00fcne Punkt im Dichte-Diagramm zeigt den aktuellen Wert von \\(\\theta\\) an.</p> <p>Wenn wir 10.000 zuf\u00e4llige Werte f\u00fcr \\(\\theta\\) ziehen, sieht der Prozess so aus.</p> <p></p> <p>Diese Animation veranschaulicht mehrere wichtige Merkmale. Die Proposal Distribution \u00e4ndert sich von einer Iteration zur n\u00e4chsten nicht. Die Dichtekurve \u00e4hnelt mit zunehmendem Stichprobenumfang immer mehr der Proposal Distribution. Das Muster der Variation sieht in allen Iterationen gleich aus, was bedeutet, dass die Kurvendarstellung station\u00e4r ist.</p> <p>In diesem Fall hat die Monte-Carlo-Simulation eine Stichprobe generiert, die der Proposal distribution \u00e4hnelt.</p>"},{"location":"Themen/bayesian_modeling/#markov-chain","title":"Markov Chain","text":"<p>Die Markov Chain ist eine Folge von Zahlen, bei der jede Zahl von der vorherigen Zahl in der Folge abh\u00e4ngt. Ein Beispiel daf\u00fcr ist das Ziehen der Werte von \\(\\theta\\) aus einer Normalverteilung, deren Median dem vorherigen Wert von \\(\\mu = \\theta_{t-1}\\) entspricht.</p> <p>Die Abbildung unten zeigt einen Trace-Plot und ein Density-Diagramm, bei dem der aktuelle Wert von \\(\\theta_t = 0,530\\) aus der Proposal Distribution mit einem Mittelwert, der dem vorherigen Wert von \\(\\theta_{t-1} = 0,712\\) entspricht.</p> <p></p> <p>Schauen wir uns die n\u00e4chste Iteration genauer an. Der Wert von \\(\\theta_t\\) betr\u00e4gt nun 0,4111, und der Mittelwert der Normalverteilung \\(\\theta_{t-1}\\) ist \\(0,530\\).</p> <p></p> <p>Wenn wir erneut 10.000 zuf\u00e4llige Werte f\u00fcr \\(\\theta\\) ziehen, sieht der Prozess wie folgt aus.</p> <p></p> <p>Diese Animation zeigt die Unterschiede zwischen Monte Carlo und Markov Chain. Im Vergleich zur Monte Carlo Methode \u00e4ndert sich die Proposal Distribution mit jeder Iteration. Dadurch entsteht eine Kurve mit einem \"Random-Walk\"-Muster. Die resultierende Dichtekurve \u00e4hnelt weder der Proposal Distribution noch einer anderen n\u00fctzlichen Verteilung. </p> <p>Der Grund daf\u00fcr, dass MCMC allein keine Stichprobe aus der Posterior-Verteilung erzeugen konnte, liegt darin, dass wir keine Informationen \u00fcber die Posterior-Verteilung in den Prozess der Stichprobenbildung einbezogen haben. Wir k\u00f6nnten unsere Stichprobe verbessern, indem wir die vorgeschlagenen Werte von \\(\\theta\\), die gem\u00e4\u00df der Posterior-Verteilung wahrscheinlicher sind, beibehalten und die weniger wahrscheinlichen Werte verwerfen.</p> <p>Die offensichtliche Schwierigkeit bei der Annahme und Ablehnung vorgeschlagener Werte von \\(\\theta\\) basierend auf der Posterior-Verteilung besteht jedoch darin, dass wir die funktionale Form der Posterior-Verteilung in der Regel nicht kennen. Wenn wir die funktionale Form kennen w\u00fcrden, k\u00f6nnten wir die Wahrscheinlichkeiten direkt berechnen, ohne eine Zufallsstichprobe zu erzeugen. Wie k\u00f6nnen wir also vorgeschlagene Werte von \\(\\theta\\) akzeptieren oder ablehnen, ohne die funktionale Form zu kennen? Die Antwort lautet: der Metropolis-Hastings-Algorithmus!</p>"},{"location":"Themen/bayesian_modeling/#metropolis-hastings","title":"Metropolis Hastings","text":"<p>Der Metropolis-Hastings-Algorithmus kann verwendet werden, um zu entscheiden, welche vorgeschlagenen Werte von \\(\\theta\\) akzeptiert oder abgelehnt werden sollen, auch wenn wir die funktionale Form der posterioren Verteilung nicht kennen. Schauen wir uns den Algorithmus genauer an. </p> <p></p> <p>Diese Abbildung zeigt einen Trace-Plot und ein Dichtediagramm f\u00fcr eine Proposal Distribution mit einem Mittelwert von \\(\\theta_{t-1} = 0,517\\). Der gezogene Wert \\(\\theta_{new} = 0,380\\) wird als \"new\" bezeichnet, da noch nicht entschieden wurde, ob dieser Wert akzeptiert oder abgelehnt wird.</p> <p>Im ersten Schritt des Metropolis-Hastings-Algorithmus berechnen wir das Verh\u00e4ltnis \\(r(\\theta_{new}, \\theta_{t-1})\\):</p> \\[r(\\theta_{new}, \\theta_{t-1}) = \\frac{Posteriori(\\theta_{new})}{Posteriori(\\theta_{t-1})}\\] <p>In der Abbildung betr\u00e4gt dieser Wert f\u00fcr \\(\\theta_{new} = 0,380\\) und \\(\\theta_{t-1} = 0,517\\) 1,307. </p> <p>Im zweiten Schritt wird die Akzeptanzwahrscheinlichkeit \\(\\alpha(\\theta_{new},\\theta_{t-1})\\) berechnet. Diese ergibt sich als Minimum von \\(r(\\theta_{new}, \\theta_{t-1})\\) und 1. Dieser Schritt is wichtig, damit die Wahrscheinlichkeiten zwischen 0 und 1 liegt. </p> \\[alpha(\\theta_{new},\\theta_{t-1}) = \\text{min}\\\\{r(\\theta_{new}, \\theta_{t-1}), 1\\\\}\\] <p>In der Abbildung ist dieser Wert 1,00. Das bedeutet, dass wir \\(\\theta_{new} = 0,380\\) akzeptieren und diesen Wert als Mittelwert f\u00fcr die Proposal Distribution in der n\u00e4chsten Iteration verwenden.</p> <p></p> <p>Diese Abbildung zeigt die n\u00e4chste Iteration, bei der \\(\\theta_{new} = 0,286\\) aus der Proposal Distribution mit dem Mittelwert \\(\\theta_{t-1} = 0,380\\) gezogen wurde. Das zuvor berechnete Verh\u00e4ltnis \\(r(\\theta_{new}, \\theta_{t-1})\\) betr\u00e4gt 0,747, also weniger als 1. Die berechnete Akzeptanzwahrscheinlichkeit \\(\\alpha(\\theta_{new},\\theta_{t-1})\\) betr\u00e4gt 0,747.</p> <p>Wir lehnen \\(\\theta_{new}\\) nicht automatisch ab, nur weil die Akzeptanzwahrscheinlichkeit kleiner als 1 ist. Stattdessen ziehen wir in Schritt 3 eine Zufallszahl \\(u\\) aus einer uniformen Verteilung zwischen 0 und 1. Wenn \\(u\\) kleiner als die Akzeptanzwahrscheinlichkeit ist, akzeptieren wir den vorgeschlagenen Wert von \\(\\theta_{new}\\). Andernfalls lehnen wir \\(\\theta_{new}\\) ab und behalten \\(\\theta_{t-1}\\) bei.</p> <p>Hier ist \\(u = 0,094\\) kleiner als die Akzeptanzwahrscheinlichkeit (\\(0,747\\)), also akzeptieren wir \\(\\theta_{new} = 0,286\\). \\(\\theta_{new}\\) und \\(0,286\\) sind gr\u00fcn dargestellt, um anzuzeigen, dass sie akzeptiert wurden. Im Folgenden betrachten wir eine Iteration, in der \\(\\theta_{new}\\) abgelehnt wird.</p> <p></p> <p>Diese Abbildung zeigt die n\u00e4chste Iteration, bei der \\(\\theta_{new} = 0,088\\) aus unserer Proposal Distribution mit dem Mittelwert \\(\\theta_{t-1} = 0,286\\) gezogen wurde. Das zuvor berechnete Verh\u00e4ltnis \\(r(\\theta_{new}, \\theta_{t-1})\\) betr\u00e4gt 0,039, also kleiner als 1. Die berechnete Akzeptanzwahrscheinlichkeit \\(\\alpha(\\theta_{new},\\theta_{t-1})\\) betr\u00e4gt 0,039.</p> <p>Der berechnete Wert von \\(u\\) in Schritt 3 betr\u00e4gt \\(0,247\\) und ist somit gr\u00f6\u00dfer als die Akzeptanzwahrscheinlichkeit. Daher verwerfen wir \\(\\theta_{new} = 0,088\\) und behalten \\(\\theta_{t-1} = 0,286\\) in Schritt 4 bei.</p> <p>Schauen wir uns nun diesen Prozess f\u00fcr 10.000 Zufallszahlen f\u00fcr \\(\\theta\\) an. </p> <p></p> <p>Diese Animation veranschaulicht mehrere Aspekte. Die Proposal Distribution \u00e4ndert sich in den meisten Iterationen. Der Trace-Plot zeigt kein \"Random-Walk\"-Muster. Das Dichte-Diagramm \u00e4hnelt einer n\u00fctzlichen Verteilung.</p> <p>Betrachten wir nun das Dichte-Diagramm genauer.</p> <p></p> <p>Wir sehen ein Histogramm der Stichprobe, die wir mit MCMC und dem Metropolis-Hastings-Algorithmus erstellt haben. In diesem Beispiel wissen wir, dass die Posterior-Verteilung eine Beta-Verteilung mit den Parametern 5 und 7 ist. Die rote Linie zeigt die analytische Posterior-Verteilung, und die blaue Linie ist ein Kernel-Dichte-Diagramm f\u00fcr unsere Stichprobe. Die Kernel-Dichte-Darstellung \u00e4hnelt der Beta(5,7)-Verteilung, was darauf hindeutet, dass unsere Stichprobe eine gute Ann\u00e4herung an die theoretische Posterior-Verteilung ist.</p> <p>Mit dieser Stichprobe k\u00f6nnten wir den Mittelwert oder Median der Posterior-Verteilung, ein 95%-Glaubw\u00fcrdigkeitsintervall oder die Wahrscheinlichkeit berechnen, dass \\(\\theta\\) in ein beliebiges Intervall f\u00e4llt.</p>"},{"location":"Themen/bayesian_modeling/#bayesian-modeling_1","title":"Bayesian Modeling","text":"<p>In diesem Abschnitt betrachten wir, wie ein Bayes'sches Modell in Python implementiert werden kann. Dazu verwenden wir PyMC3 und vergleichen klassische lineare Regression mit Bayes'scher linearer Regression.</p>"},{"location":"Themen/bayesian_modeling/#pymc3","title":"PyMC3","text":"<p>PyMC3 ist eine Python-Bibliothek f\u00fcr probabilistische Programmierung, die bei der bayesianischen Statistik hilft. Mit PyMC3 k\u00f6nnen komplexe Modelle erstellt und analysiert werden, um bayesianische Inferenz und Sch\u00e4tzungen durchzuf\u00fchren.</p> <p><pre><code>import pymc3 as pm\n</code></pre> Zun\u00e4chst erzeugen wir 10 zuf\u00e4llige Werte 'x_vals' zwischen 0 und 1 und berechnen die entsprechenden wahren y-Werte (true_y_vals). Schlie\u00dflich werden zu den wahren y-Werten Zufallszahlen aus einer Normalverteilung mit einer Standardabweichung (sigma) addiert, um die beobachteten y-Werte (y_vals) zu erhalten.</p>"},{"location":"Themen/bayesian_modeling/#erzeugen-von-daten","title":"Erzeugen von Daten","text":"\\[y_{true}=mx+b$$ $$y=y_{true} + N(0,\\sigma)\\] <pre><code>true_slope = 5\ntrue_intercept = 10\ntrue_sigma = 1\n\ntrue_params = {'slope': true_slope, 'intercept': true_intercept, 'sigma': true_sigma}\nnum_points = 10\n\nx_vals = np.linspace(0, 1, num_points)\ntrue_y_vals = true_slope * x_vals + true_intercept\ny_vals = true_y_vals + np.random.normal(scale=true_sigma, size=num_points)\n</code></pre> <p>In der folgenden Abbildung sehen wir die erzeugten y-Werte (y_vals) und x-Werte (x_vals) sowie die berechnete Gerade. Unser Ziel ist es, eine Gerade zu finden, die die x_vals am besten beschreibt.</p> <p></p> <p>Nun f\u00fchren wir die klassiche Lineare Regression aus.</p> <pre><code>clf = LinearRegression()\nclf.fit(x_vals.reshape(-1,1), y_vals)\npreds = clf.predict(x_vals.reshape(-1,1))\nresids = preds - y_vals\n\nOutput:\n\nTrue Model:\ny_true = 5*x + 10\nTrue sigma: 1\n\nEstimated Model:\ny_hat = 6.602808064975681*x + 9.703400820259425\nSd Residuals: 0.4703668242377374\n</code></pre>"},{"location":"Themen/bayesian_modeling/#bayes-lineare-regression","title":"Bayes Lineare Regression","text":"<p>Im folgenden Abschnitt gehen wir auf die Bayes'sche lineare Regression ein und vergleichen abschlie\u00dfend die Ergebnisse mit der klassischen linearen Regression.</p> <p>Piori: \\(\\(m \\sim N(0,20)\\)\\) \\(\\(b \\sim N(0,20)\\)\\) \\(\\(\\sigma \\sim Exp(1)\\)\\)</p> <p>Likelihood: \\(\\(y|m,b,\\sigma \\sim N(mx+b, \\sigma)\\)\\)</p> <p>Posterori: \\(\\(m,b,\\sigma | y \\sim ?\\)\\) \\(\\(P(m,b,\\sigma|y) \\propto P(y|m,b,\\sigma) \\times P(m) \\times P(b) \\times P(\\sigma)\\)\\)</p>"},{"location":"Themen/bayesian_modeling/#modell-erstellen","title":"Modell erstellen","text":"<p>Mit Hilfe von PyMC3 l\u00e4sst sich das Modell sehr einfach und schnell definieren.</p> <pre><code>basic_model = pm.Model()\n\nwith basic_model:\n    #priors\n    sigma = pm.Exponential(\"sigma\", lam=1.0)\n    intercept = pm.Normal(\"intercept\", mu=0, sigma=20)\n    slope = pm.Normal(\"slope\", mu=0, sigma=20)\n\n    # Expected value of outcome\n    mu = slope*x_vals + intercept\n\n    # Likelihood (sampling distribution) of observations\n    likelihood = pm.Normal(\"Y_obs\", mu=mu, sigma=sigma, observed=y_vals)\n</code></pre>"},{"location":"Themen/bayesian_modeling/#mcmc_1","title":"MCMC","text":"<p>Da es sich hier nicht um konjugierte Prioriverteilungen handelt, m\u00fcssen wir die Posterioriverteilung approximativ bestimmen. </p> <p><pre><code>with basic_model:\n    trace = pm.sample(1000, cores = 4)\n</code></pre> Wir f\u00fchren hier 1000 Iterationen mit 4 Kernen durch. Die Proposal-Distributions sind in diesem Fall die Prioriverteilungen.</p> <p>PyMC3 entscheidet automatisch, welche der MCMC-Methoden f\u00fcr das vorliegende Problem am besten geeignet ist. In diesem Fall wird der Hamilton-Monte-Carlo-NUTS-Algorithmus verwendet, um die Posterioriverteilungen zu approximieren. </p> <p>Dies ist das Ergebnis. Auf der linken Seite sehen wir die entstandenen Dichteverteilungen, und auf der rechten Seite den entsprechenden Trace-Plot. Wir sehen, dass die Ergebnisse f\u00fcr slope, intercept und sigma identisch mit den Ergebnissen der klassischen linearen Regression sind.</p> <p>Ein bayesianischer Ansatz wie in diesem Beispiel liefert also nicht unbedingt andere oder bessere Ergebnisse, sondern eine bessere Interpretation des Ergebnisses. Nur in sehr bestimmten F\u00e4llen lohnt sich der bayesianische Ansatz.</p>"},{"location":"Themen/bayesian_modeling/#vor-und-nachteile","title":"Vor und Nachteile","text":"<p>In folgenden F\u00e4llen lohnt sich ein bayesianischer Ansatz:</p> <ul> <li>Quantifizierbare vorherige Erkenntnisse</li> <li>Wenige Daten</li> <li>Quantifizierung von Unsicherheit</li> <li>Hierarchische Modellierung</li> </ul> <p>Nachteile eines bayesianischen Ansatzes:</p> <ul> <li>Auswahl der Priori</li> <li>Hoher Rechenaufwand</li> <li>Erfordert mehr statistisches Fachwissen als einige andere Methoden</li> </ul> <p>Das war unsere kurze Einf\u00fchrung in das bayesianische Modellieren. Es ist wichtig, dass man die Bayes-Statistik beherrscht, um bayesianische Methoden effektiv einsetzen zu k\u00f6nnen. Weiterf\u00fchrende Materialien, einschlie\u00dflich eines entsprechenden Kurses der LMU und weiteren relevanten Quellen, finden Sie unter \"Weiterf\u00fchrende Materialien\". F\u00fcr viele klassische Machine Learning Modelle gibt es auch entsprechende bayesianische Modelle wie z.B. bayesianische logistische Regression und bayesianische neuronale Netze. Allerdings w\u00fcrde die Behandlung dieser Modelle den Rahmen dieser Ausarbeitung sprengen. </p>"},{"location":"Themen/bayesian_modeling/#weiterfuhrendes-material","title":"Weiterf\u00fchrendes Material","text":"<ul> <li>LMU Kurs</li> <li>PyMC3 </li> <li>Visualizierungen</li> </ul>"},{"location":"Themen/bayesian_modeling/#podcast","title":"Podcast","text":"<p>Hier Link zum Podcast.</p>"},{"location":"Themen/bayesian_modeling/#talk","title":"Talk","text":"<p>Hier einfach Youtube oder THD System embedden.</p>"},{"location":"Themen/bayesian_modeling/#demo","title":"Demo","text":"<p>Link zur Code Demo</p>"},{"location":"Themen/bayesian_modeling/#literaturliste","title":"Literaturliste","text":"<ul> <li>MCMC Metroplis Hastings</li> </ul>"},{"location":"Themen/t7_Process_Mining/","title":"Process Mining","text":"<p>von Amelie Kammerer, Joshua Groszeibl und Tabea Haas</p>"},{"location":"Themen/t7_Process_Mining/#abstract","title":"Abstract","text":""},{"location":"Themen/t7_Process_Mining/#von-tabea-haas","title":"von Tabea Haas","text":"<p>In der heutigen digitalen \u00c4ra ist die F\u00e4higkeit, Daten in wertvolle Erkenntnisse umzuwandeln, entscheidend f\u00fcr den Erfolg eines Unternehmens. Prozessanalyse, insbesondere durch Process Mining, ist zu einem zentralen Ansatz geworden, um Effizienz, Transparenz und Qualit\u00e4t in Organisationen zu verbessern. Nahfolgendes soll einen umfassenden Einblick in das Feld des Process Minings gew\u00e4hren, und die verschiedenen Aspekte beleuchten, die diese aufstrebende Technologie so erfolgreich machen.</p>"},{"location":"Themen/t7_Process_Mining/#einleitung","title":"Einleitung","text":"<p>Heute werden wir uns mit einem faszinierenden Thema besch\u00e4ftigen - Process Mining. Vielleicht haben Sie schon davon geh\u00f6rt, aber falls nicht, keine Sorge! Process Mining ist eine Technologie mit enormem Potenzial, die vielen noch unbekannt ist. Dabei handelt es sich um eine Methode, um Abl\u00e4ufe und Prozesse in Unternehmen mithilfe von Daten zu analysieren und zu visualisieren.</p> <p>Stellen Sie sich vor, Sie k\u00f6nnten wie mit einer R\u00f6ntgenaufnahme in die Abl\u00e4ufe eines Unternehmens blicken. Das ist genau das, was Process Mining erm\u00f6glicht. Indem es Daten aus verschiedenen Quellen wie Datenbanken oder Anwendungsprotokollen sammelt, k\u00f6nnen wir einen detaillierten Einblick in die Prozesse erhalten. Basierend auf diesen Daten wird ein Prozessmodell erstellt, das uns zeigt, wie Aktivit\u00e4ten miteinander verbunden sind, wie lange sie dauern und welche Ressourcen daf\u00fcr ben\u00f6tigt werden.</p> <p>Process Mining hat eine breite Anwendungspalette und findet in verschiedenen Bereichen wie Logistik, Fertigung, Gesundheitswesen, Finanzen und Forensik Anwendung. Sogar gro\u00dfe Unternehmen wie BMW und Siemens nutzen Process Mining, um ihre Prozessqualit\u00e4t zu verbessern und effizienter zu arbeiten.</p> <p>Die Vorteile von Process Mining liegen auf der Hand. Durch die objektive und datenbasierte Analyse erhalten Unternehmen ein umfassendes Bild ihrer tats\u00e4chlichen Abl\u00e4ufe. Dadurch k\u00f6nnen Fehlerquellen, Ineffizienzen und Verbesserungspotenziale identifiziert werden. Prozesse k\u00f6nnen transparenter gestaltet und die Effizienz gesteigert werden, was letztendlich zu Kosteneinsparungen und einer h\u00f6heren Kundenzufriedenheit f\u00fchrt.</p> <p>Es gibt jedoch auch Grenzen und Herausforderungen beim Einsatz von Process Mining. Es erfordert ausreichend Daten, um aussagekr\u00e4ftige Analysen durchf\u00fchren zu k\u00f6nnen. Zudem besteht die Gefahr, dass Unternehmen sich zu sehr in die Analyse vertiefen und zu wenig Zeit in die Umsetzung von Verbesserungsma\u00dfnahmen investieren, was als \"Analyse-Paralyse\" bezeichnet wird.</p> <p>In diesem Artikel werden wir Process Mining genauer betrachten, die Funktionsweise anhand einer Code Demonstration erkl\u00e4ren, und \u00fcber die Vor- und Nachteile dieser Technologie diskutieren.</p>"},{"location":"Themen/t7_Process_Mining/#wisschenschaftlicher-blogartikel","title":"Wisschenschaftlicher Blogartikel","text":""},{"location":"Themen/t7_Process_Mining/#von-amelie-kammerer","title":"von Amelie Kammerer","text":""},{"location":"Themen/t7_Process_Mining/#einfuhrung","title":"Einf\u00fchrung","text":"<p>Process-Mining wird generell als die Br\u00fccke zwischen traditioneller Prozess-Analyse und datenzentrierter Analysetechniken, wie Maschinellem Lernen und Data Mining bezeichnet. Das Ziel ist, auf Basis von Event Daten Einblicke in Prozess zu bekommen und dadurch Prozesse positiv zu ver\u00e4ndern. Auf diese Weise bekommt man Informationen \u00fcber die Prozesse, wie mehr Perspektiven f\u00fcr beispielsweise den zeitlichen Kontext, f\u00fcr die Interpretation der Daten und f\u00fcr das Verstehen des Prozesses. Process-Mining kann dabei f\u00fcr alle m\u00f6glichen Bereiche hergenommen werden, sei es Gesundheitswesen, Finanzen, Logistik oder auch bei Informations- und Kommunikationstechnologien. Bei allen Prozessen, die Event Daten liefern (s. Kapitel Event Daten). Da heutzutage immer mehr IT-System in die Prozesse, die sie unterst\u00fctzen integriert werden, entstehen auch immer mehr Daten. Die Unternehmen haben aber Probleme, mit dieser Menge an Daten umzugehen und einen tats\u00e4chlich Wert daraus zu ziehen. Hier kommt Process-Mining ins Spiel, das mithilfe der folgenden drei Phasen die Daten praktisch verwertet. Im Gegensatz zu normalem Prozess Management liegt der Fokus nicht nur auf dem Modellieren der Prozesse, sondern in der Nutzung der Daten.</p>"},{"location":"Themen/t7_Process_Mining/#die-3-phasen-im-process-mining","title":"Die 3 Phasen im Process Mining","text":"<p>Prozesserkennung | process discovery: Auf Basis der gegebenen Daten wird ein Modell erstellt, das das tats\u00e4chliche Modell eines Prozesses widerspiegelt. Anfangs ging das nur mit sequentiellen Modellen, wobei mit dem Alpha Algorithmus als ersten Algorithmus das Problem der Nebenl\u00e4ufigkeit gel\u00f6st wurde (s. Kapitel Algorithmen). Konformit\u00e4tspr\u00fcfung | conformance checking: Man versucht Unterschiede zwischen Prozess-Modell und tats\u00e4chlichem Ablauf des Prozesses zu finden. Ein existierendes Prozess Modell, sei es durch Process Discovery, oder manuell erstellt, wird also mit Event Logs des gleichen Prozesses verglichen. Gibt es zum Beispiel Regeln, an die sich in einem Prozess gehalten werden muss, kann gecheckt werden, ob das auch tats\u00e4chlich der Fall ist. Hier kann auch festgestellt werden, ob das Modell noch verbessert werden muss. Es werden also Abweichungen erkannt, lokalisiert und erkl\u00e4rt. Prozessverbesserung | process enhancement: Hier ist das Ziel, ein bereits existierendes Modell zu erweitern oder zu verbessern, anhand von Informationen, die bereits \u00fcber einen Prozess gesammelt wurden. Verbessern: Eine Art davon w\u00e4re das Reparieren eines Prozess Modells, das hei\u00dft, dass das Modell so angepasst wird, dass es die Realit\u00e4t besser widerspiegelt. Beispielsweise, wenn zwei Aktionen hintereinander im Modell dargestellt werden, in Realit\u00e4t aber in beliebiger Order geschehen k\u00f6nnen. Auf diese Weise wird die Fitness des Modells gesteigert. Erweitern: Eine andere Art w\u00e4re eine Erweiterung des Prozess Modells. Man kann beispielweise mit neuen Daten, neue Ansichten auf ein Modell erstellen. Mit Performance Daten in Form von Zeitstempeln zum Beispiel k\u00f6nnen dann Engp\u00e4sse identifiziert oder Durchlauf-Zeiten analysiert werden. Simulationen: Try-and-error, ohne dass die Konsequenzen der Fehler tats\u00e4chlich getragen werden m\u00fcssen. Das ist zum Beispiel sehr praktisch, wenn entschieden werden soll, worauf die Priorit\u00e4ten gesetzt werden und wie limitierte Ressourcen benutzt werden sollen.  Diese drei Phasen von Process Mining sind in der realen Welt zyklisch (s. Abb. 1) Die Real-Welt wird von IT-Systemen unterst\u00fctzt und kontrolliert wird. Durch Einbindung der IT-System in der Real-Welt werden die Event Daten gewonnen. Anhand derer wird schlie\u00dflich das Prozess Modell erstellt, das wiederum direkt Einfluss auf die IT-Systeme hat. Conformance checking steht zwischen dem Prozess Modell und den Event Daten und gleicht st\u00e4ndig ab. Auf Basis des Prozess Modells und der Event Daten l\u00e4uft Process Enhancement, welches das Prozess Modell und dar\u00fcber auch die Real-Welt beeinflusst. Mithilfe der datenbasierten Einsichten, kann ein Unternehmen Verbesserungen schnell \u00fcbernehmen und auch Automatisierungsm\u00f6glichkeiten f\u00fcr repetitive Aufgaben k\u00f6nnen gefunden werden.</p>"},{"location":"Themen/t7_Process_Mining/#perspektiven","title":"Perspektiven","text":"<p>Man muss auch au\u00dferdem beachten, dass es f\u00fcr diese Modelle auch unterschiedliche Perspektiven gibt: - Kontrollfluss-Perspektive: Hier handelt es sich um die Anordnungen der Aktivit\u00e4ten, die durchgef\u00fchrt werden. Alle m\u00f6glichen Pfade, die in einem Prozess ablaufen k\u00f6nnen, werden hier festgehalten. F\u00fcr andere Perspektiven gibt es andere Notationen, in die umgeschrieben werden kann. - Organisationaufbau-Perspektive: Hier werden die Modelle mit Informationen \u00fcber Ressourcen, wie Personen, die involviert sind angereichert. Ein Unternehmen kann soziale Netzwerke zwischen den Personen aufstellen, verschieden Mitarbeiter gruppieren und sogar einzelne Personen analysieren. Bei dieser Perspektive muss allerdings stark auf Datenschutz geachtet werden. - Case-Perspektive: Ein Case repr\u00e4sentiert einen kompletten Durchlauf in einem Prozess Modell (s. Kapitel Event Daten). Hier k\u00f6nnen die Attribute von den Cases betrachtet werden. Man w\u00fcrde dabei beispielsweise sehen, wenn es Zusammenh\u00e4nge zwischen St\u00f6rungen und Lieferanten gibt. - Performance-Perspektive: Diese Perspektive befasst sich mit allem, das mit Zeitstempeln zu tun hat. Es geht dabei oft um das Finden von Engp\u00e4ssen oder das Berechnen von Durchlauf Zeiten.</p> <p>Generell k\u00f6nnen diese unterschiedlichen Perspektiven aber nat\u00fcrlich auch \u00fcberlappend sein.</p>"},{"location":"Themen/t7_Process_Mining/#moglichkeiten-und-vorteile-bei-process-mining","title":"M\u00f6glichkeiten und Vorteile bei Process Mining","text":"<p>Nachteile bei einem manuell erstellten Prozess Modell: - Designer konzentriert sich nur auf normales und gew\u00fcnschtes Verhalten, obwohl das meist nur 80% der F\u00e4lle abdeckt. Die Anderen 20% verursachen aber \u00fcblicherweise 80% der Fehler. - Modell wird oft davon beeinflusst, welche Rolle der Designer im Unternehmen spielt - Es mag bei einem Bandarbeiter, der den ganzen Tag das gleiche macht, vielleicht leicht sein, den Prozess darzustellen, aber sobald Menschen, die in verschiedenen Prozessen gleichzeitig arbeiten, betrachtet werden, wird es schwieriger. Es ist unm\u00f6glich einen Prozess in Isolation zu betrachten, wenn die Mitarbeiten ihre Aufmerksamkeit aufteilen m\u00fcssen und unterschiedlichste Priorit\u00e4ten haben. - Das Abstraktionslevel festzulegen ist sehr schwierig und sobald man sich auf ein Abstraktionslevel festgelegt hat, ist es extrem aufw\u00e4ndig, das wieder zu \u00e4ndern. Au\u00dferdem \u00e4ndern sich die Abstraktionslevel mit der Zeit meistens. - Modell stellt oft eher die idealisierte Version eines Prozess Modells dar und nicht die tats\u00e4chliche.</p> <p>Deshalb wird Process Mining ben\u00f6tigt, hier wird auf Basis von vorhandenen tats\u00e4chlichen Daten mit dem Bottom-up Prinzip das Prozess Modell konstruiert. Auf diese Weise wird nicht der Soll-, sondern der Ist Prozess dargestellt und man bekommt einen objektiven Einblick in die Prozessstruktur. Au\u00dferdem gibt es dadurch nicht nur ein Modell, sondern unterschiedliche Modelle auf Basis derselben Fakten. Der Benutzer kann entscheiden, welches Abstraktionslevel er gerne sehen m\u00f6chte. Ein weiterer Punkt ist, dass das Modell den Mitarbeiter mit all seinen Facetten sieht, die ineffizienten T\u00e4tigkeiten, aber auch, wenn einzelne Personen flexibel mit Problemen und mit sich ver\u00e4ndernden Workloads umgehen.</p>"},{"location":"Themen/t7_Process_Mining/#event-logs","title":"Event Logs","text":"<p> In der Abbildung 2 sieht man einen sogenannten Event Log. Jede Zeile repr\u00e4sentiert ein Event, jedes Event hat eine eindeutige ID, die Events sind gruppiert in einzelne Cases, die die einzelnen Prozessabl\u00e4ufe repr\u00e4sentieren. Au\u00dferdem haben die Events noch beliebig viele Attribute, wie beispielsweise den Zeitstempel. Diese Attribute k\u00f6nnen beliebig detailliert festgehalten werden und in jeglichen Bereich gehen, wie beispielsweise Ressourcen oder Kosten. Es m\u00fcssen auch nicht alle Events die gleichen Attribute haben, wobei es typisch ist, dass Events, die die gleiche Aktivit\u00e4t repr\u00e4sentieren, den gleichen Satz an Attributen haben. Wichtig ist jedoch: um f\u00fcr Process-Mining nutzbar zu sein muss ein Event zu einem Case und zu einer Aktivit\u00e4t zugeordnet sein. Das ist das Minimum. Au\u00dferdem m\u00fcssen die Events innerhalb eines Case geordnet sein. Ohne die Information, in welcher Reihenfolge die Events abliefen, ist es unm\u00f6glich Abh\u00e4ngigkeiten herauszufinden. Um allerdings schlie\u00dflich mehr Informationen zu bekommen, beispielweise in Richtung Performance, braucht es Attribute, wie den Zeitstempel.</p>"},{"location":"Themen/t7_Process_Mining/#petri-netz","title":"Petri-Netz","text":"<p>Petri-Netze k\u00f6nnen eine visuelle Repr\u00e4sentation f\u00fcr ein Prozess-Modell sein. Es gibt wie oben beschrieben unterschiedliche Perspektiven aus denen man ein Prozess-Modell analysieren kann, weshalb es auch unterschiedliche Darstellungsformen gibt. Um es einfacher zu halten, wird ein Beispiel in der Kontrollflussperspektive gegeben und nicht auf alle einzelnen Perspektiven eingegangen.</p>"},{"location":"Themen/t7_Process_Mining/#warum-petri-netze","title":"Warum Petri-Netze","text":"<p>Petri-Netz ist die \u00e4lteste und am besten erforschte Modellierungssprache, die das Modellieren von Nebenl\u00e4ufigkeit erlaubt. Im Gegensatz zu einfacheren Darstellungsformen, lassen sich hier parallele Aktivit\u00e4ten gut darstellen. Die grafische Notation ist intuitiv und simple und trotzdem sind die Netze ausf\u00fchrbar und mit bestimmten Techniken analysierbar. F\u00fcr die unterschiedlichen Perspektiven und Nutzungsarten gibt es aber bereits erweiterte Petri Nets, die auch den Daten und Zeit Bezug mit abbilden k\u00f6nnen.  Das Petri Netz in der Abbildung 3 beschreibt die Abfertigung einer Anfrage f\u00fcr Entsch\u00e4digung in einer Airline. Man kann einen Start und ein Ende sehen. Es gibt au\u00dferdem \u00dcberg\u00e4nge, die mit Quadraten gekennzeichnet sind und Orte, die mit einem Kreis gekennzeichnet sind. Diese Orte repr\u00e4sentieren die Zust\u00e4nde in denen sich das System befinden kann. Die \u00dcberg\u00e4nge sind durch die Orte verbunden. Das Netz an sich ist statisch, allerdings gibt es zus\u00e4tzliche Tokens, die gefeuert werden, in Form von Punkten, wie hier in Start. Der Zustand des Netzes wird durch die Verteilung ebendieser Tokens \u00fcber die verschiedenen Orte beschrieben und diese Verteilung wird als \"Marking\" bezeichnet. In dem Netz das Sie jetzt sehen, ist nur der Ort \"Start\" markiert.</p>"},{"location":"Themen/t7_Process_Mining/#funktion-petri-netz","title":"Funktion Petri-Netz","text":"<p>In einem Petri Netz ist ein \u00dcbergang freigegeben, das hei\u00dft die dazugeh\u00f6rige Aktivit\u00e4t kann geschehen, wenn alle Eingabe-Orte einen Token haben, also markiert sind. In dem Beispiel aus der Abbildung hat \"register request\" nur einen Eingabe-Ort und dieser Ort ist markiert. Das hei\u00dft \"register request\" kann durchgef\u00fchrt werden. Wenn \"register request\" durchgef\u00fchrt wird, versteh man das als Feuern. Der \u00dcbergang nimmt von jedem Eingabe-Ort einen Token auf und feuert dann an jeden Ausgabe-Ort einen Token ab. Hier konsumiert \"register request\" also einen Token und produziert zwei neue Tokens f\u00fcr c1 und c2. Nun sind drei neue \u00dcberg\u00e4nge freigegeben: \"examine throughly\", \"examine casually\" und \"check ticket\". Es kann nun entweder \"examine throughly\" oder \"examine casually\" durchgef\u00fchrt werden, da es nur einen Token gibt. Wenn einer von den zwei \u00dcberg\u00e4ngen durchgef\u00fchrt wird, wird der Token von c1 konsumiert und der andere \u00dcbergang ist nicht mehr freigegeben. \"check ticket\" hat jedoch einen eigenen Eingabe-Ort mit einem eigenen Token. Das hei\u00dft dieser \u00dcbergang kann parallel durchgef\u00fchrt werden. Oben wird dann entweder \"examine throughly\" oder \"examine carefully\" durchgef\u00fchrt, wodurch der Token in c1 konsumiert wird und ein Token zu c3 gefeuert wird und unten konsumiert und feuert \"check ticket\", sodass nun Token bei c3 und c4 sind. Dadurch wird \"decide\" freigegeben, da alle Eingabe-Orte von dem \u00dcbergang \"decide\" einen Token haben. Auf diese Weise kann das Netz weiter ausgef\u00fchrt werden. Ein solches Netz kann auch zyklisch sein, wie sich an \"reinitiate request\" erkennen l\u00e4sst. Der Prozess endet nachdem entweder die Entsch\u00e4digung gezahlt wurde, oder die Anfrage zur\u00fcckgewiesen wurde. In einem Petri Netz k\u00f6nnen auch gleichzeitig mehrere Tokens an den unterschiedlichen Orten sein, die dann unterschiedliche Cases aus dem Event Log repr\u00e4sentieren. Mithilfe dieses Token basierten Ansatzes, kann auch conformance checking sehr einfach durchgef\u00fchrt werden. Vor allem zu Beginn von Process Mining, war das sehr verbreitet. Inzwischen gibt es aber auch schon neue Ans\u00e4tze, die noch besser funktionieren., oft aber trotzdem auf \u00e4hnlichen Prinzipien basieren.</p>"},{"location":"Themen/t7_Process_Mining/#algorithmen-fur-process-mining","title":"Algorithmen f\u00fcr Process Mining","text":"<p>Es gibt jede Menge Algorithmen, die innerhalb des Process Mining genutzt werden, insbesondere auch, um das Prozess Modell zu erstellen.</p>"},{"location":"Themen/t7_Process_Mining/#ubersicht-algorithmen-und-herausforderungen","title":"\u00dcbersicht Algorithmen und Herausforderungen","text":"<p>Die ersten drei Process Miner \u00fcberhaupt waren allesamt basierend auf R\u00fcckgekoppelten Neuronalen Netzen. Dann wurde der Alpha Miner entwickelt, der Arbeitsflussmodelle konstruieren konnte und automatisch Petri Netze erstellt, die dann weiter analysiert werden konnten. Generell gibt es relativ viele Probleme mit denen die Algorithmen klarkommen m\u00fcssen. Die Algorithmen werden auch heute immer noch auf Basis dieser Probleme weiterentwickelt: - Die Korrelation finden: Teilweise ist es schwierig zu Events die zugeh\u00f6rigen Cases zu finden. Die Korrelation zwischen Case und Event kann also zum Problem werden. - Timestamps: die Events m\u00fcssen in einer Reihenfolge geordnet sein, nat\u00fcrlich braucht es eigentlich keine Timestamps, aber um diese Ordnung innerhalb der Cases herauszufinden in vielen F\u00e4llen schon. Timestamps sind aber oft fehlerhaft, sei es wegen falschen Zeiten im System, oder einfach, weil der Abschluss nicht direkt aufgezeichnet wird, wenn ein Event erfolgt ist. Ein Problem kann auch sein, dass nur der Tag des Eventabschlusses angegeben wird, an diesem Tag aber mehrere Events stattgefunden haben. - Umfang: Zu determinieren, wie gro\u00df der Umfang sein soll ist schwierig und hierf\u00fcr wird h\u00e4ufig Expertenwissen ben\u00f6tigt. In vielen Unternehmen gibt es Unmengen an Daten und es ist unklar, welche Daten \u00fcberhaupt ben\u00f6tigt werden. - Granularit\u00e4t: Es ist schwierig zu sagen, wie genau die Event Logs sein m\u00fcssen, es kann sein, dass es Unmengen an Informationen zu einzelnen Events gibt, das Ziel aber nur eine \u00dcbersicht des Arbeitsablaufs ist, das dem Stakeholder pr\u00e4sentiert werden soll. - Und ein gro\u00dfer Punkt ist auch noch die Datenqualit\u00e4t bei falschem Logging:      - Missing in log: Etwas ist in der Realit\u00e4t passiert, wurde aber nicht aufgezeichnet     - Missing in reality: Ein Event wurde aufgezeichnet, ist aber in der Realit\u00e4t gar nicht passiert     - missing attribute: Ein Attribut eines Events fehlt     - Incorrect attribute: Ein Attribut eines Events ist falsch aufgezeichnet     - Imprecise attribute: Das Attribut eines Events wurde zu ungenau aufgezeichnet     - Noise: Der Algorithmus muss mit Noise umgehen k\u00f6nnen. F\u00fcr den Algorithmus ist es eigentlich unm\u00f6glich zwischen besonderen Events und falschem Logging zu unterscheiden, hier braucht es oft menschliche Beurteilung.     - Vollst\u00e4ndigkeit: Ein weiterer Grund w\u00e4ren zu wenig Daten, um daraus ein repr\u00e4sentatives Modell zu erstellen \u21d2 Es kann also bei Noise zu viele Daten geben oder bei der Vollst\u00e4ndigkeit zu wenige Daten geben. Es ist schwierig zu sagen, ob man alle m\u00f6glichen Abl\u00e4ufe schon in den Trainingsdaten gesehen hat oder vielleicht ein Trace eigentlich m\u00f6glich aber bisher einfach noch nicht aufgetreten ist. Die Probleme in Datenqualit\u00e4t kommen oft daher, dass Event Daten oft einfach nur als Nebenprodukt gesehen werden und deshalb kein gro\u00dfer Wert auf die Qualit\u00e4t gelegt wird</p>"},{"location":"Themen/t7_Process_Mining/#alpha-algorithmus","title":"Alpha Algorithmus","text":"<p>Da der Alpha Miner der erste Prozesserkennungs-Algorithmus war, der Nebenl\u00e4ufigkeit von Events verarbeiten konnte und viele der darauffolgenden Algorithmen auf Basis dessen entstanden sind, wird im Folgenden genauer darauf eingegangen. Der Alpha Miner verarbeitet Event Logs und erstellt darauf basierend ein Petri Netz, das die Logs widerspiegelt. Daf\u00fcr basiert der Alpha Miner auf 3 Regeln: - Temporal dependency: b folgt a aber a folgt niemals b \u21d2 b ist abh\u00e4ngig von a; geschrieben a \u2192 b  - Temporal independency: es gibt Aufzeichnungen bei denen a auf b folgt, aber auch welche, bei denen b auf a folgt \u21d2 a und b k\u00f6nnen parallel durchgef\u00fchrt werden; geschrieben a || b  - Independency: Es gibt keine Aufzeichnung bei der a auf b folgt oder b auf a folgt \u21d2 die beiden Events sind unabh\u00e4ngig voneinander; geschrieben a # b</p>"},{"location":"Themen/t7_Process_Mining/#ablauf-algorithmus","title":"Ablauf Algorithmus","text":"<p> Dann wird der Algorithmus wie in Abbildung 6 zu sehen durchgef\u00fchrt. F\u00fcr ein besseres Verst\u00e4ndnis direkt an einem Beispiellog L. L = [, , ] 1.  Alle m\u00f6glichen Events werden definiert: [A,B,C,D,E] 2.  Alle m\u00f6glichen Start Events werden definiert [A] 3.  Alle m\u00f6glichen end Events werden definiert [D] 4.  Alle m\u00f6glichen Sets A und B werden definiert, wobei alle Events innerhalb von A und innerhalb von B unabh\u00e4ngig voneinander sein m\u00fcssen. Au\u00dferdem m\u00fcssen alle Events in A gleichzeitig mit den Events in B verbunden sein.      - Footprint Matrix als Unterst\u00fctzung      - ({a},{b}), ({a},{c}), ({a},{e}), ({b},{d}), ({e},{d}), ({c},{d}) unabh\u00e4ngig von sich selbst aber \u201etemporal dependency\u201c zwischen A und B     - ({a},{b,e}), ({a},{c,e}), ({b,e},{d}), ({c,e},{d}) \u201ctemporal dependency\u201d zwischen allen Events in A und allen Events in B, die Events in A sind alle unabh\u00e4ngig voneinander und die Events in B sind unabh\u00e4ngig voneinander 5.  Alle Non-Maximum Sets werden gel\u00f6scht =&gt; unn\u00f6tige Duplikate l\u00f6schen      - ({a},{b}) ist beispielsweise bereits in ({a},{b,e}) repr\u00e4sentiert und kann deswegen gel\u00f6scht werden. Das Gleiche gilt f\u00fcr mehrere Sets weshalb folgende Sets \u00fcbrigbleiben:     ({a},{b,e}), ({a},{c,e}), ({b,e},{d}), ({c,e},{d}) 6.  Es werden Orte f\u00fcr alle Sets erstellt und ein Start- und Endpunkt festgelegt 7.  Verbindungen werden aufgezeichnet \u2026 8.  \u2026und das Petri Netz wird zur\u00fcckgegeben"},{"location":"Themen/t7_Process_Mining/#daten-fur-process-mining","title":"Daten f\u00fcr Process Mining","text":"<p>Abbildung 9  Nat\u00fcrlich braucht man f\u00fcr Process Mining als Erstes die Daten, also ist es wichtig zu verstehen, wie man zu den Daten kommt. Die Anforderungen an die Daten kommen auf die Process Mining Technik an, die verwendet werden soll, damit auch das Ziel, das verfolgt wird \u21d2 Welche Frage soll beantwortet werden mit dem Modell? Welche Sichtweisen sind essentiell bei den vorhandenen Daten? Nat\u00fcrlich muss au\u00dferdem mit Qualit\u00e4ts Problemen in den Daten umgegangen werden.  Die Sichtweise ist f\u00fcr das Ausw\u00e4hlen der Daten sehr entscheidend. Schaut man sich beispielsweise ein Krankenhaus an, sind die manche daran interessiert, welche Schritte der Patient w\u00e4hrend des Krankenhausbesuches durchl\u00e4uft und andere daran, wie man den Arbeitsablauf innerhalb einer bestimmten Abteilung verbessern kann. Es kann erst einmal eine Vielzahl an Datenquellen geben, seien es Datenbanken, Nachrichtenprotokolle, Transaktionsprotokolle oder ERP-Systeme. Schlie\u00dflich wird im Zusammenhang mit Data Mining oft von ETL = extract, transform and load geredet. Wobei man bei transform vor allem auf Syntax und Semantik achten muss. Dadurch werden die gesammelten Daten verarbeitet in ein Zielsystem geladen. Teilweise gibt es schon ein Data Warehouse, teilweise noch nicht. Die Daten m\u00fcssen in jedem Fall extrahiert werden und in Event Logs umgewandelt werden. Wie in Event Logs schon betrachtet braucht es daf\u00fcr eine Reihenfolge innerhalb der Cases, einen Namen f\u00fcr die Aktivit\u00e4t der Events und die dazugeh\u00f6rige Case. Dann werden die unterschiedlichen Techniken process discovery, conformance checking und process enhancement angewendet. Allerdings ist das ganze iterativ. Sobald ein Prozess Modell erstellt wurde kommen h\u00f6chstwahrscheinlich neue Fragen auf, wof\u00fcr neue Daten gesammelt werden m\u00fcssen und so l\u00e4uft der Prozess immer wieder durch. Wie auch schon bei den Algorithmen besprochen gibt bei den Daten einige Herausforderungen, neben dem Expertenwissen, dass man in Bezug auf Datenschutz und Datenqualit\u00e4t bez\u00fcglich Inhalt braucht, muss auch auf Noise, falsches Logging und die Vollst\u00e4ndigkeit der Daten geachtet werden.</p>"},{"location":"Themen/t7_Process_Mining/#qualitatsmessung","title":"Qualit\u00e4tsmessung","text":"<p>Meistens wird die Qualit\u00e4t anhand folgender vier Kriterien bestimmt. Fitness: Ein Modell ist hier perfekt, wenn alle Abl\u00e4ufe aus dem Log von Anfang bis Ende durchlaufen k\u00f6nnen. Einfachheit: Das einfachste Modell, das das Verhalten aus dem log widerspiegeln kann, ist das beste Modell. Man versucht also ein Modell zu erstellen, das das Verhalten mit m\u00f6glichst wenigen Informationen widerspiegeln kann. Pr\u00e4zision: Ein Modell ist pr\u00e4zise wenn es nicht zu viel Verhalten erlaubt. Ein Modell, das nicht pr\u00e4zise ist underfitted. Schlechte Pr\u00e4zision w\u00fcrde Verhalten erlauben, das nicht im Event Log repr\u00e4sentiert war. Generalisierung: Ein Modell sollte generalisieren und sich nicht genau auf die Beispiele im Event Log begrenzen. Ein Modell, dass nicht generalisiert overfitted. Bei wenig Generalisierung w\u00fcrde das Modell nur genau das Verhalten aus dem Log repr\u00e4sentieren.</p>"},{"location":"Themen/t7_Process_Mining/#anwendungen","title":"Anwendungen","text":"<p>Anhand der Menge der Paper, die zu den einzelnen Themen ver\u00f6ffentlicht wurden, kristallisieren sich einige Anwendungsgebiete heraus, die besonders vertreten sind: - Das Gesundheitswesen mit beispielsweise klinischen Pfaden, Patientenbehandlungen, oder der Analyse der prim\u00e4ren Prozesse eines Krankenhauses. Ein exaktes Beispiel w\u00e4re die Analyse der Chancen einer bestimmten Behandlung oder Korrelationen zwischen verschiedenen angewandten Behandlungen. Eine Quelle sagt sogar, dass Process Mining eine von den Anwendungen ist, die die gr\u00f6\u00dften Vorteile aus den vorhandenen Daten im Gesundheitswesen ziehen kann. - Informations- und Kommunikationstechnologien: bei Software Developement, in IT-Betriebsdienstleistungen und Telekommunikationsunternehmen, wenn zum Beispiel Engp\u00e4sse identifiziert werden sollen. - In der Fertigung: Bei industriellen Aktivit\u00e4ten, die von Fabriken durchgef\u00fchrt werden und Produkte liefern, v.a. vertreten im Automobilsegment - In der Bildung: Beim E-Learning, bei wissenschaftliche Anwendungen und in Forschungszentren mit Innovationsprozessmanagement. Zum Beispiel beim Entwickeln produktiver Lernpfade, basierend auf bestimmten Nutzergruppen. - In den Finanzen: in Prozessen wie Bezahlung, Investitionen, Einlagen oder bei Risikoanalyse und -minderung - In der Logistik: Bei Transport, Lagerung und Bestandsmanagement</p>"},{"location":"Themen/t7_Process_Mining/#trends-ki","title":"Trends \u2013 KI","text":"<p> Process Mining allein hat nichts mit K\u00fcnstlicher Intelligenz zu tun, denn KI ist f\u00fcr die angestrebten Resultate \u2013 Transparenz, Auswertungen zur Prozessleistung und Identifikation von Abweichungen \u2013 eigentlich nicht notwendig. Im Bereich der analytischen Methodik unterscheidet man vier aufeinander aufbauenden Ebenen (s. Abb. 9). In der deskriptiven Analytik ist Process Mining alleine aktiv, aber f\u00fcr die darauffolgenden Ebenen hat der Einsatz von KI gro\u00dfes Potential. Um zu erkl\u00e4ren, warum etwas passiert ist \u21d2 diagnostisch; was passieren wird, also Vorhersagen \u21d2 Pr\u00e4diktive Analytik; Und schlussendlich sogar um Handlungsempfehlungen auszusprechen, also wie muss gehandelt werden, damit ein zuk\u00fcnftiges Ereignis eintritt oder eben nicht eintritt \u21d2 Pr\u00e4skriptive Analytik. Anhand der identifizierten Problemursachen k\u00f6nnen Entscheider gezielte und effektive Verbesserungsma\u00dfnahmen umsetzen. K\u00fcnstliche Intelligenz ist also kein zwingend notwendiges Kriterium, kann aber das Potential vergr\u00f6\u00dfern. KI wird also zunehmend eingesetzt um die Potentiale der Process Mining Anwendungen zu erweitern. KI wird zum Beispiel integriert, um einzelne Teilbereiche und Arbeitsschritte des Process Mining zu unterst\u00fctzen und bessere Ergebnisse zu erzielen.  Ein Beispiel w\u00e4re eine auf KI basierte Technologie, die in der Lage ist Arbeitsabl\u00e4ufe zu verstehen und daraus folgend Schlussfolgerungen abzuleiten. Auf diese Weise erweitern die Anwendungen der KI das klassische Process Mining von einem explorativen Ansatz hin zu zunehmend intelligenter und vollst\u00e4ndig automatisierter Prozessanalyse. Der entscheidende Unterschied zum klassischen Process Mining besteht darin, dass sich das System kontinuierlich mithilfe von KI automatisch optimiert und sich den st\u00e4ndig \u00e4ndernden Rahmenbedingungen anpasst. Ein anderes Beispiel w\u00e4re Maschinelles Lernen zur Verbesserung der Datenqualit\u00e4t, sowie zur Gruppierung und Strukturierung der Daten. Es w\u00fcrde also im Bereich transform von ETL eingesetzt werden, den wir zuvor schon bei dem Sammeln der Daten gesehen haben. KI hilft also mit den ganzen Problemen umzugehen, die in dem Kapitel Algorithmen ausf\u00fchrlich angesprochen wurden. Da die Qualit\u00e4t der Daten sehr ausschlaggebend f\u00fcr das Endergebnis ist, w\u00fcrde eine Verbesserung der Daten die Qualit\u00e4t des Prozess Modells sehr steigern. Maschinelles Lernen kann hierbei auch eingesetzt werden, um die Dateneingabe zu automatisieren, Duplikate zu erkennen und zu entfernen und um unstrukturierte Daten in Formate umzuwandeln, die von Process-Mining Anwendungen gelesen werden k\u00f6nnen.</p>"},{"location":"Themen/t7_Process_Mining/#code-demo-process-mining","title":"Code Demo Process Mining","text":""},{"location":"Themen/t7_Process_Mining/#von-quirin-joshua-groszeibl","title":"von Quirin Joshua Groszeibl","text":"<pre><code>import os\nimport pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"Themen/t7_Process_Mining/#daten-laden","title":"Daten laden","text":"<p>Zuerst brauchen wir Daten, die wir verwerten k\u00f6nnen. In diesem Fall laden wir dazu eine CSV-Datei hoch. Das Beispiel hierbei ist ein Datensatz von Kaggle mit \u00fcber 27k Reihen Eventlogs einer Autoversicherungsfirma, diese begrenzen wir f\u00fcr diese Demo auf 200 Reihen, damit das Kompilieren nicht allzu lange dauert. <pre><code># Load the event log as a DataFrame\nevent_log = pd.read_csv('/home/qsh1ne/CODE_DEMO_SEM_KI_23/Insurance_claims_event_log.csv',\n                        nrows=200, usecols=['case_id', 'activity_name', 'timestamp'])\n</code></pre></p>"},{"location":"Themen/t7_Process_Mining/#process-discovery-daten-verstehen","title":"Process Discovery - Daten verstehen","text":"<p>Damit wir eine Fehlerfreie Analyse durchf\u00fchren k\u00f6nnen, ben\u00f6tigen wir zuerst ein Verst\u00e4ndnis f\u00fcr unsere Daten. Eine Frequenzanalyse hilft uns direkte Zusammenh\u00e4nge zwischen den Variablen besser zu verstehen. F\u00fcr die sp\u00e4tere Visualisierung erstellen wir anschlie\u00dfend einen gerichteten Graph, dessen Knoten wir mit den Subprozessen der Case ID best\u00fccken. <pre><code># Perform frequency analysis to get the directly-follows relations\ndfg = event_log.groupby(['case_id', event_log['activity_name'].shift(-1)]).size().reset_index(name='count')\n\n# Create a directed graph\ngraph = nx.DiGraph()\n\n# Add nodes and edges to the graph\nfor _, row in dfg.iterrows():\n    graph.add_edge(row['activity_name'], row['activity_name'], weight=row['count'])\n\n# Visualize the graph\npos = nx.spring_layout(graph, seed=42)\nlabels = nx.get_edge_attributes(graph, 'weight')\nweights = [graph[u][v]['weight'] / 10 for u, v in graph.edges()]\nnx.draw_networkx(graph, pos, with_labels=True, node_size=500, node_color='lightblue',\n                 edge_color='gray', width=weights, font_size=8)\nnx.draw_networkx_edge_labels(graph, pos, edge_labels=labels, font_size=6)\n\nAnschlie\u00dfend lassen wir uns den Graphen als Bild ausgeben.\n\n# Save the process graph as an image file\noutput_dir = '/home/qsh1ne/PM_Code_Demo/output'\nos.makedirs(output_dir, exist_ok=True)\nprocess_graph_file = os.path.join(output_dir, 'process_graph.png')\nplt.savefig(process_graph_file)\n\nprint(f\"Process graph saved as {process_graph_file}\")\n</code></pre> Beispiel: Die Event Logs der Versicherungsfirma beziehen sich auf Cases, welche jeweils in 6 Schritten aufgespalten werden, wie auf der Abbildung unten zu sehen. Anhand der gleichm\u00e4\u00dfigen Gr\u00f6\u00dfe aller Subprozesse, l\u00e4sst sich darauf schlie\u00dfen, dass diese alle in etwa gleich oft vorkommen.</p>"},{"location":"Themen/t7_Process_Mining/#conformance-checking-prozess-spektrum-errechnen","title":"Conformance Checking - Prozess Spektrum errechnen","text":"<p>Das Errechnen des Prozess Spektrums ganz einfach \u00fcber das Z\u00e4hlen der einzelnen Aktivit\u00e4ten in der Anzahl. Jeder Subprozess muss einen sehr \u00e4hnlichen Ablauf haben, um mit dem PSM gute Vorhersagen treffen zu k\u00f6nnen. Beispielsweise eignen sich Maschinendaten oder B\u00fcrokratische Abl\u00e4ufe sehr gut, w\u00e4hrend Daten von Sportwettk\u00e4mpfen hierbei weniger gute Ergebnisse liefern.</p> <p>Es gilt: Je \u00e4hnlicher die Prozesse, desto einfach die Auswertung. </p> <p>Auch hier visualisieren wir die Ergebnisse zuerst und speichern diese anschlie\u00dfend als Bild ab. <pre><code># Calculate process spectrum\nspectrum = event_log['activity_name'].value_counts().sort_values(ascending=False)\n\n# Plot the process spectrum\nplt.figure()\nspectrum.plot(kind='bar', color='lightblue')\nplt.xlabel('Activity')\nplt.ylabel('Frequency')\nplt.title('Process Spectrum')\n\n# Save the process spectrum plot as an image file\nspectrum_plot_file = os.path.join(output_dir, 'process_spectrum.png')\nplt.savefig(spectrum_plot_file)\n\nprint(f\"Process spectrum saved as {spectrum_plot_file}\")\n</code></pre></p> <p>Beispiel: Hierbei f\u00e4llt auch schon der erste Fehler auf, die ersten 200 Reihen anstelle der gesamten 27k Reihen, verf\u00e4lscht in diesen Fall aus 2 Gr\u00fcnden die Ergebnisse. Zum einen ist 200%6 =! 0, dementsprechend fehlen Subprozesse des letzten eigentlichen Prozesses, aber auch betrachten wir weniger als 0,007% der Datenmenge. So k\u00f6nnen unm\u00f6glich schlagkr\u00e4ftige Ergebnisse erzielt werden, aber f\u00fcr den Zweck dieser kleinen Demonstration ist es gen\u00fcgend.</p>"},{"location":"Themen/t7_Process_Mining/#process-enchancement-performance-spektrum-errechnen","title":"Process Enchancement - Performance Spektrum errechnen","text":"<p>Die ersten komplexeren Berechnungen nimmt uns hierbei die Pandas Libary ab, in dem diese die Eventlogs in zuerst in eine einheitliches und auswertbares Zeitformat umformt. Wir setzen die Startzeiten auf die geringste Dauer, und die Endzeiten auf die h\u00f6chste der jeweiligen Cases. Anschlie\u00dfend subtrahieren wir Startzeit von der Endzeit, und uns bleibt eine durchschnittliche Zeit f\u00fcr die Dauer der jeweiligen Aktivit\u00e4ten.</p> <p>Anschlie\u00dfend visualisieren wir unsere Ergebnisse wieder, und geben diese als Bild aus. <pre><code># Calculate case durations\nevent_log['timestamp'] = pd.to_datetime(event_log['timestamp'])\nstart_times = event_log.groupby('case_id')['timestamp'].min()\nend_times = event_log.groupby('case_id')['timestamp'].max()\ncase_durations = end_times - start_times\n\n# Plot the performance spectrum\nplt.figure()\ncase_durations.dt.total_seconds().plot(kind='bar', color='lightblue')\nplt.xlabel('Case ID')\nplt.ylabel('Duration (seconds)')\nplt.title('Performance Spectrum')\n\n# Save the performance spectrum plot as an image file\nperformance_spectrum_file = os.path.join(output_dir, 'performance_spectrum.png')\nplt.savefig(performance_spectrum_file)\n\nprint(f\"Performance spectrum saved as {performance_spectrum_file}\")\n</code></pre> Auch hierbei ist die Verf\u00e4lschung deutlich zu erkennen, die einzelnen Subprozesse der Prozesse wurden unter der gemeinsamen Case ID zusammengefasst. Da die Dauer dieser Prozesse stark variiert, und der k\u00fcrzeste Teil des Prozesse nur wenige Sekunden dauert, ist auch mit dem Ergebnisse wenig anzufangen.</p> <p>Neben den hier verwendeten Libarys sind folgende erw\u00e4hnenswert, allerdings dauert die Implementation wesentlich l\u00e4nger und diese Demo ist zeitlich sehr beschr\u00e4nkt.</p> <ul> <li>pm4py</li> <li>pmi</li> </ul>"},{"location":"Themen/t7_Process_Mining/#prom-69","title":"ProM 6.9","text":"<p>Die ProM Plattform ist ein plugin-basierendes Framework speziell f\u00fcr Process Mining. Zentriert auf drei Basiskonzepten. </p> <ul> <li>Data Objects (1) im Workspace Tab (2)</li> <li>Plugins (3)</li> <li>Visualisierung (4)</li> </ul> <p></p>"},{"location":"Themen/t7_Process_Mining/#workspace","title":"Workspace","text":"<p>Ansicht (5) von allen Objekten, den Favorisierten Objekten und Importierten Objekten.</p> <p>Objekte importierbar \u00fcber den Import Button (6) oder via drag&amp;drop.</p> <p>Im Workspace kann au\u00dferdem: - (7) favorisiert (Stern), angeschaut (Auge), Plugin gestartet (Play) oder gel\u00f6scht (Kreuz), - (8) umbenannt, - (9) exportiert werden.</p> <p></p>"},{"location":"Themen/t7_Process_Mining/#praktische-anwendung","title":"Praktische Anwendung","text":"<p>Die Code Demo ist ein wenig kurz an tats\u00e4chlichen Code gekommen, aber mir war es wichtiger einen sinnvollen Einblick in die praktische Arbeit mit Prozess Mining zu geben.</p> <p>Fangen wir nun mit der Umwandlung der CSV-Datei an. Zuallererst schauen wir uns dazu wie vorab beschrieben den Inhalt an f\u00fcr ein Verst\u00e4ndnis der Daten, und klassifizieren die Eingabevariablen.</p> <p></p>"},{"location":"Themen/t7_Process_Mining/#umwandlung-in-xes-format","title":"Umwandlung in XES-Format","text":"<p>Anschlie\u00dfend \u00fcbertrage wir die Variabel Klassifizierung und alle Spalten und Reihen der CSV-Datei in ein XES Format, dies machen wir mit dem zugeh\u00f6rigen Standard-Plugin von ProM 6.9.</p> <p></p>"},{"location":"Themen/t7_Process_Mining/#analyse","title":"Analyse","text":"<p>Die XES-Datei k\u00f6nnen wir problemlos in das Performance Spektrum Plugin einf\u00fcttern. </p> <p></p> <p>Das Feintuning der Parameter und das Setzen eines eigenen Classifier ist optimal, kann aber je nach dem Ziel des Anwenders stark die Ergebnisse berichtigen und verf\u00e4lschen. Die ersten zwei Ausgabebilder werten die Versicherungsdaten in Quantilen aus:</p> <p> </p> <p>hier die mediale Auswertung:</p> <p> </p>"},{"location":"Themen/t7_Process_Mining/#auswertung","title":"Auswertung","text":"<p>Zur Auswertung nehme ich den medialen Ansatz her, da er hierbei mehr Sinn ergibt. Man kann deutlich erkennen bei der 2. Abbildung, dass die ersten Prozesse die erste Benachrichtigung \u00fcber den Verlust sind, und die letzte das Abschlie\u00dfen durch abgeschlossene Bezahlung oder durch Entscheidung keine Zahlung zu senden ist. Ebenfalls zu sehen ist, dass die meisten sehr (Dunkelblau) und langsamen (Orange) sich innerhalb sehr spezieller Prozesse befinden, w\u00e4hrend die h\u00e4ufigsten Prozesse (First Notification) schon sehr gut optimiert sind und komplett normal im Durschnitt ablaufen. In der ersten Abbildung ist ebenfalls zu sehen, wie verschiedene Prozesse sich \u00fcberschneiden. Die Linien stellen jeweils einen Prozess von Beginn bis Ende da. Dies ist bei den oft vorkommenden Prozessen leider sehr un\u00fcbersichtlich, w\u00e4hrend es bei den weniger H\u00e4ufigen Prozessen zu wesentlich besser sichtbaren Resultaten f\u00fchrt.</p>"},{"location":"Themen/t7_Process_Mining/#verwendeter-datensatz","title":"Verwendeter Datensatz","text":"<ul> <li>https://www.kaggle.com/datasets/carlosalvite/car-insurance-claims-event-log-for-process-mining?resource=download</li> </ul>"},{"location":"Themen/t7_Process_Mining/#ergebnisse-berechnungen-via-python-code","title":"Ergebnisse Berechnungen via Python Code","text":""},{"location":"Themen/t7_Process_Mining/#weiterfuhrendes-material","title":"Weiterf\u00fchrendes Material","text":""},{"location":"Themen/t7_Process_Mining/#podcast","title":"Podcast","text":"<p>Hier Link zum Podcast.</p>"},{"location":"Themen/t7_Process_Mining/#talk","title":"Talk","text":"<p>Hier einfach Youtube oder THD System embedden.</p>"},{"location":"Themen/t7_Process_Mining/#demo","title":"Demo","text":"<ul> <li>https://github.com/qsh1ne/CODE_DEMO_SEM_KI_23</li> </ul>"},{"location":"Themen/t7_Process_Mining/#literaturverzeichnis","title":"Literaturverzeichnis","text":"<ol> <li>van der Aalst, Wil (2016): Process Mining - Data Science in Action, 2.Aufl., Berlin, Deutschland: Springer</li> <li>Macak, Martin/Daubner, Lukas/Fani Sani, Mohammadreza/Buhnova, Barbora (2022): Process mining usage in cybersecurity and software reliability analysis: A systematic literature review, in: Array, Volume 13</li> <li>dos Santos Garcia, Cleiton/ Meincheim, Alex/Ribeiro Faria Junior, Elio/ Rosano Dallagassa, Marcelo/Maria Vecino Sato, Denise/ Ribeiro Carvalho, Deborah/ Alves Portela Santos, Eduardo/Emilio Scalabrin/Edson (2019): Process mining techniques and applications - A systematic mapping study, in: Expert Systems with Applications, Volume 133, S. 260-295</li> <li>IBM Technology (2023): What is Process Mining? [YoutTube],  https://www.youtube.com/watch?v=5thuFbUQ7Qg</li> <li>Study Conquest (2018): Alpha Algorithm (Process Discovery Method)[YouTube], https://www.youtube.com/watch?v=nOTehxTiFFU</li> <li>Dager, Shirin (2020): Process Mining hat nichts mit KI zu tun! Erstmal..., der-prozessmanager, [online] https://der-prozessmanager.de/aktuell/news/zusammenhang-von-process-mining-und-ki [abgerufen am 27.05.2023]</li> <li>Zaharia, Silvia, Korth, Alexander (2022): Datennutzung im E-Commerce, in: Marketing Analytics, S. 215-228</li> <li>Barenkamp, Marco (2022): K\u00fcnstliche Intelligenz als Unterst\u00fctzungsfunktion der Vorhersage und Prozessexzellenz im Process Mining, in: Wirtschaftsinformatik&amp;Management, Nr. 14, S. 160-170</li> </ol>"},{"location":"Themen/transfer_learning_nlp/","title":"Transfer Learning in der Sprachverarbeitung","text":"<p>von Simon Wolf, Tim Staudinger und Miguel Meindl</p>"},{"location":"Themen/transfer_learning_nlp/#abstract","title":"Abstract","text":"<p>Die Sprachverarbeitung ist ein grundlegender Aspekt der k\u00fcnstlichen Intelligenz (KI) und hat in den letzten Jahren betr\u00e4chtliche Fortschritte erzielt. Eine vielversprechende Methode zur Verbesserung der Leistung von Sprachmodellen ist Transfer Learning. Dies erm\u00f6glicht es, vortrainierte Modelle auf eine neue Aufgabe anzuwenden, indem das bereits erlernte Wissen auf eine verwandte Aufgabe angewendet wird.</p> <p>Der Podcast bietet eine oberfl\u00e4chliche Einf\u00fchrung in das Thema, um fachfremde Zuh\u00f6rer mit den grundlegenden Konzepten vertraut zu machen. Es wird erkl\u00e4rt, wie Transfer Learning funktioniert und genutzt werden kann. Au\u00dferdem wird ein \u00dcberblick \u00fcber verschiedene Anwendungsf\u00e4lle gegeben und die Herausforderungen von Transfer Learning beleuchtet. Zudem wird der Einfluss der Datenmenge und Qualit\u00e4t auf das Ergebnis diskutiert.</p> <p>Der Fachvortrag liefert einen tieferen Einblick in das Thema. Hier wird zun\u00e4chst versucht, die verschiedenen Aspekte von Transfer Learning einzuordnen. Die Herausforderungen zur Vermeidung von negativem Transfer werden herausgearbeitet und potenzielle L\u00f6sungen aufgezeigt. Anschlie\u00dfend werden die verschiedenen Kategorien elaboriert, um einen \u00dcberblick zu geben. Modernere Ans\u00e4tze mit Hilfe von Deep Learning werden vorgestellt und deren Vorteile aufgezeigt. Anschlie\u00dfend wird der Ansatz des Modelltransfers n\u00e4her erl\u00e4utert.</p> <p>Im Schlussteil der Arbeit wird anhand eines konkreten Beispiels gezeigt, wie Transfer Learning angewandt werden kann. Hierf\u00fcr wird das Sprachmodell BERT auf den Anwendungsfall der Fake-News-Erkennung trainiert. Die einzelnen Phasen dieses Prozesses werden im Detail erl\u00e4utert und mit Codebeispielen veranschaulicht.</p>"},{"location":"Themen/transfer_learning_nlp/#1-einleitung-und-motivation","title":"1. Einleitung und Motivation","text":"<p>In der heutigen digitalen Zeit hat die Spracherkennung einen enormen Einfluss auf unser t\u00e4gliches Leben. Von virtuellen Assistenten \u00fcber Sprachbefehle in mobilen Ger\u00e4ten bis hin zu automatisierten Kundenservice-Systemen \u2013 die F\u00e4higkeit, menschliche Sprache zu verstehen und zu verarbeiten, hat zahlreiche Anwendungen revolutioniert. Doch die Entwicklung pr\u00e4ziser und effizienter Spracherkennungssysteme  stellt nach wie vor eine Herausforderung dar.</p> <p>Die Anwendungsm\u00f6glichkeiten von Transfer Learning in der Spracherkennung sind vielf\u00e4ltig. Egal ob es darum geht,  Sprachbefehle in Smart-Home-Ger\u00e4ten zu erkennen, Transkriptionen von Audioaufnahmen zu erstellen oder Sprachanrufe automatisch zu analysieren,  Transfer Learning bietet eine effektive Methode, um ma\u00dfgeschneiderte Modelle f\u00fcr spezifische Aufgaben zu entwickeln.</p> <p>Transfer Learning bietet in der Spracherkennung eine Reihe von Vorteilen. Anstatt Modelle von Grund auf neu zu trainieren kann auf bereits existierende  Modelle zur\u00fcckgegriffen werden. Dadurch wird nicht nur die Trainingszeit erheblich verk\u00fcrzt, sondern auch der Bedarf an umfangreichen Datenmengen reduziert.</p> <p>Des Weiteren erm\u00f6glicht es die Nutzung von vortrainiertem Wissen, das bereits in einem anderen Kontext erworben wurde.  Dieses Wissen kann auf die Spracherkennung angewendet werden, um eine bessere Anpassung an spezifische Aufgaben zu erreichen.  Dadurch wird die Genauigkeit der Spracherkennungssysteme verbessert, selbst wenn die verf\u00fcgbare Datenmenge begrenzt ist.</p> <p>Die Idee von Transfer Learning hat ihre Wurzeln in der K\u00fcnstlichen Intelligenz und dem maschinellen Lernen. In den letzten Jahrzehnten wurden  verschiedene Ans\u00e4tze und Techniken entwickelt, um Transfer Learning zu erm\u00f6glichen. Diese Fortschritte haben die Spracherkennung ma\u00dfgeblich beeinflusst.</p>"},{"location":"Themen/transfer_learning_nlp/#2-stand-der-forschung","title":"2. Stand der Forschung","text":"<p>Doch wie sieht der aktuelle Stand der Forschung aus? Welche neuen Ans\u00e4tze und Techniken wurden entwickelt, um die Effizienz und Genauigkeit von Spracherkennungssystemen weiter zu verbessern? Im nachfolgenden Abschnitt wird genauer auf diese Themen eingegangen.</p> <p>In den letzten Jahren hat die Forschung intensiv daran gearbeitet, Transfer Learning in der Spracherkennung voranzutreiben.  Ein herausragendes Beispiel f\u00fcr ein erfolgreiches vortrainiertes Sprachmodell ist ChatGPT. Durch den Einsatz von vortrainierten Modellen als Ausgangspunkt k\u00f6nnen Spracherkennungsmodelle von dem breiten Wissen profitieren, das in Modellen wie beispielsweise ChatGPT vorhanden ist.  Dies erm\u00f6glicht eine verbesserte Sprachverarbeitung. Durch die \u00dcbertragung des vortrainierten Wissens auf spezifische Spracherkennungsaufgaben k\u00f6nnen Modelle schneller und genauer lernen, wodurch die Genauigkeit der Spracherkennungssysteme erh\u00f6ht wird.</p> <p>Ein weiterer vielversprechender Ansatz ist die Kombination von Transfer Learning mit Active Learning.  Active Learning erm\u00f6glicht es, gezielt unsichere Beispiele auszuw\u00e4hlen, um das Modell iterativ zu trainieren. Durch den gezielten Einsatz von Transfer Learning  in Kombination mit Active Learning k\u00f6nnen Spracherkennungsmodelle schneller und effizienter lernen. Das Modell kann von bereits gelernten Aufgaben  profitieren und sich schneller an neue Spracherkennungsaufgaben anpassen.</p> <p>Dar\u00fcber hinaus wird als weiterer Ansatz Transfer Learning mit anderen Techniken wie beispielsweise Reinforcement Learning kombiniert.  Reinforcement Learning-Algorithmen k\u00f6nnen eingesetzt werden, um die Interaktion mit dem Spracherkennungssystem zu verbessern und es an spezifische Nutzerpr\u00e4ferenzen anzupassen. Eine weitere M\u00f6glichkeit stellt die Kombination mit Generative Adversarial Networks (GANs) dar. Ziel dieses Ansatzes ist es, die Datenmenge durch k\u00fcnstlich generierte Texte zu erh\u00f6hen. Diese Kombinationen er\u00f6ffnen neue M\u00f6glichkeiten, die Leistungsf\u00e4higkeit von Spracherkennungssystemen zu steigern.</p> <p>Ein weiterer vielversprechender Aspekt ist die Erweiterung von Transfer Learning auf mehrsprachige Szenarien.  Indem Modelle auf verschiedenen Sprachen trainiert und dann auf neue Sprachen \u00fcbertragen werden, kann die Effizienz und Genauigkeit  der Spracherkennung in verschiedenen Sprachen verbessert werden. Dies ist besonders relevant in globalen Umgebungen,  in denen mehrsprachige Unterst\u00fctzung von entscheidender Bedeutung ist.</p> <p>Der aktuelle Stand der Forschung im Bereich Transfer Learning in der Spracherkennung zeigt das Potenzial dieser Technik.  Durch den Einsatz von vortrainierten Sprachmodellen wie ChatGPT, kombiniert mit Active Learning, Reinforcement Learning und anderen Techniken,  k\u00f6nnen Spracherkennungssysteme effizienter, pr\u00e4ziser und anpassungsf\u00e4higer trainiert werden.</p>"},{"location":"Themen/transfer_learning_nlp/#3-methoden","title":"3. Methoden","text":""},{"location":"Themen/transfer_learning_nlp/#31-definition","title":"3.1 Definition","text":"<p>Die Motivation f\u00fcr Transfer Learning basiert auf der Idee des \"Lernens zu lernen\", die besagt, dass die F\u00e4higkeit zu Lernen von Grund auf oft begrenzt ist und daher so viel wie m\u00f6glich aus fr\u00fcheren Erfahrungen genutzt werden sollte.</p> Abbildung 1: Visualisierung von Transfer Learning: Die linke Seite stellt das Quellsystem dar, die rechte Seite das Zielsystem. X1 und X2 sind die Featurer\u00e4ume, Y1 und Y2 die Labelr\u00e4ume der jeweiligen Modelle M1 und M2. P1(X) und P2(X) sind die jeweiligen Wahrscheinlichkeitsverteilungen der Features. <p>Es gibt verschiedene Kategorien des Transfer Learnings, die je nach Beziehung zwischen dem bereits Gelernten (Quelle) und dem Neuen (Ziel) entwickelt wurden. Auf einige davon wird im sp\u00e4teren Verlauf im Kapitel \"Kategorisierung\" noch eingegangen. Es ist allerdings zu Erw\u00e4hnen, dass die Einteilung in diese Kategorien nicht immer eindeutig ist. Des Weiteren ist die Zugeh\u00f6rigkeit dieser zum Transfer Learning teilweise umstritten.</p>"},{"location":"Themen/transfer_learning_nlp/#32-herausforderungen","title":"3.2 Herausforderungen","text":"<p>Die mit Abstand gr\u00f6\u00dfte Herausforderung von Transfer Learning besteht im simplen Konzept:      Positiven Transfer erzeugen, negativen Transfer vermeiden.</p> <p>Die Vorhersagef\u00e4higkeit von Transfermethoden h\u00e4ngt von der semantischen \u00c4hnlichkeit zwischen den Aufgabenstellungen im Quell- und Zielsystem ab. Bei einer starken Beziehung und einer geeigneten Ausnutzung durch die Transfermethode kann die Vorhersagekraft in der Zielaufgabe deutlich verbessert werden. Ist die Beziehung zwischen den Aufgaben jedoch unzureichend oder wird sie von der Transfermethode nicht optimal genutzt, kann die Leistung abnehmen.</p> <p>Um negativen Transfer zu vermeiden, m\u00fcssen Transfermethoden vorsichtig sein und die Beziehung zwischen Quell- und Zielaufgabe ber\u00fccksichtigen. Vorsichtige Ans\u00e4tze f\u00fchren m\u00f6glicherweise zu geringerem positivem Transfer, bieten jedoch Schutz vor negativem Transfer. Aggressive Ans\u00e4tze erzielen m\u00f6glicherweise gr\u00f6\u00dfere Leistungssteigerungen, bergen jedoch auch das Risiko von negativem Transfer, wenn die Quellaufgabe nicht gut zur Zielaufgabe passt.</p>"},{"location":"Themen/transfer_learning_nlp/#321-ablehnung-schlechter-informationen","title":"3.2.1 Ablehnung schlechter Informationen","text":"<p>Eine M\u00f6glichkeit negativen Transfer zu vermeiden besteht darin, dass sch\u00e4dliche Informationen der Quellaufgabe w\u00e4hrend des Lernens der Zielaufgabe erkannt und abgelehnt werden. Eine Methode, um dies zu erreichen, ist das optionenbasierte Transfer Learning im Bereich des Reinforcement Learning, bei dem der Agent basierend auf der Leistung bestimmte Optionen ausw\u00e4hlt oder ablehnt. Ein weiterer Ansatz ist der KBKR-Ratschlag-Algorithmus (Advice-Taking Algorithm), der die Ratschl\u00e4ge der Quellaufgabe als weiche Einschr\u00e4nkung ber\u00fccksichtigt. Zus\u00e4tzlich wurden Methoden zur Erkennung von negativem Transfer entwickelt, z.B. durch die Verwendung eines Hyperpriors, dessen Varianz mit der Un\u00e4hnlichkeit der Aufgaben korreliert. Dadurch kann entschieden werden, ob \u00fcberhaupt ein Transfer stattfinden sollte.</p>"},{"location":"Themen/transfer_learning_nlp/#322-auswahl-der-quellaufgabe","title":"3.2.2 Auswahl der Quellaufgabe","text":"<p>Um negativen Transfer zu vermeiden, k\u00f6nnen mehrere Quellaufgaben zur Auswahl stehen. Eine M\u00f6glichkeit besteht darin, die Aufgaben nach Schwierigkeitsgrad zu ordnen und eine Quellaufgabe auszuw\u00e4hlen, die nur moderat schwieriger ist als die Zielaufgabe. Eine andere Methode ist die Suche nach \u00e4hnlichen Aufgaben mit Hilfe von Graphenrepr\u00e4sentationen. Zudem kann auch die Auswahl aus Kandidatenl\u00f6sungen einer Quellaufgabe anstelle von Quellaufgaben selbst in Betracht gezogen werden. Dieser Ansatz erm\u00f6glicht die Ber\u00fccksichtigung der Komplexit\u00e4t der Modelle und die Auswahl einer geeigneten Aufl\u00f6sung f\u00fcr den Transfer.</p> Abbildung 2: Visualisierung der Auswahl der Quellaufgabe: Eine M\u00f6glichkeit, negativen Transfer zu vermeiden, besteht darin, eine geeignete Quellaufgabe auszuw\u00e4hlen, von der der Transfer erfolgen soll. In diesem Beispiel wird Task 2 als am Relevantesten ausgew\u00e4hlt."},{"location":"Themen/transfer_learning_nlp/#323-modellierung-von-aufgabenahnlichkeit","title":"3.2.3 Modellierung von Aufgaben\u00e4hnlichkeit","text":"<p>Bei der Auswahl von Quellaufgaben kann es vorteilhaft sein, mehrere Aufgaben zu ber\u00fccksichtigen, anstatt nur eine auszuw\u00e4hlen. Einige Ans\u00e4tze modellieren explizit die Beziehungen zwischen den Aufgaben und integrieren diese Informationen in die Transfermethode. Dies erm\u00f6glicht eine bessere Nutzung des Wissens aus den Quellaufgaben und verringert das Risiko von negativem Transfer. Beispiele f\u00fcr solche Ans\u00e4tze sind die Entwicklung von \u00c4hnlichkeitsma\u00dfen f\u00fcr Aufgaben im Bereich des Reinforcement Learning, die Konstruktion eines Graphen zur Darstellung der Aufgaben und die Verwendung von Kernel-Methoden zur Berechnung eines \u00c4hnlichkeitskerns f\u00fcr die Zielaufgabe.</p> Abbildung 3: Modellierung von Aufgaben\u00e4hnlichkeit: Eine andere M\u00f6glichkeit, negativen Transfer zu vermeiden, besteht darin, das Verh\u00e4ltnis zwischen den Quellaufgaben und der Zielaufgabe zu modellieren und das Wissen unter Ber\u00fccksichtigung dieser Beziehungen zu kombinieren."},{"location":"Themen/transfer_learning_nlp/#33-kategorisierung","title":"3.3 Kategorisierung","text":"<p>Die folgende Tabelle gibt einen \u00dcberblick \u00fcber die Gebiete des Transferlernens.</p> Abbildung 4: \u00dcberblickstabelle \u00fcber die Kategorien von Transfer Learning: Variable Beschreibung X: Feature space (Audio-, Text-, Bilddaten, \u2026) y: Label space (Phoneme, Kategorien, \u2026) M(X): Model P(X): Verteilung der Features +: Daten und Tasks sind gleich f\u00fcr Quell- und Zieldom\u00e4ne -: Daten und Tasks sind unterschiedlich f\u00fcr Quell- und Zieldom\u00e4ne"},{"location":"Themen/transfer_learning_nlp/#331-modelladaption-und-inkrementelles-lernen","title":"3.3.1 Modelladaption und inkrementelles Lernen","text":"<p>Die einfachste Art des Transfer Learnings ist die Modelladaption. Hier bleiben das Model und die Label- und Featurer\u00e4ume gleich, wobei das vorhandene Modell an die ver\u00e4nderte Datenverteilung angepasst wird. Es gibt verschiedene Ans\u00e4tze f\u00fcr die Modellanpassung, wie die Maximum-a-posteriori-Sch\u00e4tzung (MAP) und den Maximum-Likelihood-Lineare-Regression (MLLR) Algorithmus. Falls sich die Verteilung stetig \u00e4ndert, spricht man von inkrementellem Lernen. Die Anpassung kann supervised oder unsupervised erfolgen. Falls das Quellmodell allerdings erst die Label generieren muss, spricht man von semi-supervised Learning. Eine alternative Herangehensweise mit ungelabelten Daten umzugehen, besteht darin, neue Merkmale zu extrahieren, indem Daten aus Quell- und Ziel- Dom\u00e4nen linear abgeleitet werden. Dies kann mit Hilfe von Techniken wie der transfer component analysis (TCA) erreicht werden. In einigen F\u00e4llen k\u00f6nnen ungelabelte Daten verwendet werden, um robustere Merkmale abzuleiten. Dieser Ansatz wird als self-taught learning bezeichnet und \u00e4hnelt dem Konzept des tiefen Repr\u00e4sentationslernens.</p> Abbildung 5: Beispiel f\u00fcr die A-priori-Verteilung, die Likelihood-Funktion der Daten sowie die A-posteriori-Verteilung."},{"location":"Themen/transfer_learning_nlp/#332-heterogenes-transfer-learning","title":"3.3.2 Heterogenes Transfer Learning","text":"<p>Heterogenes Transfer Learning bezieht sich auf den Fall, in dem sich die Merkmale der Quell- und Ziel-Dom\u00e4nen unterscheiden, w\u00e4hrend die Labels und das Modell unver\u00e4ndert bleiben. Das Ziel besteht darin, die vorhandene Entsprechung zwischen den Dom\u00e4nen zu nutzen, um Wissen von einer Dom\u00e4ne auf die andere zu \u00fcbertragen. Fr\u00fchere Ans\u00e4tze konzentrierten sich auf die Definition und Nutzung der Entsprechung auf Instanzebene. Aktuellere Ans\u00e4tze zielen darauf ab, gemeinsame Repr\u00e4sentationen der Quell- und Ziel-Dom\u00e4nen zu finden, entweder durch Matrixfaktorisierung, RBM-basiertes latentes Faktorlernen oder durch die Kombination von Deep Learning und Transfer Learning. Eine besondere Herausforderung besteht darin, aus sehr unterschiedlichen Aufgaben zu lernen, bei denen sich der Labelraum von der Ziel-Dom\u00e4ne unterscheidet. Das Lernen von Korrespondenzen zwischen solchen unabh\u00e4ngigen, aber analogen Dom\u00e4nen ist f\u00fcr Maschinen schwierig, obwohl Menschen dazu neigen, Analogien leichter zu erkennen. Aktuelle Fortschritte im Bereich des Deep Learning bieten jedoch neue M\u00f6glichkeiten durch ein einheitliches Framework f\u00fcr Representation Learning und Multitask Learning.</p>"},{"location":"Themen/transfer_learning_nlp/#multitask-learning","title":"Multitask-Learning","text":"Abbildung 6: Grafische Darstellung von Multitask Learning <p>Multitask Learning bezieht sich auf den Fall, in dem die Merkmalsr\u00e4ume der Quell- und Ziel-Dom\u00e4nen identisch, jedoch die Aufgabenlabels signifikant unterschiedlich sind. Bei diesem Ansatz wird angenommen, dass die Quell- und Ziel-Aufgaben eng miteinander verbunden sind und das Lernen einer Aufgabe das Lernen der anderen Aufgabe in Form einer gegenseitigen Regularisierung unterst\u00fctzt. Multitask Learning ist ein allgemeiner Ansatz, der auf verschiedene Modelle angewendet werden kann, einschlie\u00dflich Kernel-Regression und k-nearest neighbor. Die Bewertung der Relevanz von zwei Aufgaben ist eine Herausforderung, und es gibt interessante Ans\u00e4tze, die die \u00dcberlappung verschiedener Aufgaben im selben semantischen Raum zur Sch\u00e4tzung der Relevanz verwenden.</p> <p>Eine M\u00f6glichkeit eine solche Sch\u00e4tzung durchzuf\u00fchren ist es einen Score f\u00fcr die \u00dcberlappung der semantischen R\u00e4ume zu definieren:</p> Abbildung 7: Definition eines \u00dcberlappungsscores der semantischen R\u00e4ume bei Multitask-Learning <p>Der hier definierte Score weist einen Wert zwischen 0 und 1 auf. Eine starke \u00dcberlappung weist einen Wert nahe 1 auf.</p>"},{"location":"Themen/transfer_learning_nlp/#34-deep-transfer-learning","title":"3.4 Deep Transfer Learning","text":"<p>Deep Learning hat einen starken Einfluss auf Transfer Learning, insbesondere in den Bereichen der gesprochenen und geschriebenen Sprache. Es umfasst verschiedene Modelle wie Deep Belief Networks, Deep Boltzmann Machines, Deep Autoencoders, Deep Neural Networks und Deep Recurrent Neural Networks. Diese Modelle sind in der Lage, mehrschichtige Repr\u00e4sentationen zu lernen, die eine hierarchische Verarbeitung von Informationen nachahmen. Das mehrschichtige Feature-Lernen bietet mehrere Vorteile, wie Robustheit gegen\u00fcber Datenvariationen, hierarchische Parameterverteilung, die M\u00f6glichkeit des Supervised Learnings und die Anpassungsf\u00e4higkeit an spezifische Aufgaben durch feinabstimmendes Training. Dadurch bietet Deep Learning einen geeigneten Rahmen f\u00fcr das Transfer Learning, bei dem robuste Features gelernt werden, die von mehreren Merkmalen und Aufgaben gemeinsam genutzt werden.</p> <p>In einer beispielhaften Umsetzung von Deep Transfer Learning werden bei einem gro\u00dfen Modell die meisten Schichten bis zu einem gewissen Punkt eingefroren. Die Restlichen werden anschlie\u00dfend neu trainiert. In folgender Visualisierung wird dies illustriert:</p> Abbildung 8: Visualisierung von Deep Transfer Learning mit gefrorenen Schichten. <p>Folgende Abbildung zeigt eine Transfer Learning Architektur, die auf tiefer Repr\u00e4sentation basiert. Im linken Teil der Abbildung findet das gemeinsame Training statt, bei dem unterschiedliche Eingabemerkmale durch Vorverarbeitungsnetzwerke in einen gemeinsamen semantischen Raum projiziert werden. Die gemeinsamen Merkmale umfassen aussagekr\u00e4ftige Faktoren, die f\u00fcr mehrere Aufgaben verwendet werden k\u00f6nnen. Alleinstehend handelt es sich bei der linken Seite der Abbildung im Wesentlichen um ein Multitask-Learning.</p> <p>Im rechten Teil der Abbildung wird die Anpassungsphase dargestellt, in der neue Daten f\u00fcr die Zielaufgabe bereitgestellt werden, entweder mit oder ohne Labels. Das Modell wird mit den neuen Daten aktualisiert, die einer anderen Verteilung folgen als in der gemeinsamen Trainingsphase.</p> Abbildung 9: Visualisierung von Deep Transfer Learning <p>Ein gro\u00dfer Vorteil des Unsupervised Trainings ist die F\u00e4higkeit, den Merkmalsextraktor ohne gelabelte Daten zu trainieren und somit den Bedarf an gelabelten Daten zu reduzieren. Durch Unsupervised Learning kann die \u00fcberwachte Lernphase verbessert werden, indem Konvergenzgeschwindigkeit, Datenmenge und Modellqualit\u00e4t beeinflusst werden.</p> <p>In der Studie \"Domain adaptation for large-scale sentiment classi\ufb01cation: A deep learning approach\" von X. Glorot et al. wurden hochrangige Merkmale mittels Unsupervised Learning extrahiert. Die Ergebnisse zeigten, dass diese abstrakten Merkmale dom\u00e4nenunabh\u00e4ngig sind und erfolgreich auf neue Dom\u00e4nen \u00fcbertragen werden k\u00f6nnen, ohne Anpassungen vorzunehmen. \u00c4hnliche Ergebnisse wurden auch in anderen Studien erzielt, z.B. bei der \u00dcbertragung von CNN-basierten Merkmalen auf Bilderkennungsaufgaben. Es wurde gezeigt, dass nur wenige gelabelte Daten ausreichen, um Modelle anzupassen und unbekannte Objekte zu erkennen. In einigen F\u00e4llen kann sogar die Beziehung zwischen Eingangsdaten, Aufgabenvektor und Aufgabenlabels in einem Deep-Network erlernt werden, was zu Zero-Data Learning und Zero-Shot Learning f\u00fchrt.</p>"},{"location":"Themen/transfer_learning_nlp/#35-modelltransfer","title":"3.5 Modelltransfer","text":"<p>Beim Transfer von Wissen zwischen Modellen gibt es verschiedene Ans\u00e4tze. Ein h\u00e4ufig verwendetes Verfahren ist das Modelltransferverfahren, bei dem das im Quellmodell gelernte Wissen auf das Zielmodell \u00fcbertragen wird. Dabei kann das Quellmodell beispielsweise ein Gaussian mixture model (GMM) sein, w\u00e4hrend das Zielmodell ein Deep Neural Network (DNN) ist. Das Wissen wird durch Initialisierung und Anpassung des Zielmodells mithilfe des GMM genutzt.</p> <p>Ein weiterer Ansatz ist das Lehrer-Sch\u00fcler-Modell, bei dem ein neues Modell von einem bestehenden Modell lernt. Das Lehrermodell enth\u00e4lt bereits reichhaltiges Wissen, das zur Anleitung des Sch\u00fclermodells genutzt wird. Es gibt verschiedene Methoden, um das Wissen des Lehrermodells auf das Sch\u00fclermodell zu \u00fcbertragen.</p> <p>Eine M\u00f6glichkeit ist das Abgleichen der Aktivierungen (Logit matching) des Sch\u00fclermodells mit denen des Lehrermodells. Dabei werden die Logits verglichen, um eine m\u00f6glichst geringe quadratische Abweichung zu erzielen. Eine andere Methode ist die Verwendung von \"dark knowledge\", bei der die Ausgaben des Sch\u00fclermodells an die Ausgaben des Lehrermodells angepasst werden. Im Folgenden wird erkl\u00e4rt, wie hier die loss function errechnet wird.</p> <p>Die Loss Funktion des Sch\u00fclermodells wird wie folgt errechnet:</p> Abbildung 10: Formel zur Errechnung der Loss Funktion des Sch\u00fclermodells mit Hilfe von Dark Knowledge Destilation Abbildung 11: Roadmap zum besseren Verst\u00e4ndnis der Formel Variable Beschreibung L: Loss function wird errechnet aus Sch\u00fcler- und Lehrer- loss function T: \u201cTemperatur Parameter\u201d Wenn T=1: Softmax-Funktion, mit zunehmendem T wird die Wahrscheinlichkeitsverteilung, die von der Softmax-Funktion generiert wird, weicher und liefert mehr Informationen dar\u00fcber, welche Klassen das Lehrermodell als \u00e4hnlicher zur vorhergesagten Klasse betrachtet hat. H: Cross Entropy loss function W: Child model Parameter zs und zt: Logits von Lehrer- und Sch\u00fclermodell \u03c4, \u03b1 und \u03b2: Hyperparameter \u03c3: Softmax Funktion x: Input y: Wahres Label"},{"location":"Themen/transfer_learning_nlp/#4-anwendungen","title":"4. Anwendungen","text":"<p>Im Folgenden wird anhand eines Beispiels die Vorgehensweise bei der Implementierung von Transfer Learning in der Sprachverarbeitung erl\u00e4utert. Konkret geht es hierbei um die Implementierung einer Fake-News-Erkennung. </p> <p>Zun\u00e4chst ein kurzer \u00dcberblick \u00fcber die einzelnen Phasen, welche wir durchlaufen werden. In Phase 1 werden die Daten visualisiert und vorbereitet.  Phase 2 besch\u00e4ftigt sich mit dem Large Language Model BERT, welches wir f\u00fcr unseren Anwendungsfall fine-tunen wollen. Es wird auf die urspr\u00fcnglichen Anwendungsf\u00e4lle  eingegangen, wof\u00fcr das Modell einst trainiert wurde. Anschlie\u00dfend wird das Modell angepasst, damit es f\u00fcr die Fake-News-Erkennung verwendet werden kann.  Ein entscheidender Schritt im Transfer Learning ist das Einfrieren der einzelnen Schichten. Dies wird im Phase 4 erl\u00e4utert bevor in Phase 5 das Modell trainiert wird.  Zu guter Letzt muss das Modell noch evaluiert werden.</p> Abbildung 12: Verschiedene Phasen der Code Demo"},{"location":"Themen/transfer_learning_nlp/#41-daten-visualisierung","title":"4.1 Daten - Visualisierung","text":"<p>Beide Datens\u00e4tze bestehen aus folgenden Variablen: - title: Entspricht der Schlagzeile des Artikels. Diese Variable wird sp\u00e4ter zum Trainieren verwendet. - text: Enth\u00e4lt den gesamten Text des Artikels. - subject: Beschreibt, wo der Artikel ver\u00f6ffentlicht wurde. - date: Datum der Ver\u00f6ffentlichung</p> <p>Hierbei f\u00e4llt auf, dass keine Variable Auskunft dar\u00fcber gibt, ob der Artikel fake oder tats\u00e4chlich wahr ist. Bevor die Datens\u00e4tze zusammengef\u00fcgt werden,  muss zun\u00e4chst diese Variable generiert werden.</p> <pre><code>true_data['Target'] = ['True'] * len(true_data)\nfake_data['Target'] = ['Fake'] * len(fake_data)\n\ndata = pd.concat([true_data, fake_data]).sample(frac=1).reset_index(drop=True)\ndata['label'] = pd.get_dummies(data.Target)['Fake']\n</code></pre> <p>Als N\u00e4chstes wurde die Verteilung der Daten visualisiert, um sicherzustellen, dass ein ausbalancierter Datensatz vorliegt. Mit einer Verteilung von  52,3 % Fake- und 47,7 % Echt-Daten, ist dies der Fall. Die L\u00e4nge der Schlagzeilen wurde ebenfalls visualisiert. Diese wird sp\u00e4ter beim Tokenisieren noch eine Rolle spielen.</p> Abbildung 13: L\u00e4nge der \u00dcberschriften"},{"location":"Themen/transfer_learning_nlp/#42-daten-vorbereitung","title":"4.2 Daten - Vorbereitung","text":"<p>Wie bei jedem maschinellen Lernverfahren m\u00fcssen die Daten in Trainings-, Validierungs- und Testdaten aufgeteilt werden.  Da ein Modell mit Textdaten nicht arbeiten kann, m\u00fcssen diese zun\u00e4chst tokenisiert werden. Damit ist gemeint, dass die W\u00f6rter in sogenannte Tokens umgewandelt werden, welche wiederum numerische Repr\u00e4sentationen darstellen. </p> <p>Da wir das Sprachmodell BERT verwenden wollen, nehmen wir hierf\u00fcr BertTokenizerFast, welcher speziell f\u00fcr das Modell entwickelt wurde. Dieser verwendet den WordPiece-Algorithmus,  welcher auf der Idee basiert, h\u00e4ufig vorkommende Zeichenfolgen in einem Textkorpus zu identifizieren und sie zu einem neuen Wort zusammenzuf\u00fcgen. </p> <p>Zus\u00e4tzlich zum Text, welcher tokenisiert werden soll, sind folgende Parameter zu \u00fcbergeben: - max_length: Dieser Parameter definiert die maximale L\u00e4nge einer Sequence. Wenn wir uns die Grafik L\u00e4nge der \u00dcberschriften nochmals genauer ansehen f\u00e4llt auf,  dass die meisten Schlagzeilen unter 20 W\u00f6rter haben. Um nicht unn\u00f6tig gro\u00dfe Datenmenge verarbeiten zu m\u00fcssen, setzen wir die maximale L\u00e4nge der Sequenzen daher auf diesen Wert. - padding: Da unser Modell mit einer bestimmten Anzahl an Tokens rechnet, m\u00fcssen wir diesen Parameter auf true setzen. Dies sorgt daf\u00fcr, dass Schlagzeilen welche weniger als 20 W\u00f6rter enthalten, am Ende der Sequenz mit Nullen aufgef\u00fcllt werden.  - truncation: Es gibt allerdings auch Schlagzeilen mit mehr als 20 W\u00f6rtern. Wird dieser Parameter auf true gesetzt, so werden alle Sequenzen l\u00e4nger als der definierte  Wert bei max_length abgeschnitten.</p> <pre><code>MAX_LENGTH = 20\n\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\ntokens_train = tokenizer.batch_encode_plus(\n    train_text.tolist(),\n    max_length=MAX_LENGTH,\n    padding=True,\n    truncation=True\n)\n\ntokens_val = tokenizer.batch_encode_plus(\n    val_text.tolist(),\n    max_length=MAX_LENGTH,\n    padding=True,\n    truncation=True\n)\n\ntokens_test = tokenizer.batch_encode_plus(\n    test_text.tolist(),\n    max_length=MAX_LENGTH,\n    padding=True,\n    truncation=True\n)\n</code></pre> <p>Betrachten wir nun die Ausgabe des Tokenizers anhand eines Beispiels. Als R\u00fcckgabewert erhalten wir ein Dictionary mit drei key-value Paaren: - input_ids: Enth\u00e4lt die tokenisierten Sequenzen. Bei genauer Betrachtung f\u00e4llt auf, dass diese jeweils mit dem Wert 101 starten und  mit 102 enden. Das hat den Grund, dass diese Tokens keine W\u00f6rter darstellen, sondern dem Algorithmus den Anfang und das Ende einer  Sequenz signalisieren. Des Weiteren kann der zuvor beschriebene Effekt des padding bei der zweiten Sequenz beobachtet werden. Hier wurden  zwei Nullen an das Ende angef\u00fcgt, damit Sequenz 1 und 2 die gleiche L\u00e4nge haben. - token_type_ids: Wird beim Umgang mit Sequenzpaaren verwendet und gibt an welcher Token zu welchem Satz geh\u00f6rt. Dies ist f\u00fcr unseren Anwendungsfall jedoch nicht relevant. - attention_mask: Bin\u00e4re Sequenz, die angibt, welche Token vom Modell ber\u00fccksichtigt bzw. ignoriert werden sollen. Beispielsweise sollen die Eintr\u00e4ge welche bei Sequenz 2 durch padding hinzugef\u00fcgt wurden, nicht beachtet werden.</p> <pre><code>sample_data = [\"Build a fake news detection model.\",\n               \"Using a bert model.\"]\n\ntokenized_sample_data = tokenizer.batch_encode_plus(sample_data, padding=True)\nprint(tokenized_sample_data)\n</code></pre> <pre><code>{'input_ids': [[101, 3857, 1037, 8275, 2739, 10788, 2944, 1012, 102], [101, 2478, 1037, 14324, 2944, 1012, 102, 0, 0]],\n 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], \n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 0, 0]]\n }\n</code></pre> <p>Da wir nun wissen, welche Daten wir an das Modell \u00fcbergeben m\u00fcssen und wie diese aussehen, werden die Daten im n\u00e4chsten Schritt zu Tensoren konvertiert. Dies ist notwendig, da wir mit der PyTorch-Bibliothek arbeiten wollen und diese auf Tensoren als grundlegende Datenstruktur f\u00fcr Berechnungen aufbaut. Des Weiteren verwenden wir einen sogenannten data loader, welcher uns beim Laden und Verwalten der Daten behilflich ist und uns diese in Batches aufteilt.</p> <pre><code># Convert lists to tensors\ntrain_seq = torch.tensor(tokens_train['input_ids'])\ntrain_mask = torch.tensor(tokens_train['attention_mask'])\ntrain_y = torch.tensor(train_labels.tolist())\n\n\n# Crate data loader\nbatch_size = 32\ntrain_data = TensorDataset(train_seq, train_mask, train_y)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n</code></pre>"},{"location":"Themen/transfer_learning_nlp/#43-bert-bidirectional-encoder-representations-from-transformers","title":"4.3 BERT (Bidirectional Encoder Representations from Transformers)","text":"<p>Das bekannte Large Language Model Bert (Bidirectional Encoder Representations from Transformers) wird als Ausgangspunkt verwendet. Das Modell wurde 2018 von Google ver\u00f6ffentlicht und wurde mittlerweile f\u00fcr eine Vielzahl von NLP-Aufgaben eingesetzt. Urspr\u00fcnglich wurde das Modell auf einem gro\u00dfen Textkorpus trainiert,  welcher beispielsweise die gesamte Wikipedia (2.5 Milliarden W\u00f6rter) und den sogenannten BookCorpus (985 Millionen W\u00f6rter) enth\u00e4lt. Das Training wurde hierf\u00fcr in zwei Phasen aufgeteilt: - Masked Language Modeling: In dieser Phase wurden 15 % der W\u00f6rter zuf\u00e4llig maskiert. Die Aufgabe bestand nun darin, die maskierten W\u00f6rter basierend auf dem Kontext vorherzusagen.  Dabei lernte das Modell die Beziehungen zwischen W\u00f6rtern innerhalb von S\u00e4tzen. Nachfolgend befindet sich die beschriebene Funktion als ein ausf\u00fchrbares Codebeispiel. </p> <pre><code>unmasker = pipeline('fill-mask', model='bert-base-uncased')\n\ntext = \"I will need an [MASK] because it is raining.\"\nunmasker(text)\n</code></pre> <ul> <li>Next Sentence Prediction: Als n\u00e4chsten Schritt musste das Modell die Beziehungen zwischen S\u00e4tzen lernen. Hierf\u00fcr wurde die Aufgabe so umgewandelt, dass das Modell vorhersagen sollte,  ob zwei S\u00e4tze aufeinanderfolgen. Wie f\u00fcr die erste Phase wird auch hierf\u00fcr ein Codebeispiel zur Verf\u00fcgung gestellt. Das Modell soll vorhersagen, ob Satz 1 und 2 bzw. Satz 2 und 3  in einer Beziehung zueinander stehen.</li> </ul> <pre><code>model = BertForNextSentencePrediction.from_pretrained(\"bert-base-uncased\")\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\nsentences = [\"Elon Musk lives in California.\", \"You can't buy anything on sundays in germany.\",\n             \"You are not supposed to work on sundays in germany.\"]\n\nfor i in range(2):\n    inputs = tokenizer(sentences[i], sentences[i + 1], return_tensors=\"pt\")\n    outputs = model(**inputs)\n    prediction = torch.argmax(outputs.logits)\n\n    if prediction == 0:\n        print(\"The sentences belong together.\")\n    else:\n        print(\"The sentences do not belong together.\")\n</code></pre>"},{"location":"Themen/transfer_learning_nlp/#44-model","title":"4.4 Model","text":"<p>In Phase 3 erstellen wir ein Modell, welches die Architektur des BERT Modells als Grundlage verwendet. Um das Modell auf unseren Anwendungsfall anzupassen, f\u00fcgen wir weitere Schichten hinzu. So werden beispielsweise zwei Linear-Layers hinzugef\u00fcgt, um die Anzahl der Ausg\u00e4nge auf zwei (Fake/Wahr) zu reduzieren. Ebenfalls wird die Regularisierungstechnik Dropout angewandt, um Overfitting vorzubeugen.</p> <p>In der forward Funktion wird definiert, wie die Eingabe durch das Modell flie\u00dft und die Ausgabe berechnet wird.</p> <pre><code>class BERT_Arch(nn.Module):\n    def __init__(self, bert):\n        super(BERT_Arch, self).__init__()\n        self.bert = bert\n        self.dropout = nn.Dropout(0.1)\n        self.relu = nn.ReLU()\n        self.fc1 = nn.Linear(768, 512)\n        self.fc2 = nn.Linear(512, 2)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, sent_id, mask):\n        cls_hs = self.bert(sent_id, attention_mask=mask)['pooler_output']\n        x = self.fc1(cls_hs)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.softmax(x)\n        return x\n</code></pre>"},{"location":"Themen/transfer_learning_nlp/#45-einfrieren-der-schichten","title":"4.5 Einfrieren der Schichten","text":"<p>Beim Transfer Learning werden oft bestimmte Schichten des vortrainierten Modells eingefroren, um die Gewichte w\u00e4hrend des Trainings nicht zu aktualisieren. Dies wird aus folgenden Gr\u00fcnden gemacht: - Schutz des bereits gelernten Wissens - Reduzieren der Trainingszeit - Pr\u00e4vention vor Overfitting</p> <p>Besonders n\u00fctzlich ist dies, wenn die urspr\u00fcngliche Aufgabe \u00c4hnlichkeiten mit der neuen Aufgabe aufweist. Es gibt drei verschiedene Ans\u00e4tze welche verfolgt werden k\u00f6nnen. Im Folgenden werden diese stichpunktartig beschrieben. 1. Keine Schichten einfrieren:     1. Es wird das gesamte Modell trainiert    2. Gro\u00dfer Datensatz ben\u00f6tigt    3. Die urspr\u00fcngliche Aufgabe unterscheidet sich stark von der neuen Aufgabe</p> <ol> <li>Teilweises einfrieren der Schichten:</li> <li>Es werden nur die unteren Schichten eingefroren</li> <li>Die oberen Schichten werden trainiert</li> <li> <p>Mittelgro\u00dfer Datensatz notwendig</p> </li> <li> <p>Alle Schichten einfrieren:</p> </li> <li>Alle Schichte des vortrainierten Modells werden eingefroren</li> <li>Nur die aufgabenspezifische Schichten werden trainiert</li> <li>Kleiner Datensatz</li> <li>\u00c4hnlichkeiten zwischen urspr\u00fcnglicher und neuer Aufgabe sind vorhanden</li> </ol> <p>F\u00fcr unseren Anwendungsfall w\u00e4hlen wir Methode 3. Hierf\u00fcr iterieren wir \u00fcber die einzelnen Schichten des BERT Modells und setzten den Parameter requires_grad jeweils auf den Wert false. Dadurch wird verhindert, dass der Gradient w\u00e4hrend des Trainings berechnet und die Gewichte aktualisiert werden.</p> <pre><code>for param in bert.parameters():\n    param.requires_grad = False\n</code></pre>"},{"location":"Themen/transfer_learning_nlp/#46-fine-tuning","title":"4.6 Fine-Tuning","text":"<p>Wie es f\u00fcr Pytorch \u00fcblich ist, m\u00fcssen nun die Trainings- und Evaluierungsschleife implementiert werden. In der Trainingsschleife erfolgt pro Iteration der gleiche Ablauf: 1. Datenbereitstellung: Die Eingabedaten und die Labels werden aus den aktuellen Batch extrahiert. 2. Vorw\u00e4rtsdurchlauf: Die Eingabedaten werden in das Modell gegeben, welches Vorhersagen generiert. 3. Fehlerberechnung: Es folgt ein Abgleich der vorhergesagten Werte mit den tats\u00e4chlichen Werten. 4. R\u00fcckw\u00e4rtsdurchlauf und Gewichtsaktualisierung: Der Backpropagation-Algorithmus wird verwendet, um die Gradienten der Gewichte des Modells zu berechnen. Der Optimizer nutzt diese Gradienten, um die Gewichte entsprechend anzupassen und das Modell zu optimieren.</p> <p>\u00c4hnlich verh\u00e4lt sich die Evaluierungsschleife, mit dem Unterschied, dass der R\u00fcckw\u00e4rtsdurchlauf nicht durchgef\u00fchrt wird.</p> <pre><code>def train():\n    model.train()\n    total_loss, total_accuracy = 0, 0\n\n    for step, batch in enumerate(train_dataloader):\n        if step % 50 == 0 and not step == 0:\n            print('  Batch {:&gt;5,}  of  {:&gt;5,}.'.format(step, len(train_dataloader)))\n\n        input_id, mask, labels = batch\n        model.zero_grad()\n        preds = model(input_id, mask)\n        loss = cross_entropy(preds, labels)\n        total_loss = total_loss + loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n\n    avg_loss = total_loss / len(train_dataloader)\n    return avg_loss\n</code></pre> <p>Nachdem das Modell trainiert wurde, ist es an der Zeit die Performance des Modells auf den Testdaten zu \u00fcberpr\u00fcfen. Die folgende Tabelle zeigt die erhaltenen Metriken:</p> precision recall f1-score support 0 0.99 0.99 0.99 3212 1 0.99 0.99 0.99 3523 accuracy 0.99 6735 macro avg 0.99 0.99 0.99 6735 weighted avg 0.99 0.99 0.99 6735"},{"location":"Themen/transfer_learning_nlp/#47-inference","title":"4.7 Inference","text":"<p>Um mit dem Modell Vorhersagen machen zu k\u00f6nnen, m\u00fcssen folgende Schritte durchgef\u00fchrt werden: 1. Tokenisieren der Schlagzeile  <pre><code>unseen_news_text = [\"Donald Trump Sends Out Embarrassing New Year\u2019s Eve Message; This is Disturbing\"]\n\nMAX_LENGTH = 20\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\ntokens_unseen = tokenizer.batch_encode_plus(\n    unseen_news_text,\n    max_length=MAX_LENGTH,\n    padding=True,\n    truncation=True\n)\n</code></pre> 2. input_ids und attention_mask zu Tensoren konvertieren <pre><code>unseen_seq = torch.tensor(tokens_unseen['input_ids'])\nunseen_mask = torch.tensor(tokens_unseen['attention_mask'])\n</code></pre> 3. Vorhersage und Ausgabe <pre><code>preds = model(unseen_seq, unseen_mask)\npreds = np.argmax(preds, axis=1)\n\nfor idx, pred in enumerate(preds):\n    if pred == 0:\n        print(f\"Headline {idx+1} is True\")\n        continue\n    print(f\"Headline {idx+1} is Fake\")\n</code></pre></p>"},{"location":"Themen/transfer_learning_nlp/#5-fazit","title":"5. Fazit","text":"<p>Transfer Learning in der Sprachverarbeitung hat sich als eine vielversprechende Methode erwiesen, um die Leistung von Sprachmodellen zu verbessern. Es erm\u00f6glicht die Anwendung vortrainierter Modelle auf neue sprachverarbeitende Aufgaben, indem das bereits erworbene Wissen auf eine andere, verwandte Aufgabe \u00fcbertragen wird. In den letzten Jahren wurden bedeutende Fortschritte im Bereich des Transfer Learning in der Sprachverarbeitung erzielt, insbesondere mit Hilfe von Deep Learning-Modellen wie BERT.</p> <p>Die Anwendung von Transfer Learning bietet eine Reihe von Vorteilen. Es verk\u00fcrzt nicht nur die Trainingszeit erheblich, sondern reduziert auch den Bedarf an umfangreichen Datenmengen. Durch die Nutzung vortrainierten Wissens, das in einem anderen Kontext erworben wurde, kann die Genauigkeit von Spracherkennungssystemen verbessert werden, selbst wenn nur begrenzte Daten zur Verf\u00fcgung stehen. Transfer Learning erm\u00f6glicht die Entwicklung ma\u00dfgeschneiderter Modelle f\u00fcr spezifische sprachverarbeitende Aufgaben und findet Anwendung in verschiedenen Bereichen wie der Sprachbefehlserkennung, der Transkription von Audioaufnahmen und der automatisierten Analyse von Sprachanrufen.</p> <p>Der aktuelle Stand der Forschung zeigt das Potenzial dieser Technik. Forscher haben verschiedene Ans\u00e4tze und Techniken entwickelt, um die Effizienz und Genauigkeit von Spracherkennungssystemen weiter zu verbessern. Die Kombination von Transfer Learning mit Active Learning, Reinforcement Learning und Generative Adversarial Networks er\u00f6ffnet neue M\u00f6glichkeiten, um die Leistungsf\u00e4higkeit der Systeme zu steigern. Die Erweiterung des Transfer Learning auf mehrsprachige Szenarien erm\u00f6glicht eine verbesserte Spracherkennung in verschiedenen Sprachen.</p> <p>Die Implementierung erfordert eine sorgf\u00e4ltige Vorbereitung der Daten, die Auswahl geeigneter vortrainierter Modelle und die Anpassung des Modells an die spezifische Aufgabe. Das Einfrieren bestimmter Schichten des vortrainierten Modells w\u00e4hrend des Trainings kann den Schutz des bereits gelernten Wissens gew\u00e4hrleisten und die Trainingszeit reduzieren. Durch das Fine-Tuning des Modells k\u00f6nnen optimale Ergebnisse erzielt werden.</p> <p>Insgesamt ist Transfer Learning ein vielversprechender Ansatz, um pr\u00e4zisere und effizientere Spracherkennungssysteme zu entwickeln. Mit weiteren Fortschritten in der Forschung und der Anwendung von Deep Learning-Modellen wird Transfer Learning eine immer wichtigere Rolle in der k\u00fcnstlichen Intelligenz und im maschinellen Lernen spielen. Es er\u00f6ffnet neue M\u00f6glichkeiten f\u00fcr die Verbesserung von Spracherkennungssystemen und hat das Potenzial, unsere t\u00e4gliche Interaktion mit sprachbasierten Technologien weiter zu verbessern.</p>"},{"location":"Themen/transfer_learning_nlp/#6-weiterfuhrendes-material","title":"6. Weiterf\u00fchrendes Material","text":""},{"location":"Themen/transfer_learning_nlp/#61-podcast","title":"6.1 Podcast","text":"<p>Der Campus Talk - Silicon Forest</p>"},{"location":"Themen/transfer_learning_nlp/#62-talk","title":"6.2 Talk","text":"<p>Video Fachvortrag</p>"},{"location":"Themen/transfer_learning_nlp/#63-demo","title":"6.3 Demo","text":"<p>Video Code Demonstration</p> <p>Source Code</p>"},{"location":"Themen/transfer_learning_nlp/#64-literaturliste","title":"6.4 Literaturliste","text":"<p>Taylor, Matthew E., and Peter Stone. \"Transfer learning for reinforcement learning domains: A survey.\" Journal of Machine Learning Research 10, no. 7 2009.</p> <p>Wang, Dong, and Thomas Fang Zheng. \"Transfer learning for speech and language processing.\" 2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA). IEEE, 2015.</p> <p>S. Thrun and L. Pratt, Learning to learn. Springer Science &amp; Business Media, 2012.</p> <p>S. J. Pan and Q. Yang, \u201cA survey on transfer learning,\u201d Knowledge and Data Engineering, IEEE Transactions on, vol. 22, no. 10, pp. 1345\u2013 1359, 2010.</p> <p>Torrey, Lisa, and Jude Shavlik. \"Transfer learning.\" Handbook of research on machine learning applications and trends: algorithms, methods, and techniques. IGI global, 2010. 242-264.</p> <p>R. Caruana, \u201cMultitask learning,\u201d Machine learning, vol. 28, no. 1, pp. 41\u201375, 1997.</p> <p>J.-L. Gauvain and C.-H. Lee, \u201cMaximum a posteriori estimation for multivariate Gaussian mixture observations of Markov chains,\u201d IEEE Transactions on Speech and audio processing, vol. 2, no. 2, pp. 291\u2013 298, 1994.</p> <p>C. J. Leggetter and P. Woodland, \u201cMaximum likelihood linear re- gression for speaker adaptation of continuous density hidden Markov models,\u201d Computer Speech &amp; Language, vol. 9, no. 2, pp. 171\u2013185, 1995.</p> <p>P. E. Utgoff, \u201cIncremental induction of decision trees,\u201d Machine learning, vol. 4, no. 2, pp. 161\u2013186, 1989.</p> <p>A. Blum and T. Mitchell, \u201cCombining labeled and unlabeled data with co-training,\u201d in Proceedings of the eleventh annual conference on Computational learning theory. ACM, 1998, pp. 92\u2013100.</p> <p>C. Wang and S. Mahadevan, \u201cHeterogeneous domain adaptation using manifold alignment,\u201d in IJCAI Proceedings-International Joint Confer- ence on Arti\ufb01cial Intelligence, vol. 22, no. 1, 2011, p. 1541.</p> <p>Y. Zhu, Y. Chen, Z. Lu, S. J. Pan, G.-R. Xue, Y. Yu, and Q. Yang, \u201cHeterogeneous transfer learning for image classi\ufb01cation.\u201d in AAAI, 2011.</p> <p>O. Arandjelovic and R. Cipolla, \u201cIncremental learning of temporally- coherent Gaussian mixture models,\u201d Society of Manufacturing Engi- neers (SME) Technical Papers, pp. 1\u20131, 2006.</p> <p>A. Declercq and J. H. Piater, \u201cOnline learning of Gaussian mixture models-a two-level approach.\u201d in VISAPP (1), 2008, pp. 605\u2013611.</p> <p>X. Zhu, \u201cSemi-supervised learning literature survey,\u201d Computer Sci- ences TRP 1530, University of Wisconsin C Madison, 2005.</p> <p>S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang, \u201cDomain adaptation via transfer component analysis,\u201d Neural Networks, IEEE Transactions on, vol. 22, no. 2, pp. 199\u2013210, 2011.</p> <p>R. Raina, A. Battle, H. Lee, B. Packer, and A. Y. Ng, \u201cSelf-taught learning: transfer learning from unlabeled data,\u201d in Proceedings of the 24th international conference on Machine learning. ACM, 2007, pp. 759\u2013766.</p> <p>P. Prettenhofer and B. Stein, \u201cCross-lingual adaptation using structural correspondence learning,\u201d ACM Transactions on Intelligent Systems and Technology (TIST), vol. 3, no. 1, p. 13, 2011.</p> <p>W. Dai, Y. Chen, G.-R. Xue, Q. Yang, and Y. Yu, \u201cTranslated learning: Transfer learning across different feature spaces,\u201d in Advances in neural information processing systems, 2008, pp. 353\u2013360.</p> <p>B. Kulis, K. Saenko, and T. Darrell, \u201cWhat you saw is not what you get: Domain adaptation using asymmetric kernel transforms,\u201d in Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on. IEEE, 2011, pp. 1785\u20131792.</p> <p>B. Wei and C. J. Pal, \u201cHeterogeneous transfer learning with RBMs.\u201d in AAAI, 2011.</p> <p>X. Shi, Q. Liu, W. Fan, P. S. Yu, and R. Zhu, \u201cTransfer learning on heterogenous feature spaces via spectral transformation,\u201d in Data Mining (ICDM), 2010 IEEE 10th International Conference on. IEEE, 2010, pp. 1049\u20131054.</p> <p>L. Duan, D. Xu, and I. Tsang, \u201cLearning with augmented features for heterogeneous domain adaptation,\u201d arXiv preprint arXiv:1206.4660, 2012.</p> <p>J. T. Zhou, S. J. Pan, I. W. Tsang, and Y. Yan, \u201cHybrid heterogeneous transfer learning through deep learning,\u201d in Twenty-Eighth AAAI Con- ference on Arti\ufb01cial Intelligence, 2014.</p> <p>D. Gentner, \u201cStructure-mapping: A theoretical framework for analogy,\u201d Cognitive science, vol. 7, no. 2, pp. 155\u2013170, 1983.</p> <p>D. Gentner and K. J. Holyoak, \u201cReasoning and learning by analogy: Introduction.\u201d American Psychologist, vol. 52, no. 1, p. 32, 1997.</p> <p>J. Blitzer, R. McDonald, and F. Pereira, \u201cDomain adaptation with struc- tural correspondence learning,\u201d in Proceedings of the 2006 conference on empirical methods in natural language processing. Association for Computational Linguistics, 2006, pp. 120\u2013128.</p> <p>H.-Y. Wang and Q. Yang, \u201cTransfer learning by structural analogy,\u201d in AAAI. Citeseer, 2011.</p> <p>J. G. Carbonell, Learning by analogy: Formulating and generalizing plans from past experience. Springer, 1983.</p> <p>J. Baxter, \u201cA model of inductive bias learning,\u201d J. Artif. Intell. Res.(JAIR), vol. 12, pp. 149\u2013198, 2000.</p> <p>J. Guinney, Q. Wu, and S. Mukherjee, \u201cEstimating variable structure and dependence in multitask learning via gradients,\u201d Machine Learning, vol. 83, no. 3, pp. 265\u2013287, 2011.</p> <p>B. Romera-Paredes, A. Argyriou, N. Berthouze, and M. Pontil, \u201cEx- ploiting unrelated tasks in multi-task learning,\u201d in International Con- ference on Arti\ufb01cial Intelligence and Statistics, 2012, pp. 951\u2013959.</p> <p>D. Wang, C. Liu, Z. Tang, Z. Zhang, and M. Zhao, \u201cRecurrent neural network training with dark knowledge transfer,\u201d arXiv preprint arXiv:1505.04630, 2015.</p> <p>Z. Tang, D. Wang, Y. Pan, and Z. Zhang, \u201cKnowledge transfer pre- training,\u201d arXiv preprint arXiv:1506.02256, 2015.</p> <p>J. Ba and R. Caruana, \u201cDo deep nets really need to be deep?\u201d in Advances in Neural Information Processing Systems, 2014, pp. 2654\u2013 2662.</p> <p>G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.</p> <p>L. Deng and D. Yu, \u201cDeep learning: Methods and applications,\u201d Foundations and Trends in Signal Processing, vol. 7, no. 3-4, pp. 197\u2013 387, 2013</p> <p>X. He, J. Gao, and L. Deng, \u201cDeep learning for natural language processing and related applications (Tutorial at ICASSP),\u201d in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2014.</p> <p>J. Hirschberg and C. D. Manning, \u201cAdvances in natural language processing,\u201d Science, vol. 349, no. 6245, pp. 261\u2013266, 2015.</p> <p>G. E. Hinton, S. Osindero, and Y.-W. Teh, \u201cA fast learning algorithm for deep belief nets,\u201d Neural computation, vol. 18, no. 7, pp. 1527\u2013 1554, 2006.</p> <p>R. Salakhutdinov and G. E. Hinton, \u201cDeep boltzmann machines,\u201d in International Conference on Arti\ufb01cial Intelligence and Statistics, 2009, pp. 448\u2013455.</p> <p>Y. Bengio, P. Lamblin, D. Popovici, H. Larochelle et al., \u201cGreedy layer-wise training of deep networks,\u201d Advances in neural information processing systems, vol. 19, p. 153, 2007.</p> <p>P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol, \u201cStacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion,\u201d The Journal of Machine Learning Research, vol. 11, pp. 3371\u20133408, 2010.</p> <p>G. E. Dahl, D. Yu, L. Deng, and A. Acero, \u201cContext-dependent pre- trained deep neural networks for large-vocabulary speech recognition,\u201d Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 1, pp. 30\u201342, 2012.</p> <p>A. Graves, A.-R. Mohamed, and G. Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2013, pp. 6645\u20136649.</p> <p>Y. Bengio and O. Delalleau, \u201cOn the expressive power of deep architectures,\u201d in Algorithmic Learning Theory. Springer, 2011, pp. 18\u201336.</p> <p>R. Collobert and J. Weston, \u201cA uni\ufb01ed architecture for natural lan- guage processing: Deep neural networks with multitask learning,\u201d in Proceedings of the 25th international conference on Machine learning. ACM, 2008, pp. 160\u2013167.</p> <p>L. Deng, J. Li, J.-T. Huang, K. Yao, D. Yu, F. Seide, M. Seltzer, G. Zweig, X. He, J. Williams et al., \u201cRecent advances in deep learning for speech research at Microsoft,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8604\u20138608.</p> <p>S. M. Gutstein, Transfer learning techniques for deep neural nets. The University of Texas at El Paso, 2010.</p> <p>J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng, \u201cMultimodal deep learning,\u201d in Proceedings of the 28th international conference on machine learning (ICML-11), 2011, pp. 689\u2013696.</p> <p>Y. Bengio, I. J. Goodfellow, and A. Courville, Deep Learning, 2015, book in preparation for MIT Press.</p> <p>G. E. Hinton and R. R. Salakhutdinov, \u201cReducing the dimensionality of data with neural networks,\u201d Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.</p> <p>X. Glorot, A. Bordes, and Y. Bengio, \u201cDomain adaptation for large- scale sentiment classi\ufb01cation: A deep learning approach,\u201d in Proceed- ings of the 28th International Conference on Machine Learning (ICML- 11), 2011, pp. 513\u2013520.</p> <p>M. Oquab, L. Bottou, I. Laptev, and J. Sivic, \u201cLearning and transferring mid-level image representations using convolutional neural networks,\u201d in Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEE, 2014, pp. 1717\u20131724.</p> <p>W. Zhang, R. Li, T. Zeng, Q. Sun, S. Kumar, J. Ye, and S. Ji, \u201cDeep model based transfer and multi-task learning for biological image analysis,\u201d in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2015, pp. 1475\u20131484.</p> <p>L. Fei-Fei, R. Fergus, and P. Perona, \u201cOne-shot learning of object cate- gories,\u201d Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 28, no. 4, pp. 594\u2013611, 2006.</p> <p>H. Larochelle, D. Erhan, and Y. Bengio, \u201cZero-data learning of new tasks.\u201d in AAAI, vol. 1, no. 2, 2008, p. 3.</p> <p>R. Socher, M. Ganjoo, C. D. Manning, and A. Ng, \u201cZero-shot learning through cross-modal transfer,\u201d in Advances in neural information processing systems, 2013, pp. 935\u2013943.</p> <p>Y. Bengio, \u201cDeep learning of representations for unsupervised and transfer learning,\u201d in ICML Unsupervised and Transfer Learning, 2012.</p> <p>T. Croonenborghs, K. Driessens, and M. Bruynooghe. \"Learning relational skills for inductive transfer in relational reinforcement learning.\" In International Conference on Inductive Logic Programming, 2007.</p> <p>L. Torrey, J. Shavlik, T. Walker, and R. Maclin. \"Relational skill transfer via advice taking.\" In ICML Workshop on Structural Knowledge Transfer for Machine Learning, 2006.</p> <p>L. Torrey, T. Walker, J. Shavlik, and R. Maclin. \"Using advice to transfer knowledge acquired in one reinforcement learning task to another.\" In European Conference on Machine Learning, 2005.</p> <p>M. Rosenstein, Z. Marx, L. Kaelbling, and T. Dietterich. \"To transfer or not to transfer.\" In NIPS Workshop on Inductive Transfer, 2005.</p> <p>M. Taylor, G. Kuhlmann, and P. Stone. \"Accelerating search with transferred heuristics.\" In ICAPS Workshop on AI Planning and Learning, 2007.</p> <p>E. Talvitie and S. Singh. \"An experts algorithm for transfer learning. In Interna- tional Joint Conference on Arti\ufb01cial Intelligence, 2007.</p> <p>G. Kuhlmann and P. Stone. \"Graph-based domain mapping for transfer learning in general games.\" In European Conference on Machine Learning, 2007.</p> <p>E. Eaton and M. DesJardins. \"Knowledge transfer with a multiresolution ensemble of classi\ufb01ers.\" In ICML Workshop on Structural Knowledge Transfer for Machine Learning, 2006.</p> <p>C. Carroll and K. Seppi. \"Task similarity measures for transfer in reinforcement learning task libraries.\" In IEEE International Joint Conference on Neural Net- works, 2005.</p> <p>E. Eaton, M. DesJardins, and T. Lane. \"Modeling transfer relationships between learning tasks for improved inductive transfer.\" In European Conference on Machine Learning, 2008.</p> <p>U. Ruckert and S. Kramer. \"Kernel-based inductive transfer.\" In European Confer- ence on Machine Learning, 2008.</p> <p>Bardovi\u2010Harlig K, Sprouse RA. \"Negative versus positive transfer.\" The TESOL encyclopedia of English language teaching. 2018 Feb 12:1-6.</p> <p>Korner-Nievergelt, Fr\u00e4nzi &amp; H\u00fcppop, Ommo. \"A short introduction to bayes statistics with R for ornithologists.\" Vogelwarte. 2016. pp. 181-194.</p> <p>Knowledge Distillation, 2023.07.06</p> <p>Bucilu\u01ce, Cristian, Rich Caruana, and Alexandru Niculescu-Mizil. \"Model compression.\" In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 535-541. 2006.</p> <p>Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. \"Distilling the knowledge in a neural network.\" arXiv preprint arXiv:1503.02531 2015.</p> <p>Tann, Hokchhay, Soheil Hashemi, R. Iris Bahar, and Sherief Reda. \"Hardware-software codesign of accurate, multiplier-free deep neural networks.\" In Proceedings of the 54th Annual Design Automation Conference 2017, pp. 1-6. 2017.</p> <p>Mishra, Asit, and Debbie Marr. \"Apprentice: Using knowledge distillation techniques to improve low-precision network accuracy.\" arXiv preprint arXiv:1711.05852 2017.</p> <p>Polino, Antonio, Razvan Pascanu, and Dan Alistarh. \"Model compression via distillation and quantization.\" arXiv preprint arXiv:1802.05668 2018.</p> <p>Ashok, Anubhav, Nicholas Rhinehart, Fares Beainy, and Kris M. Kitani. \"N2n learning: Network to network compression via policy gradient reinforcement learning.\" arXiv preprint arXiv:1709.06030 2017.</p> <p>Theis, Lucas, Iryna Korshunova, Alykhan Tejani, and Ferenc Husz\u00e1r. \"Faster gaze prediction with dense networks and fisher pruning.\" arXiv preprint arXiv:1801.05787 2018.</p> <p>Preprocess data for Natural Language Processing</p> <p>An Explanatory Guide to BERT Tokenizer</p> <p>BERT base model (uncased)</p> <p>Transfer Learning Guide: A Practical Tutorial With Examples for Images and Text in Keras</p>"}]}